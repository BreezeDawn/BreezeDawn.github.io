<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MachineLearning-聚类算法]]></title>
    <url>%2Fposts%2F37668%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-支持向量机（SVM）]]></title>
    <url>%2Fposts%2F11763%2F</url>
    <content type="text"><![CDATA[支持向量机的代价函数$$J(\theta) = \min_{\theta} C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta^Tx^{(i)})] + \frac{1}{2}\sum_{i=1}^n\theta_j^{2}$$ C 可以看作 $\frac{1}{\lambda}$ 支持向量机的作用 人们有时将支持向量机看作是大间距分类器 支持向量机能够努力的将正样本和负样本用最大的间距分开。这也是支持向量机具有鲁棒性的原因，鲁棒是Robust的音译，也就是健壮和强壮的意思。 支持向量机（SVM）实际上是一种凸优化问题，因此它总是能找到全局最小值或者接近它的值从而不用担心局部最优 关于内积和范数1. 内积：设有n维向量 ​ 令 ， 则称[x,y]为向量x与y的内积。 2. 范数：称 为向量x的范数(或长度)。 支持向量机产生大间距分类的原因由内积和范数引起变化，具体先不写。。。 核函数公式$$exp(-\frac{||x-l^{i}||^2}{2\sigma^2})$$ 目的使用核函数构造复杂的非线性分类器，能够根据数据的相似与否定义许多新的特征值 相似度函数就是核函数就是高斯核函数，$\sigma$ 是高斯核函数的参数 我们通过标记点和核函数来定义新的特征变量从而训练复杂的非线性边界 如何使用我们通过核函数能够得到 如何选取标记点每一个标记点的位置都与样本点的位置精确对应，选出 $m$ 个标记点。这样就说明特征函数基本上是在描述每一个样本距离样本集中其他样本的距离 支持向量机如何通过核函数有效的学习复杂非线性函数如果我们要进行预测，首先我们需要计算特征向量 $f_{(m+1)×1}$ ，内部值都是 传入 $x$ 与 标记点 通过核函数 与m个样本点进行相似度比较产出的。 我们再使用参数转置乘特征向量： $\theta^Tf = \theta_0f_0 + \theta_1f_1 + \theta_2f_2 + ……+ \theta_mf_m$ 如果结果 大于等于零，预测结果为 1。 但是我们怎么获得参数 $\theta$ 的值，我们通过最小化下式就能得到支持向量机的参数$$J(\theta) = \min_{\theta} C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)}) + (1-y^{(i)})cost_0(\theta^Tf^{(i)})] + \frac{1}{2}\sum_{i=1}^n\theta_j^{2}$$ 这里的 $n = m$ ，这里我们仍然不对 $\theta_0$ 做正则化处理 最后的 $\sum_{i=1}^n\theta_j^{2}$ 还能够被写为 $\theta^T\theta$ 或是别的比如 $\theta^TM\theta$ ，这取决于我们使用的是什么核函数，这能够使支持向量机更有效率的运行，这样修改能够适应超大的训练集，那时 求解m维参数的成本会非常高，主要为了计算效率。 核函数虽然也能用在逻辑回归上，但是它毕竟是为支持向量机开发的，用在逻辑回归上会十分缓慢。 使用支持向量机时，怎么选择支持向量机里的参数参数 $C$在使用支持向量机时，其中一个要选择的事情是目标函数中的参数 $C$ 。 我们知道 $C$ 的作用类似于 $\frac{1}{\lambda}$ 。 如果使用较大的 $C$ ，这意味着我们没有使用正则化，这可能使我们可能得到一个低偏差高方差的模型。 如果使用较小的 $C$ ，这相当于我们在逻辑回归中用了一个大的 $\lambda$，这可能使我们可能得到一个高偏差低方差的模型。 高斯核函数中的参数 $\sigma^2$当 $\sigma^2$ 偏大时，由核函数得到的相似度会变化的很平缓，这会给模型带来较高的偏差和较低的方差。 当 $\sigma^2$ 偏小时，由核函数得到的相似度会变化的很剧烈，会有较大斜率和较大的导数，这会给模型带来较低的偏差和较高的方差。 运用SVM时的一些细节 使用不带有核函数的支持向量机就叫做线核的SVM，即没有 $f$ ，你可以把它想象为给了你一个线性分类器 线核SVM如果有大量的特征值（N很大），且训练的样本数很小（M很小），那么是不会去想着拟合拟合一个非常复杂的非线性函数的，因为没有足够多的数据很有可能过度拟合。 高斯核函数如果有少量的特征值（N很小），且训练的样本数很大（M很大）， 提供核函数高斯核函数和线性核函数是最普遍的核函数， 注意事项：如果你有大小不一样的特征变量，为了不使间距被大型特征操控（小的特征都被忽略掉），在使用高斯核函数前最好将这些特征变量的大小按比例归一化。 warning：所有的核函数都已经满足一个技术条件，它叫做莫塞尔定理。 吴恩达很少很少很少使用其他核函数， 多类分类（K分类）中如何使用支持向量机现成的多类分类的函数包， 逻辑回归算法于支持向量机的选择$n&gt;=m$ 逻辑回归或者线核SVM，因为没有更好更多的数据拟合复杂的非线性函数 $n&lt;=m$ 高斯SVM $n&lt;&lt;=m$ 增加或者创建更多的特征变量，然后使用逻辑回归或者线核SVM 为什么不使用神经网络训练起来会特别的慢 最后算法确实很重要，但更重要的是我们有多少数据，我们是否擅长做误差分析和诊断学习算法来指出设定新的特征变量，或找出其他能够决定我们学习算法的变量等方面，通常这些方面会比我们使用逻辑回归还是SVM这方面更加重要]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-如何有效使用机器学习算法]]></title>
    <url>%2Fposts%2F16394%2F</url>
    <content type="text"><![CDATA[怎么改进算法当使用训练好的模型时，新样本输出的数据产生了巨大的误差，如何改进算法的性能。 使用更多的训练样本，但通常来讲并没有什么卵用 尝试选用更少的特征集，来防止过拟合 或许也需要更多的特征集，当目前的特征集对你没有多大用处时，可以从更多的特征角度去收集更多的特征 增加多项式特征 减小正则化中的 $\lambda$ 的值 增大正则化中的 $\lambda$ 的值 我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。 怎么评估算法的性能（机器学习诊断法[machine learning diagnostics]） “诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。 标准方法 将所有数据按照 7：3 的比例分成训练集和测试集，使用 $(x_{test}^{(i)}，y_{test}^{(i)})$ 表示测试集数据。 如果所有的数据存在规律或顺序，最好先打乱顺序再按比例分割。 测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算测试集误差： 对于线性回归模型，我们利用测试集数据计算代价函数$J_{test}(\theta)$ 对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外：$$J_{test}{(\theta)} = -\frac{1}{m}{test}\sum{i=1}^{m_{test}}\log{h_{\theta}(x^{(i)}{test})}+(1-{y^{(i)}{test}})\log{h_{\theta}(x^{(i)}_{test})}$$ 误分类的比率，对于每一个测试集样本，计算： ​ 然后对计算结果求平均。 怎么选择模型通过交叉验证，选择能最好的拟合数据的多项式次数的模型 定义多个模型，每个模型的次数不同，使用 $d$ 表示模型的多项式最高次数。 将所有数据按照 6：2：2 的比例分成训练集、交叉验证集和测试集，使用 $(x_{cv}^{(i)}，y_{cv}^{(i)})$ 表示验证集数据，使用 $(x_{test}^{(i)}，y_{test}^{(i)})$ 表示测试集数据。 如果所有的数据存在规律或顺序，最好先打乱顺序再按比例分割。 $m_{cv}$ 表示验证集总数，$m_{test}$ 表示测试集总数。 同样的，我们能够定义训练集误差 $J_{train}{(\theta)}$、验证集误差 $J_{cv}{(\theta)}$、测试集误差 $J_{test}{(\theta)}$ 使用训练集代入所有模型并通过训练使得最终代价函数 $J(\theta)$ 最小，再使用验证集代入训练后的所有模型来算出$J_{cv}{(\theta)}$，选出能最好的对交叉验证集进行预测的模型（$J_{cv}{(\theta)}$ 最小的模型），确定最终模型的最高次数 $d$。 使用测试集，预测或估计，通过学习算法得出的模型的泛化误差。 最好按比例分出三份不一样的数据，如果只分为两份，让其中一份既作为验证集又作为测试集，并不好。 模型出现问题，是欠拟合还是过拟合 偏差比较大（欠拟合） 方差比较大（过拟合） 高偏差（欠拟合）： 当 训练集数据 和 验证集数据 出现的误差都很大时，且两个误差可能很接近或者可能验证误差稍大一点。 高方差（过拟合）： 当 训练集数据 出现的误差很小， 验证集数据 出现的误差很大时，且 $J_{cv}{(\theta)} &gt; &gt; J_{train}{(\theta)}$ 误差即 $J(\theta)$ $&gt;&gt;$ 远大于 如何选取正则化参数 $\lambda$之前通过交叉验证后我们已经选择了一个合适的模型，但是我们还没有正则化项。 这次我们仍然通过交叉验证，来进行选择一个合适的正则化参数 $\lambda$ 。 定义多个正则化参数 $\lambda$，每个模型的$\lambda$不同。 将所有数据按照 6：2：2 的比例分成训练集、交叉验证集和测试集，使用 $(x_{cv}^{(i)}，y_{cv}^{(i)})$ 表示验证集数据，使用 $(x_{test}^{(i)}，y_{test}^{(i)})J_{cv}{(\theta)}$ 表示测试集数据。 如果所有的数据存在规律或顺序，最好先打乱顺序再按比例分割。 $m_{cv}$ 表示验证集总数，$m_{test}$ 表示测试集总数。 同样的，我们能够定义训练集误差 $J_{train}{(\theta)}$、验证集误差 $J_{cv}{(\theta)}$、测试集误差 $J_{test}{(\theta)}$ 使用训练集代入所有模型并通过训练使得最终代价函数 $J(\theta)$ 最小，再使用验证集代入训练后的所有模型来算出$J_{cv}{(\theta)}$，选出能最好的对交叉验证集进行预测的模型（$J_{cv}{(\theta)}$ 最小的模型），确定最终正则化参数 $\lambda$ 。 使用测试集，预测或估计，通过学习算法得出的模型的对新样本的泛化能力。 关于学习曲线从学习曲线，我们能够看出模型面临的是什么问题。 当模型处于高偏差时，误差会趋于水平不会再降，哪怕添加再多的训练集数据模型产生的误差也不会有所改善。 当模型处于高方差时，训练集误差会始终很小，验证集误差会始终很大，但是如果继续增大训练集数据，是能够改进模型的泛化能力的。 因此，画出模型的学习曲线，搞清楚当前算法是否存在高偏差或是高方差，对于改善算法来讲是非常有意义的，我们能够选择是否添加更多的训练及数据来应对问题。 应该采用哪种方法改进算法 获得更多的训练样本——解决高方差 尝试减少特征的数量——解决高方差 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 尝试增加正则化程度λ——解决高方差 1234567使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络，然后选择交叉验证集代价最小的神经网络。 误差分析构建一个学习算法的推荐方法为： 首先使用简单快速的方式实现算法，然后使用交叉验证集数据验证这个算法。 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他方式完善算法。 进行误差分析：人工检查交叉验证集在我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势。 数值评估误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 通过一个量化的数值评估，我们可以看看这个数字，误差是变大还是变小了，它可以直观地告诉我们：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高我们实践算法时的速度。 类偏斜类偏斜情况表现为在训练集中同一种类的样本特别多，其他类的样本特别少。 在类偏斜的情况下，模型的准确率的提升并不能说明我们的模型质量得到了提升，比如当我们的模型准确率为98时，我们完善了模型后，把准确率提升到了99％，但是训练集中的99％的数据是属于同一类的，我们并不知道我们的算法究竟是得到了完善还是变成了碰到数据就把它归为99％那一类的撒比算法。 所以我们需要一个不同寻常的评估度量值：查准率 和 召回率。 查准率：预测出的实际结果数量 占 预测结果总数量 的百分比 召回率：预测出的实际结果数量 占 实际结果总数量 的百分比 这两个数字越高，就说明模型质量越好。 查准率和召回率之间的权衡我们选取的阈值决定了我们最终预测结果的走向是0还是1，所以阈值是影响查准率和召回率的重要因素，那么我们应该选取 $0-1$ 之间哪个数为阈值呢? 不同的阈值对应着不同的查准率和召回率，查准率和召回率都是越高越好，但是我们无法判断究竟查准率 $0.7$ 召回率 $0.3$ 好 ，还是查准率 $0.4$ 召回率 $0.6$ 好 。 这时我们就需要一个评估度量值来综合的考虑两个因素给定一个结果：阈值设为多少合适。 这个评估度量值的计算公式有很多个，我们现在看在机器学习中经常用到的一个：$${F}_{1} = \frac{PR}{P+R}$$ P 查准率 R 召回率 机器学习的数据123事实上，如果你选择任意一个算法，可能是选择了一个&quot;劣等的&quot;算法，如果你给这个劣等算法更多的数据，那么从这些例子中看起来的话，它看上去很有可能会其他算法更好，甚至会比&quot;优等算法&quot;更好。由于这项原始的研究非常具有影响力，因此已经有一系列许多不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现，这还取决于一些细节，但是真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识：&quot;取得成功的人不是拥有最好算法的人，而是拥有最多数据的人&quot;。现在假设我们使用了非常非常大的训练集，在这种情况下，尽管我们希望有很多参数，但是如果训练集比参数的数量还大，甚至是更多，那么这些算法就不太可能会过度拟合。也就是说训练误差有希望接近测试误差。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-see，HexoGUI，发布文章-提交仓库-重置静态文件-开启本地服务]]></title>
    <url>%2Fposts%2F48526%2F</url>
    <content type="text"><![CDATA[一、hexo-see简介Python3 实现。 Hexo的可视化界面，摆脱命令行。 很粗糙，请见谅。 目前有个BUG：只要是GUI界面创建的文章，在每次打开后保存(或自动保存)时，所有内容会丢失，但只要撤回一下内容就回来了。这个BUG可能由于使用了动态生成链接的插件导致的，也可能都会出现。 二、功能1、界面化创建文章！ 2、创建文章后可选择直接打开 3、提交至远程仓库 4、清除本地public文件 5、开启本地服务 三、所用包 包 操作 tkinter 实现GUI界面 os 进行命令操作 threading 进行多线程操作 win32api 实现界面居中 四、按钮与命令的映射关系 按钮名称 对应命令 重新生成静态文件 hexo g 清除本地public文件 hexo clean 创建文章 hexo n post 提交仓库 hexo d 本地预览 hexo s 退出 退出本程序 五、使用配置 tkinter、os、threading 都是内置包，因此仅需安装 win32api， Python3 使用 pip3 install pypiwin32安装即可。 如安装失败，请手动安装whl文件。 whl文件源地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/。 更改 if __name__ == ‘__main__‘: 里初始化 Hexo 时的路径输入。 改为自己博客 站点配置根路径 即可使用！ 使用说明 输入 标题、标签、分类 直接创建！ 标题 不可为空！，标签和分类 可以为空。 如果标题中出现 空格 会被替换掉。出现 、?/\&lt;&gt;*都会被替换为 - 。 多个标签/多个分类使用空格分割！ 多个标签/多个分类使用空格分割！ 多个标签/多个分类使用空格分割！ 如果想要使用 .exe 可执行文件，需自行转换（因为需要配置自己的路径）。 可使用 pyinstaller 包进行转换，pip install pyinstaller。 下面有关于本工具的打包说明。 除本地预览为后台开启，其他都会有控制台出现，方便查看执行过程。 本地预览暂时不支持关闭（因为是后台执行，虽然也不需要关，毕竟可以一直本地访问）， 即使程序退出，本地服务也不会关闭。 因为本地服务有可能在后台运行，因此点击本地预览时将会使用taskkill杀掉 占用4000端口的服务， 然后才开启Hexo本地服务。 六、exe 可执行程序转换说明pyinstaller的参数说明123456789101112131415-c 参数 使用控制台，无界面(默认)-w 参数 使用窗口，无控制台.如果程序里有使用到控制台(如print)的就不可以使用-w, 否则会报错 '''failed to excute script xxx''' 如果想要捕捉错误信息可以先用控制台捕捉,没有报错后再使用无控制台. -D 参数 创建一个目录，包含exe文件，但会依赖很多文件（默认选项）。-F 参数 打包成一个exe文件-p 多文件打包时,以-p [其他.py] 的形式跟在主文件后 '''如:pyinstaller -w -F main.py -p view.py -p other.py'''-i 参数 修改打包后的exe图标,图标应放在py同级目录下,需要是ico格式,只改后缀不可用. '''如:pyinstaller -w -F -i zzz.ico main.py -p view.py -p other.py''' 本程序的打包说明 将配置完毕的 Hexo.py 与 favicon.ico 放在同一文件目录 使用命令行进入文件目录 pyinstaller -w -F -i favicon.ico Hexo.py 愉快使用 七、额外说明本工具开源协议为 不知道协议，因为我还没有区分这些协议的意思…… 总之，随便用，欢迎 star、 fork、issue。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MachineLearning-神经网络(二)]]></title>
    <url>%2Fposts%2F36766%2F</url>
    <content type="text"><![CDATA[神经网络的代价函数 符号 意义 $L$ 神经网络结构总层数 $S_l$ 第 $l$ 层的单元数量 (不包括偏差单元) $K = S_L$ 输出层的单元数量 (日了个仙人板板，手写一直渲染错误只能贴图了，: ) 浪费好久时间 ) 代价函数中 $\theta_0$ 总是被忽略的，因为我们并不想把 $\theta_0$ 加入到正则化里，也不想使它为 0，即不把偏差项正则化。 反向传播为什么使用反向传播在神经网路中，我们的 $\theta$ 数量居多，如果一个一个计算代价函数的偏导项再进行梯度下降计算，计算量实在是太大了，在使用梯度下降算法进行训练时速度会特别慢。因此，为了计算代价函数的偏导项，我们选择使用反向传播计算每一个神经节点激励值与期望神经节点激励值的误差，然后通过误差与神经元的激励值再次计算得出偏导项的计算结果。 思想我们能够明白，如果输出层的输出与期望得到的输出 存在误差，那么当下的每个神经元的激励值必定与得到期望输出时的每个神经元的激励值 也存在误差，我们将使用 $\delta_j^{(l)}$ 代表第 $l$ 层的第 $j$ 个神经元当下激励值与期望神经元的激励值之间存在的误差。 而反向传播算法从直观上说，就是从输出层开始到输入层为止，反向推导出每一个神经节点的激励值的误差$\delta$。 方法使用反向传播前，也就是求代价函数的导数前，首先需要使用前向传播将每一个神经节点的激励值算出，然后从后向前计算每一个神经节点的 $\delta$。我们还要明白的就是：我们此时只知道输出层神经节点的期望激励值，因此我们只能够从输出层开始计算。 那么，假设我们有一层输入层，一层输出层，两层隐藏层，一共四层，我们能够以 $\delta_2^{(4)}$ 表示输出层的第二个神经节点的激励值误差， 并且它的值能够通过计算得出： $\delta_2^{(4)} = a_2^{(4)} - y_2$ ，通常我们会以向量化的形式表示整个一层的误差值 即 $\delta^{(4)}$ = $a^{(4)} - y$。 而我们会使用这样一个公式，反向计算上一层的误差值：$$\delta^{(l-1)} = (\Theta^{(l-1)})^T\delta^{(l)} .* g’(z^{(l-1)})$$ 如 第 3 层： $$\delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)} .* g’(z^{(3)})$$ $.*$ 代表两个向量(矩阵)对应值两两相乘。 $g$ 代表激励函数，通过计算能够得出 $g’(z^{(l)}) = a^{(l)} .*(1 - a^{(l)})$ 同样的，我们能够计算出 第 2 层 $\delta^{(2)}$ ，但是我们并不需要计算 $\delta^{(1)}$，因为输入层是明确的已知值。 如何计算代价函数的导数项不使用求导的方法，我们能够通过以下公式得到导数项的最终结果（忽略正则化）：$$\frac{\partial}{\partial(\Theta_{ij}^{(l)})}J(\Theta) = a_j^{(l)} \delta_i^{(l+1)}$$ 也就是代价函数 $J(\theta)$ 对 第 $l$ 层 第 $i$ 行 $j$ 列 的 $\theta$ 求偏导 $=$ 第 $l$ 层 第 $j$ 个 神经节点的激励值 $a$ × 第 $l+1$ 层 第 $i$ 个 神经节点的误差值 $\delta$ 由此，我们能够很快求出 所有参数 $\theta$ 的偏导数。 但是由于每条训练集数据都不相同，因此针对与每条数据，得到的输出层结果与期望结果也总是不同的，那么 虽然每层的参数矩阵 $\Theta$ 一直不变，但由于每条数据的每层的误差 $\delta$ 各不相同，那么每条数据的代价函数求导自然得出的值也不相同。 所以我们需要计算出针对于每层 $\Theta$ 的每条数据的代价偏导 $\frac{\partial}{\partial(\Theta_{ij}^{(l)})}J(\Theta)$，然后进行相加，最终得出针对 $m$ 条数据算出的第 $l$ 层的总体代价函数偏导值：$\Delta_{ij}^{(l)}$ 。 所以我们能够得出代价函数针对每一个参数的平均偏导数 $D_{ij}^{(l)}$ ：$$\frac{\partial}{\partial(\Theta_{ij}^{(l)})}J(\Theta) = D_{ij}^{(l)} = \frac{1}{m}\Delta_{ij}^{(l)}$$ 当 $j = 0$ 时，最终结果为上式。 当 $j \neq 0$ 时，最终结果应为 $\frac{\partial}{\partial(\Theta_{ij}^{(l)})}J(\Theta) = D_{ij}^{(l)} = \frac{1}{m}\Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(j)}$ 为什么取平均值：稳定性。 矩阵向量化其实就是把矩阵写为一行。 梯度检验本质上就是使用求斜率的方法计算出偏导项结果，然后与反向传播算法计算出的偏导项结果进行比较校验 随机初始化为了训练神经网络，应该对权重进行随机初始化，初始化为 $-\epsilon &lt; \theta &lt; \epsilon$ 接近于0的小数，然后进行反向传播，执行梯度检验，使用梯度下降或者使用更好的优化算法试着使 $J$ 最小。作为参数 $\theta$ 的需要使用随机的初始值来打破对称性，使得梯度下降或是更好的优化算法找到 $\theta$ 的最优值。 关于隐藏层的层数设定与每层隐藏单元的个数设定 第一层的单元数即我们训练集的特征数量。 最后一层的单元数是我们训练集的结果的类的数量。 普遍地，我们会设定隐藏层的层数为 1 层。 如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。 一般来讲，每个隐藏层所包含的单元数量还应该和输入 $x$ 的维度相匹配，也要和特征的数目相匹配。 可能隐藏单元的单元数量和输入特征的数量相同，一般来说，隐藏单元的数目取为稍大于输入特征数目。 关于神经网络的使用过程总结 参数的随机初始化 利用正向传播方法计算所有的$h_{\theta}(x)$ 编写计算代价函数 $J$ 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数 ps 代价函数 $J$ 度量的就是这个神经网络对训练数据的拟合情况。 BP算法的基本思想是，学习过程由信号的正向传播与误差的反向传播两个过程组成。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>代价函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Hexo+Github搭建个人博客(详细)]]></title>
    <url>%2Fposts%2F54972%2F</url>
    <content type="text"><![CDATA[搭建环境准备： 下载Node.js 安装Git 拥有github账号 下载完成后（全部按NEXT就好），按下WIN+R，调出运行窗口，打cmd回车进入命令行，验证node和Git是否安装正确，输入下面指令: 123node -vnpm -vgit --version 如果都安装成功就会显示对应的软件版本号。 安装Hexo进去你要放置博客文件夹的目录，点击鼠标右键，点击 Git Bash Here，然后依次输入并执行下面的代码。 12345$ npm install hexo-cli -g $ hexo init Hexo$ cd Hexo$ npm install$ hexo server 第一行是安装 hexo 扩展插件。 第二行是创建一个为 Hexo 的文件夹，我们要把 hexo 相应的代码下载到该文件中下。 第三行进入到新创建的文件夹内。 第四行是安装 hexo 相关的代码。 第五行启动本地服务，启动完成后，在浏览器输入 http://localhost:4000/ 就可以访问刚刚创建的博客了。 如果报错: 执行 npm cache clean --force 清缓存再安装 Hexo目录里会多出很多文件，此时的目录情况如下: 123456node_modules npm 文件缓存目录scaffolds 文夹件下存放文章和页面模版scource 文夹件下存放资源文件themes 文夹件下存放主题文件package.json 站点版本和站点所需的依赖文件_config.yml 站点配置文件 hexo 常用命令 ： 12345678$ hexo g 生成静态文件$ hexo s 启动本地服务$ hexo d 提交到远程仓库$ hexo n page 创建页面 $ hexo n &quot;&quot; 创建文章$ hexo d -g 生成静态并提交到远程仓库$ hexo s -g 生成静态文件并启动本地预览$ hexo clean 清除本地 public 文件 外网访问第一步 - 创建仓库在 Github 创建一个名字为 username.github.io 的仓库，这里的 username 必须是你的github用户名。 第二步 - 修改配置注意所有的命令都在git-bash里敲! 注意所有的命令都在git-bash里敲! 注意所有的命令都在git-bash里敲! 注意所有的配置冒号后都要有空格! 注意所有的配置冒号后都要有空格! 注意所有的配置冒号后都要有空格! 修改depoly ，这里的 username 仍然是你的 Github 用户名： 1234deploy: type: git repo: git@github.com:username/username.github.io.git // 后面对应的是仓库链接 branch: master 示例： 1234deploy: type: git repo: git@github.com:BreezeDawn/BreezeDawn.github.io.git branch: master 修改 site 相关信息 ： 1234567title: xxsubtitle: description: 中文最好用编辑器不要用记事本打开keywords:author: xxlanguage: zh-Hanstimezone: 注 ：网站名称（title），作者 (author)，语言 (language)，签名(description) 第三步 - 给本地 Git 添加 ssh免密登陆首先我们需要修改 Git 的全局配置 user.name 和user.email 12git config –-global user.name “xxxxxx” // 自己的 github 用户名git config –-global user.email “xxxxxx” // 自己的 github 里绑定的邮箱 在这里 user 不需要替换成自己…..只需要修改双引号里的内容 然后我们进入存放密匙的文件夹，检查本地是否有 SSH key 12$ cd ~/.ssh$ ls 如果 SSH key 存在，就会显示 id_rsa、id_rsa.pub、know_hosts 三个文件 。 没有的话我们就来创建 SSH key 1$ ssh-keygen -t rsa -C &quot;你的邮箱&quot; 然后点击回车，接着会让你输入文件名，点击回车直接忽略，再然后会让设置密码并确认密码，我们点击两次回车，直接把密码设置为空，不用输入 。然后你会看到一堆泡泡，说明密匙创建成功。 创建成功后，可以通过如下命令拷贝 SSH key 的内容 ： 1clip &lt; ~/.ssh/id_rsa.pub&quot; 你也可以手动打开~/.ssh/目录下的id_rsa.pub文件进行拷贝，所有内容一字不漏的拷贝! 然后我们打开 GitHub 点击右上角头像进入个人资料，点击Settings -&gt; 左边 SSH and GPG keys，然后点击 New SSH key，title随便填，把之前拷贝的内容粘贴到 key 里面，然后点击 Add SSH key。 怎么去验证是否已经添加成功了呢 ？通过如下命令 ： 1$ ssh -T git@github.com 验证成功，你会看到 successfully !! 但是我们还差一步~ 第四步 - 更新静态文件，提交到 github 仓库执行 $ npm install hexo-deployer-git --save - 安装关联 Github 的插件 执行 hexo d -g - 更新静态文件并提交到你的 github 仓库。 然后使用浏览器打开 https://username.github.io.git ，是不是可以外网访问了呢? 记住一定是https的哦，不然访问不到 更换主题下载主题如果不喜欢现在的主题，我们可以在 github 中搜索 hexo theme 寻找自己喜欢的主题。 我用的是人气最高的 Next 主题，它提供非常详尽的官方文档，并且支持很多第三方插件，十分的友好。 Next Github：https://github.com/iissnan/hexo-theme-next/ Next 官方文档：http://theme-next.iissnan.com/ 我们现在通过 git 方式下载 Next，命令如下 ： 12$ cd themes$ git clone https://github.com/iissnan/hexo-theme-next next hexo 的主题文件都放在 themes 文件夹下，所以我们要进入主题文件夹 下下载 Next。 下载完成后，我的博客 themes 下就多了一个 next 文件夹。 配置主题首先我们要区分两个文件。 第一个是我们网站的 站点配置文件 _config.yml，它在我们的博客根目录 Hexo/下，Hexo 为 hexo init 初始化时自动创建的文件夹名称。 第二个是我们网站的 主题配置文件 _config.yml，它在我们的主题目录 Hexo/themes/next下。 然后我们修改 站点配置文件 : 1theme : next 注 ：把默认主题 landscape 切换成 next。 此时我们的博客主题已经修改为 Next 主题，但 Next 主题其实有四个风格，Muse、Mist、Pisces、Gemini，且这里默认为 Muse。 如果我们想要修改，就需要打开 主题配置文件， 修改 Schemes ： 1234scheme: Muse#scheme: Mist#scheme: Pisces#scheme: Gemini 根据自己的选择进行注释。 修改完毕后我们需要把静态文件按照新的配置重新生成，还记得重新生成静态文件的命令吗? 1$ hexo g 然后我们就可以提交给远程仓库了 1$ hexo d 完结接下来我们就可以在 github 上看见我们提交的静态文件了，也可以通过 https://username.github.io/ 访问我们的博客了，username 改成你的github 用户名，一定要记得是https!! 如果你想发布文章、生成新页面、增加搜索或其他功能，在 Next 官方文档中你都能找到~ 再放一下官方文档： Next 官方文档：http://theme-next.iissnan.com/ 当然还有一些不在官方文档上的骚操作，等我更新吧… 如果在进行操作时有 bug ，欢迎留言~~]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习-神经网络(一)]]></title>
    <url>%2Fposts%2F6267%2F</url>
    <content type="text"><![CDATA[为什么要使用神经网络当特征太多时，计算的负荷会特别大，而普通的线性回归/逻辑回归都无法有效地处理这么多的特征，这个时候我们需要神经网络。 神经网络的模型表示首先，我们为神经网络里的每一层都增加了一个偏差单元，即每一层的0号下标的单元，它的值永远为1，而偏差单元我们只在当作输入时使用。 这时，我们把输入的样本特征$x_0x_1x_2x_3$看作第一层输入，$a_1a_2a_3$看作第一层的输出，把$a_0a_1a_2a_3$看作第二层的输入，$h_\theta(x)$看作第二层的输出。 PS：输出的意思就是经过一个激励函数$g(z)$的运算得出的，这里：$$g(z)=\frac{1}{1+e^{-z}}$$ 输出怎么得到从线性回归中我们能够知道：$h_\theta(x) = \theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3 = y$ 如果我们想要使用相同的输入 $x_0x_1x_2x_3$ 得出一个不一样的 $y_1$ ，我们必须要改变 $\theta$的值使得与第一次运算的 $\theta$ 不一样。 同理，当我们把输入的样本特征 $x_0x_1x_2x_3$ 看作第一层输入时，我们就需要三组不同的 $\theta$ 值，使得经过激励函数后得到三个不同的值 $a_1a_2a_3$。因此，我们就有了关于第一层输入的 $\theta$ 矩阵 $\Theta^{(1)}$，它的尺寸为 3*4。那么第二层输入的 $\theta$ 矩阵 $\theta^{(2)}$ 的尺寸则为 1*4。当然我们也可能会有许多次输入输出，如果我们进行多次的输入输出，$a_{i}^{\left( j \right)}$ 则代表第 $j$ 层的第 $i$ 个激活单元(输入)。${\theta }^{\left( j \right)}$代表从第 $j$ 层映射到第 $ j+1$ 层时的权重的矩阵，例如 ${\theta }^{\left( 1 \right)}$ 代表从第一层映射(输出)到第二层的权重的矩阵。其尺寸为：以第 $j+1$层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵。 输入-输出的过程因为 $a_1a_2a_3$ 是样本特征 $x_0x_1x_2x_3$ 与 $\Theta^{(1)}$ 经过激励函数后得到的值，因此此过程可写为： $a_{1}^{2}=g(\Theta_{10}^{1}x_{0}+\Theta_{11}^{1}x_{1}+\Theta_{12}^{1}x_{2}+\Theta_{13}^{1}x_{3})$ $a_{2}^{2}=g(\Theta_{20}^{1}x_{0}+\Theta_{21}^{1}x_{1}+\Theta_{22}^{1}x_{2}+\Theta_{23}^{1}x_{3})$ $a_{3}^{2}=g(\Theta_{30}^{1}x_{0}+\Theta_{31}^{1}x_{1}+\Theta_{32}^{1}x_{2}+\Theta_{33}^{1}x_{3})$ $h_\Theta(x)=g(\Theta_{10}^{2}a_{0}^{2}+\Theta_{11}^{2}a_{1}^{2}+\Theta_{12}^{2}a_{2}^{2}+\Theta_{13}^{2}a_{3}^{2})$ 上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。 我们可以知道：每一个 $a$ 都是由上一层所有的$x$和每一个$x$所对应的决定的。 （我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )） 把$x$, $\theta$, $a$ 分别用矩阵表示： 我们可以得到 $g(\theta \cdot X)=a$ 。 如果细分下去，我们就能够得到向量化的结果：$$g(\Theta^{(1)}\cdot{X^T})=a^{(2)}$$ 即： 以上是以第一层为例进行的说明，那么现在我们看第二层，则有$$g(\Theta^{(2)}\cdot a^{(2)})=h_\theta(x)$$我们令 ${z}^\left( 3 \right)={\theta }^{( 2 )}{a}^{( 2 )}$，则 $h_\theta(x)={a}^{\left(3\right)}=g({z}^{\left(3\right)})$。 更好的理解 其实神经网络就像是logistic regression，只不过我们把logistic regression中的输入向量$[ x_1\sim {x_3} ]$ 变成了中间层的$[ a_1^{(2)}\sim a_3^{(2)} ]$, 即: $h_\theta(x)=g( \Theta_0^{ 2 }a_0^{ 2 }+\Theta_1^{ 2 }a_1^{ 2 }+\Theta_{2}^{ 2 }a_{2}^{ 2 }+\Theta_{3}^{ 2 }a_{3}^{ 2 } )$ 我们可以把 $a_0, a_1, a_2, a_3$ 看成更为高级的特征值，也就是 $x_0, x_1, x_2, x_3$ 的进化体，并且它们是由 $x$与$\theta$决定的，因为是梯度下降的，所以 $a$ 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 $x$次方厉害，也能更好的预测新数据。这就是神经网络相比于逻辑回归和线性回归的优势。从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征 $x_1,x_2,…,{x}_{n}$ ，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 单层神经元计算的简化理解神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。当输入特征为布尔值（0或1）时，我们可以用一个单一的激活层可以作为二元逻辑运算符，为了表示不同的运算符，我们只需要选择不同的权重即可。 在理解之前，我们再复习一下关于激励函数与判定边界。当$g(z)$中 $z &gt; 0$ 时，$g(z)$ &gt; 0.5 ，假如我们的阈值就是 0.5，那么我们此时就把 $g(z)$ 的结果归为 1 ，反之则为 0。 逻辑与那么，如果我们现在有输出函数 $h_\theta(x) = g(\theta_0x_0+\theta_1x_1+\theta_2x_2)$，且 $\Theta = [-30,20,20]$，此时 $h_\theta(x) = g(-30+20x_1+20x_2)$，我们就能够得到$x_1x_2$分别取值时的结果对照表： 经过复习和上表的对照我们能够很轻松的理解。 此时$h_\theta(x)$ 得出的结果 等于 $x_1 AND x_2$ 得出的结果，因此我们就能够把此时的输出函数中进行的计算简化理解为：对输入做 逻辑与(AND)​ 运算。 逻辑或而当 $\Theta = [-10,20,20]$ 时，此时 $h_\theta(x) = g(-10+20x_1+20x_2)$ ，我们就能够得到$x_1x_2$分别取值时的结果对照表： 此时$h_\theta(x)$ 得出的结果 等于 $x_1 OR x_2$ 得出的结果，因此我们就能够把此时的输出函数中进行的计算简化理解为：对输入做 逻辑或(OR) 运算。 逻辑非当 $\Theta = [10,-20,0]$ 时，此时 $h_\theta(x) = g(10-20x_1)$ ，我们就能够知道$x_1$取 0 时，结果为 1，$x_1$取 1 时，结果为 0 。我们就能够把此时的神经元的作用等同于 逻辑非(NOT) XNOR(输入两个值相等时，结果为 1 )那么我们如何表示呢? 首先我们构造一个能表达 $(NOT x_1) AND (NOTx_2)$ 的神经元进行第一层计算，如图： 然后构造一个能表示 OR 的神经元进行第二层计算，再让这两层组合在一起： 这样我们就得到了一个能实现 $\text{XNOR}$ 功能的神经网络。 因此，我们能够组合三种简单的运算来逐渐构造出复杂的函数，这样我们也能得到更加复杂有趣的特征值。 这就是神经网络的厉害之处。 多类处理的神经网络我们在上面的神经网络仅仅输出了一个结果，也就是我们只做了二分类问题，那么我们如果想要进行多个类的分类呢? 很简单，我们只需要让结果值的数目等于你要分类的类数目。比如说：如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车，第三个值用于预测是否是摩托车，第四个值用于判断是否为卡车。 如果我们构造两个中间层进行，输出层4个神经元分别用来表示4类，那么神经网络图可能如下所示： 我们希望当输入人的图片时，输出的结果为 [1 , 0, 0, 0]，输入卡车图片时，输出的结果为 [0 , 0, 0, 1]。 这样，我们就实现了神经网络的多分类处理。 查看我的CSDN: https://blog.csdn.net/qq_28827635]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-正则化]]></title>
    <url>%2Fposts%2F1179%2F</url>
    <content type="text"><![CDATA[1. 正则化它可以改善或者减少过度拟合问题 2. 欠拟合(模型的高偏差)欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或者模型没有很好地捕捉到数据特征，不能够很好地拟合数据。 3. 过拟合(模型的高方差)为什么出现过拟合 特征过多 训练集数据较少 模型复杂 对过拟合的理解如果我们拟合一个高阶多项式，那么这个函数能很好的拟合训练集能拟合几乎所有的训练数据，这就面临可能函数太过庞大的问题，即变量太多。同时如果我们没有足够的数据去约束这个变量过多的模型，就会出现过度拟合的情况。虽然训练出的模型能够很好的拟合训练集的样本数据，但很有可能无法泛化新样本。 如何解决过拟合 尽量减少选取特征数量。后面会学到模型选择算法 ，它能够自动选择采用哪些特征变量，且自动舍弃不需要的变量 正则化保留所有特征变量，但是减少参数$\theta_j$的大小 4. 怎么应用正则化思想与做法修改代价函数，从而收缩(惩罚)所有的参数值，因为我们并不知道具体的去收缩(惩罚)哪些参数， 修改后的代价函数如下：$$J\left(\theta\right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{({h_\theta}({x}^{(i)})-{y}^{(i)})^{2}+\lambda \sum\limits_{j=1}^{n}\theta_{j}^{2}]}$$$\lambda $又称为正则化参数（Regularization Parameter），它能够平衡代价函数，使$\theta_j$尽可能的小。 注：根据惯例，我们的$j$是从1开始的，也就是我们不对${\theta_{0}}$ 进行惩罚。 举一个例子我们看这个假设函数: $h_\theta\left( x \right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^2+\theta_{3}x_{3}^3+\theta_{4}x_{4}^4$ 。通常地，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。于是我们将修改代价函数，在其中${\theta_{3}}$和${\theta_{4}}$ 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的${\theta_{3}}$和${\theta_{4}}$。修改后如下 :$$\underset{\theta}{\mathop\min }\,\frac{1}{2m}[\sum\limits_{i=1}^{m}{\left({h}_{\theta }\left( {x}^{(i)} \right)-{y}^{(i)} \right)^{2}+1000\theta _{3}^{2}+10000\theta _{4}^{2}]}$$但是正是因为我们并不知道具体的哪一个$\theta$是高次项，因此我们只能去收缩(惩罚)所有参数。 但是如果我们令 $\lambda$ 的值很大的话，那么$\theta $（不包括${\theta_{0}}$）都会趋近于0，这样我们所得到的只能是一条平行于$x$轴的直线。所以对于正则化，我们要取一个合理的 $\lambda$ 的值，这样才能更好的应用正则化。 5. 正则化线性回归正则化代价函数$$J\left(\theta\right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[(({h_\theta}({x}^{(i)})-{y}^{(i)})}^{2}+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2})]}$$ 正则化梯度下降要使梯度下降法令正则化后的线性回归代价函数最小化，因为我们没有对$\theta_0$进行正则化，所以梯度下降算法有两种情形：$${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}(({h_\theta}({x}^{(i)})-{y}^{(i)})x_{0}^{(i)})$$ $${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left(i\right)}+\frac{\lambda }{m}{\theta_j}]$$ 对第二个式子进行变化后，可得:$${\theta_j}:={\theta_j}(1-a\frac{\lambda }{m})-a\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left(i\right)}$$可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta $值减少了一个额外的值。 PS：梯度下降仍然是对$J(\theta)$进行最小化，通过求导，得出梯度下降算法 正则化正规方程 注：图中的矩阵尺寸为 $(n+1)*(n+1)$。 值得一提的是，哪怕此时$X$不可逆，经过$\lambda$相加变化后的矩阵将是可逆的。 6. 正则化逻辑回归正则化代价函数$$J\left(\theta\right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{y}^{(i)}\log \left({h_\theta}\left({x}^{(i)}\right)\right)-\left(1-{y}^{(i)} \right)\log\left(1-{h_\theta}\left({x}^{(i)}\right) \right)]}+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta _{j}^{2}$$ ​ ps：注意这里$\lambda$的仍为$\frac{1}{2m}$ 正则化梯度下降类似地，因为我们没有对$\theta_0$进行正则化，所以梯度下降算法有两种情形：$${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}(({h_\theta}({x}^{(i)})-{y}^{(i)})x_{0}^{(i)})$$ $${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left( i \right)}+\frac{\lambda }{m}{\theta_j}]$$ 看起来和线性回归的一模一样，实际上我们知道这里 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$，所以与线性回归不同。 PS：值得注意的是，${\theta_{0}}$仍然不参与其中的任何一个正则化。 注:泛化能力（generalization ability）泛化能力是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的模型也能给出合适的输出，该能力称为泛化能力。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-逻辑回归]]></title>
    <url>%2Fposts%2F12210%2F</url>
    <content type="text"><![CDATA[什么是逻辑回归逻辑回归算法是分类算法，可能它的名字里出现了“回归”让我们以为它属于回归问题，但逻辑回归算法实际上是一种分类算法，它主要处理当 $y$ 取值离散的情况，如：1 0 。 为什么不使用线性回归算法处理分类问题假设我们遇到的问题为 二分类问题，那么我们可能将结果分为负向类和正向类，即$y\in0,1$ ，其中 0 表示负向类，1 表示正向类。如果我们使用线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，但是我们需要的假设函数输出值需要在0到 1 之间，因此我们需要用到逻辑回归算法。 逻辑回归的假设函数与理解逻辑回归的假设函数 sigmoid function 表示方法 :$$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$$ 理解记忆:其实里面的$\theta_Tx$就是线性回归时的假设函数 h(x) ，$$h(x) = \theta^Tx = \sum_{j=0}^{n}{\theta_jx_j}$$而逻辑回归的假设函数其实就是将线性回归的表达式 h(x) 以 z 的形式代入到了 S 型函数(sigmoid function) 中 :$$g(h(x)) = g(z) = \frac{1}{1+e^{-z}}$$ps: 这里我们用$h(x)$表示的是线性回归的假设函数，之后的$h$都将表示 逻辑回归的假设函数。值得一提的是S型函数和我们的假设函数没关系，它只是一个输出值在0~1之间的函数，仅此而已。我们做的只是把之前得到的线性回归假设函数给代入进去形成逻辑回归的假设函数，这样 对假设函数的解释 :给定 x ，根据选择的参数计算出y = 1 的概率 ，具体的概率公式如下 :$$h_\theta(x) = P(y=1|x; \theta)$$ Sigmoid - Python:1234import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z)) 判定边界(decision boundary)如何得出判定边界 :在 逻辑回归的假设函数中，但凡输出结果 $h_\theta(x)$大于 0.5 的，我们都将预测结果 $y$ 收敛于 1 ；小于 0.5 的，收敛于 0 ；而恰好等于 0.5 的，收敛1 或 0 都可以，我们可以自己设定它如何收敛。由此，我们的输出值就都在 0 到 1 之间了。而当 $h_\theta(x)$ 大于 0.5 时，$\theta^Tx$ 大于 0.5， $h_\theta(x)$ 小于 0.5 时，$\theta^Tx$ 小于 0.5， $h_\theta(x)$ 等于 0.5 时，$\theta^Tx$ 等于 0.5。当然，具体的阈值是可以调整的，比如说你是一个比较保守的人，可能将阈值设为 0.9 ，也就是说有超过 90% 的把握，才相信这个$y$收敛于 1 。 由此，我们能够绘制出判定边界 :$$\theta^Tx = 0$$ 关于判定边界 : 决策边界不是训练集的属性，而是假设本身及其参数的属性 只要给出确定的参数$\theta$，就确定了我们的决策边界 高阶多项式(多个特征变量)能够让我们得到更复杂的决策边界 逻辑回归的代价函数，梯度下降自动拟合$\theta$，以及代价函数的推导过程逻辑回归的代价函数 :$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})$$其中$Cost$ :$$Cost(h_\theta(x),y) = -ylog(h_\theta(x)) - (1-y)log(1-h_\theta(x))$$因此$J(\theta)$ :$$J(\theta) =-\frac{1}{m}[\sum_{i=1}^{m} y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$ 使用对数几率的原因: 代价函数 $J(\theta)$ 会是一个凸函数，并且没有局部最优值。否则我们的代价函数将是一个非凸函数。 逻辑回归的梯度下降算法 :Repeat {$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$$(simultaneously update all )} 求导后得到： Repeat {$$\theta_j := \theta_j - \alpha\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$ (simultaneously update all )} ps : 逻辑回归梯度下降结果与线性回归梯度下降结果一致，但其中的$h_\theta(x)$并不一样，因此本质上是不同的。 关于特征缩放和均值归一化:思想:在有多个特征的情况下，如果你能确保这些不同的特征都处在一个相近的范围，这样梯度下降法就能更快地收敛。使代价函数$J(θ)$的轮廓图的形状就会变得更圆一些。 做法:一般地，我们执行特征缩放时，我们通常将特征的取值约束到接近−1到+1的范围。其中，特征x0总是等于1，因此这已经是在这个范围内了，但对于其他的特征，我们需要通过除以不同的数来让它们处于同一范围内。除了在特征缩放中将特征除以最大值以外，有时候我们也会进行一个称为均值归一化的操作:$$x_n = \frac{x_n-μ_n}{s_n}$$其中，$μ_n$是平均值，$s_n$是标准差 好处: 更好的进行梯度下降，提高代价函数的收敛速度 提高代价函数求解的精度 更适合解决大型机器学习的问题​ 其他相较于梯度下降算法更好的的令代价函数最小的算法(高级优化[超纲])常用算法: 共轭梯度(Conjugate Gradient) 局部优化法(BFGS - Broyden fletcher goldfarb shann) 有限内存局部优化法(LBFGS) 好处: 这些算法内部有一个智能的内部循环(线性搜索算法)，能够尝试不同的 $\alpha​$ 并自动的选择一个好的学习速率 $\alpha​$ ，这样就不需要手动选择 $\alpha​$ 收敛速度通常比梯度下降算法更快速 缺点: 比梯度下降算法更加复杂 使用逻辑回归算法解决多类别问题思想:将多分类问题拆分成多个二分类问题并得出多个模型。最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 做法:我们将多个类中的一个类标记为正向类（$y=1$），然后将其他所有类都标记为负向类，这个模型记作$h_\theta^{\left( 1 \right)}\left( x \right)$。接着，类似地我们选择另一个类标记为正向类（$y=2$），再将其它类都标记为负向类，将这个模型记作 $h_\theta^{\left( 2 \right)}\left( x \right)$,依此类推。最后我们得到一系列的模型简记为： $h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)$其中：$i=\left( 1,2,3….k \right)$ 。然后我们将这多个逻辑回归分类器进行训练并得出最终模型：$h_\theta^{\left( i \right)}\left( x \right)$， 其中 $i$ 对应每一个可能的 $y=i$，最后，当我们需要进行预测时，输入一个新的 $x$ 值，我们要做的就是在这多个分类器里面输入 $x$，然后在多个分类器得出的结果中，选出一个最大的$ i$，即$\mathop{\max}\limits_i\,h_\theta^{\left( i \right)}\left( x \right)$。 逻辑回归梯度下降中代价函数求导过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-向量化]]></title>
    <url>%2Fposts%2F16142%2F</url>
    <content type="text"><![CDATA[向量化 - 传统累加运算 - 代码实现:1234567891011121314151617181920import timeimport numpy as np# 定义两组向量vector1 = np.random.rand(100000)vector2 = np.random.rand(100000)# 使用向量化start_time = time.time() # 开始时间res = np.dot(vector1, vector2) # 向量直接相乘得到最终结果end_time = time.time() # 结束时间print("Vectorized: " + str((end_time - start_time)*1000) + "ms" + " res =" + str(res))# 使用for循环res = 0start_time = time.time() # 开始时间for i in range(100000): # 传统的累加运算,需要累加100000次 res += vector1[i] * vector2[i]end_time = time.time() # 结束时间print("For loop: " + str((end_time - start_time)*1000) + "ms" + " res =" + str(res)) 结果对比:12Vectorized :1.0001659393310547ms res =24969.775960643143For loop:79.94818687438965ms res =24969.775960642968 ​ 从执行结果来看向量化的运算速度要比非向量化的运算快了近80倍，而这个对比结果还会随着运算集的数目增加而增加。 为什么:​ CPU 与 GPU 都能够使用 SIMD 指令进行并行化操作，即以同步方式，在同一时间内执行同一条指令。一般来讲可扩展的深度学习都在 GPU 上做，但其实 CPU 也不是太差，只是没有 GPU 擅长。 ​ 而 Python 的 numpy 的一些内置函数能够充分利用并行化来加速运算，比如 np.dot，因此，不到逼不得已，还是不要使用 for 循环吧 注:​ GPU - 图形处理器也，叫做图像处理单元，显卡的处理器。与 CPU 类似，只不过 GPU 是专为执行复杂的数学和几何计算而设计的，这些计算是图形渲染所必需的。​ SIMD - 单指令多数据流，以同步方式，在同一时间内执行同一条指令。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vscode-插件]]></title>
    <url>%2Fposts%2F57651%2F</url>
    <content type="text"><![CDATA[常用插件 Auto Close TagAuto Rename TagBeautifyBracket Pair ColorizerColor HighlightColor PickerHtml Css SupportHtml SnippetsJavaScript(ES6)code snippetsjQuery Code SnippetsMaterial Icon Themenpmopen in browserOutput ColorizerPreview on Web ServerVeturView In Browser]]></content>
      <categories>
        <category>Vscode</category>
      </categories>
      <tags>
        <tag>Vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[装机必备（官网链接）]]></title>
    <url>%2Fposts%2F48776%2F</url>
    <content type="text"><![CDATA[装机必备 编程 Anaconda https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ Git https://git-scm.com/downloads node https://nodejs.org/zh-cn/ Postman https://www.getpostman.com/apps pycharm http://www.jetbrains.com/pycharm/download/ python-2 and python-3 https://www.python.org/ VSCode https://code.visualstudio.com/Download VSCode常用插件：http://xingtu.info/posts/57651/ 解压 BANDIZIP http://www.bandisoft.com/bandizip/ 浏览器 Chrome https://www.google.cn/chrome/ Twinkstar https://www.twinkstar.com/ 本地文件搜索 Everything http://www.voidtools.com/ 播放器 PotPlayer64 http://potplayer.daum.net/ 即时通讯 tim http://office.qq.com/download.html WeChat https://weixin.qq.com/cgi-bin/readtemplate?t=win_weixin 文本编辑器 typora https://www.typora.io/ WPS http://www.wps.cn/ 词典 YoudaoDict http://cidian.youdao.com/ 必应 https://cn.bing.com/dict/ 防火墙/杀毒/告别360/告别腾讯管家/简洁/强大 火绒 https://www.huorong.cn/ 工具截图 Snipaste https://zh.snipaste.com/download.html 录屏 Faststone Capture (腾讯软件中心下载链接) https://pc.qq.com/detail/0/detail_720.html 格式转换 FormatFactory（格式工厂） http://www.pcfreetime.com/formatfactory/CN/index.html 文件存储 BaiduNetdisk （百度云） https://pan.baidu.com/download 翻墙 Chromium （自带vpn的Chrome） https://github.com/chromium/chromium lantern https://github.com/getlantern/download 驱动 驱动精灵 http://www.drivergenius.com/ 鲁大师 http://www.ludashi.com/ 下载 fdm （类似迅雷/不限速） https://www.freedownloadmanager.org/download.htm JiJiDown （哔哩哔哩下载器） http://client.jijidown.com/ 软件整理 小Q书桌 (腾讯软件中心下载链接) https://pc.qq.com/detail/5/detail_6105.html 编程工具 sublime http://www.sublimetext.com/3 mongodb https://www.mongodb.com/download-center/community mysql https://dev.mysql.com/downloads/installer/ 思维导图 XMind https://www.xmind.net/download/ 幕布 https://mubu.com/apps 输入法 搜狗输入法 https://pinyin.sogou.com/ 系统包 ALI213-Microsoft.Visual.C++.2015.Redistributable.Package.x86.x64 （第三方链接不保证无毒） http://patch.ali213.net/showpatch/52575.html flashplayer http://soft.jxsgdsmb.cn/1/3889.html?tab=42844 Microsoft Visual C++ Build Tools （全文搜索：Visual C++ Build Tools 2015） https://blogs.msdn.microsoft.com/pythonengineering/2016/04/11/unable-to-find-vcvarsall-bat/ vcredist_x64 （CSDN下载） https://download.csdn.net/download/similing/9873980 游戏 Steam https://store.steampowered.com/about/ wegame https://www.wegame.com/]]></content>
      <categories>
        <category>经验</category>
      </categories>
      <tags>
        <tag>经验</tag>
        <tag>装机必备</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[醒途-觉醒之路]]></title>
    <url>%2Fposts%2F44353%2F</url>
    <content type="text"><![CDATA[碎碎念 1.耳濡目染关注科班圈大牛圈,一是接触到了,就更能明白自己和大牛之间的差距,认清自己哪些地方欠缺,哪些地方不足,就更能够补上短板.该学什么,怎么学,都有一个清晰的认识.再就是看得多了,更能补全三观,近朱者赤近墨者黑,耳濡目染的多了,思想与认识自然就更开阔了.眼界与认知自然就更深了. 2.忌固化宜变通即使是固化的事情,也要主动思考是否可变通.不要把思维定住,不要认为教的都是不可以变的,不要既定思维,要活泛. 3.优秀与否不重要,学会自信优秀与不优秀,不要妄自菲薄,不要自我否定,人外有人,天外有天,你的人外是有别人,但可能你也属于别人的人外人,如果你觉得自己现在很差,那么意思就是你这么久的努力就这样轻而易举的被自己否定了.当你觉得自己不优秀时,你还怎么让别人觉得你优秀呢?从一知半解开始到现在,你做的已经很好了,相比于从前.所以说,请照一照镜子,看一看自己,你,很优秀,并且会愈来愈优秀.加油. 4.费曼技巧费曼技巧有四个简单的步骤： 选择一个概念 把它教给完全不懂的另外一个人 如果卡壳，回到原始材料 回顾后将语言条理化，简化。 检测知识最终的途径是你能有能力把它传播给另一个人。 那些声称清楚自己所想，但却不能清晰表达的人，其实通常不知道自己所想。— Mortimer Adler python奇淫技巧1.字典中value最大值对应的键123print(max(word_num, key=word_num.get))print(max(zip(word_num.values(),word_num.keys()))) 2.map1234567891011map()函数接收两个参数，一个是函数，一个是序列，map将传入的函数依次作用到序列的每个元素，并把结果作为新的list返回。 def add_one(x): return x+1for i in map(add_one,[1,2,3,4,5]): print(i) 输出:2 3 4 5 6由于list包含的元素可以是任何类型，因此，map() 不仅仅可以处理只包含数值的 list，事实上它可以处理包含任意类型的 list，只要传入的函数f可以处理这种数据类型。 3.列表去重,顺序不变123l = list(set(list))l.sort(key=list.index) 4.sorted()123456789101112131415通常我们得到一个列表排序后产生的新列表很容易这么写: new_ls = ls.sort()但是sort方法并没有返回值,因此new_ls的值为NONE.虽然ls.sort()之后ls已经排序成功,但我们经常并不希望原列表发生改变,这时我们就用到了sorted. 输入:ls=[3,1,2,4] ls.sort() 输出:ls:[1,2,3,4]Python的一个内置函数，使用方法与list.sort()大体一致，不同在于两个地方: 1.sorted(list)返回一个排序后的list，不改变原始的list. sort()是对原始的L进行操作，调用后原始的L会改变，没有返回值。 2.sorted()适用于任何可迭代容器. list.sort()仅支持list（sort()本身就是list的一个方法）基于以上两点，sorted使用频率比list.sort()更高些，所以Python中更高级的排序技巧便是 sorted().PS:如果想按照倒序排序的话，则只要将reverse置为true即可。 5.split默认切割123字符串的切割split()方法中如果没有参数,默认以空格分隔字符串。输入: '2018 08 04'.split()输出: ['2018','08','04'] 6.num进制的数字字符串 –&gt; 整型1int(str,num) # num表示该字符串内的数字为num进制 7.format 格式化123456789101112131415161718192021222324252627282930313233type 【可选】格式化类型 •传入” 字符串类型 “的参数 •s，格式化字符串类型数据•空白，未指定类型，则默认是None，同s •传入“ 整数类型 ”的参数 •b，将10进制整数自动转换成2进制表示然后格式化 •c，将10进制整数自动转换为其对应的unicode字符 •d，十进制整数 •o，将10进制整数自动转换成8进制表示然后格式化； •x，将10进制整数自动转换成16进制表示然后格式化（小写x） •X，将10进制整数自动转换成16进制表示然后格式化（大写X） •传入“ 浮点型或小数类型 ”的参数 •e， 转换为科学计数法（小写e）表示，然后格式化； •E， 转换为科学计数法（大写E）表示，然后格式化; •f ， 转换为浮点型（默认小数点后保留6位）表示，然后格式化； •F， 转换为浮点型（默认小数点后保留6位）表示，然后格式化； •g， 自动在e和f中切换 •G， 自动在E和F中切换 •%，显示百分比（默认显示小数点后6位） a = &quot;i am &#123;&#125;,age &#123;&#125;&quot;.format(&quot;seven&quot;,18,&quot;alex&quot;)b = &quot;i am &#123;&#125;,age &#123;&#125;, &#123;&#125;&quot;.format(*[&quot;seven&quot;, 18 ,&quot;alex&quot;])c = &quot;i am &#123;0&#125;, age &#123;1&#125;, really &#123;0&#125;&quot;.format(&quot;seven&quot;, 18)d = &quot;i am &#123;0&#125;, age&#123;1&#125;, really &#123;0&#125;&quot;.format(*[&quot;seven&quot;, 18])e = &quot;i am &#123;name&#125;, age &#123;age&#125;, really &#123;name&#125;&quot;.format(name=&quot;seven&quot;, age = 18)f = &quot;i am &#123;name&#125;, age &#123;age&#125;, rally &#123;name&#125;&quot;.format(**&#123;&quot;name&quot;:&quot;seven&quot;, &quot;age&quot;:18&#125;)g = &quot;i am &#123;0[0]&#125;,age&#123;0[1]&#125;, really&#123;0[2]&#125;&quot;.format([1,2,3],[11,22,33])h = &quot;i am &#123;:s&#125;, age &#123;:d&#125;, money &#123;:f&#125;&quot;.format(&quot;seven&quot;, 18, 888.1)i = &quot;i am &#123;:s&#125;, age &#123;:d&#125;&quot;.format(*[&quot;seven&quot;, 18])j = &quot;i am &#123;name:s&#125;, age &#123;age:d&#125;&quot;.format(name=&quot;seven&quot;,age=18)k = &quot;i am &#123;name:s&#125;, age &#123;age:d&#125;&quot;.format(**&#123;&quot;name&quot;:&quot;seven&quot;,&quot;age&quot;:18&#125;)l = &quot;numers:&#123;:b&#125;,&#123;:o&#125;,&#123;:d&#125;,&#123;:x&#125;,&#123;:X&#125;,&#123;:%&#125;&quot;.format(15,15,15,15,15,15.32445,2)m = &quot;numbers:&#123;0:b&#125;,&#123;0:o&#125;,&#123;0:d&#125;,&#123;0:x&#125;,&#123;0:%&#125;&quot;.format(15)&quot;numbers: &#123;num:b&#125;,&#123;num:o&#125;,&#123;num:d&#125;,&#123;num:x&#125;,&#123;num:X&#125;, &#123;num:%&#125;&quot;.format(num=15) 8.httplib123httplib模块改名为http.client import http.clienthttplib是一个底层的 HTTP 协议客户端，被更高层的 urllib.request 模块所使用。 9.获取屏幕分辨率/鼠标位置1234567891011from win32api import GetSystemMetricsfrom win32api import GetCursorPos# 分辨率x = GetSystemMetrics(0)y = GetSystemMetrics(1)# 鼠标位置GetCursorPos()python3 安装 pypiwin32后使用 10.pyinstaller123456789101112131415-c 参数 使用控制台，无界面(默认)-w 参数 使用窗口，无控制台.如果程序里有使用到控制台(如print)的就不可以使用-w, 否则会报错 '''failed to excute script xxx''' 如果想要捕捉错误信息可以先用控制台捕捉,没有报错后再使用无控制台. -D 参数 创建一个目录，包含exe文件，但会依赖很多文件（默认选项）。-F 参数 打包成一个exe文件-p 多文件打包时,以-p [其他.py] 的形式跟在主文件后 '''如:pyinstaller -w -F main.py -p view.py -p other.py'''-i 参数 修改打包后的exe图标,图标应放在py同级目录下,需要是ico格式,只改后缀不可用. '''如:pyinstaller -w -F -i zzz.ico main.py -p view.py -p other.py''' 11.程序退出的两种方式123451.import os 'os退出会直接退出,不抛出异常' os._exit() 2.import sys 'sys退出会抛出异常,如果捕捉异常代码依旧执行' sys.exit() 12.PYQT12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class GUI(QMainWindow):def __init__(self): super().__init__() self.iniUI()def iniUI(self): self.move(510,347) self.setWindowTitle('天命') self.resize(362,250) # 设置状态消息栏文本 self.statusBar().showMessage("Mai") # 标签 self.label1 = QLabel('天命', ) # 给标签1设字体 self.label1.setFont(QFont("cour", 25, QFont.Bold)) # 按钮 self.pushButton1 = QPushButton('天灵灵地灵灵') self.pushButton1.setFont(QFont("cour", 18)) self.pushButton1.clicked.connect(self.God_will) # 创建一个网格布局对象 grid_layout = QGridLayout() # 在网格中添加窗口部件 grid_layout.addWidget(self.label1, 0, 0) # 放置在0行0列 grid_layout.addWidget(self.pushButton1, 2, 0) # 2行0列 # 对齐方式 grid_layout.setAlignment(Qt.AlignTop) grid_layout.setAlignment(self.label1, Qt.AlignCenter) grid_layout.setAlignment(self.pushButton1, Qt.AlignCenter) # 创建一个窗口对象 layout_widget = QWidget() # 设置窗口的布局层 layout_widget.setLayout(grid_layout) self.setCentralWidget(layout_widget) # 无边框 self.setWindowFlags(Qt.FramelessWindowHint) # esc退出def keyPressEvent(self, e): if e.key() == Qt.Key_Escape: self.close() # 无边框时候拖动窗口def mousePressEvent(self, event): if event.button() == Qt.LeftButton: self.m_flag = True self.m_Position = event.globalPos() - self.pos() # 获取鼠标相对窗口的位置 event.accept() self.setCursor(QCursor(Qt.OpenHandCursor)) # 更改鼠标图标def mouseMoveEvent(self, QMouseEvent): if Qt.LeftButton and self.m_flag: self.move(QMouseEvent.globalPos() - self.m_Position) # 更改窗口位置 QMouseEvent.accept()def mouseReleaseEvent(self, QMouseEvent): self.m_flag = False self.setCursor(QCursor(Qt.ArrowCursor)) 123456if __name__ == '__main__': app = QApplication(sys.argv) gui = GUI() gui.setWindowOpacity(0.8) gui.show() sys.exit(app.exec_()) 13.python requests接收chunked编码问题 (https://blog.csdn.net/wangzuxi/article/details/40377467)123456789101112131415161718192021IncompleteRead(4360 bytes read)', IncompleteRead(4360 bytes read))原因:接收第一个chunked正常，第二次时line为空，导致int转换时出异常解决方案：方案1、修改httplib.py第581行为： chunk_left = int(line, 16) if line else 0 方案2、自己的程序忽略这个异常；方案3、import http.client try: response = request.urlopen(req) except http.client.IncompleteRead as e: response = e.partial partial 会将error接收到的原文返回 方案4、用pycurl来代替requests，但必须将HTTP协议版本设置为1.0，否则与方案2无差别，因为 Transfer-Encoding:chunked ， Connection:keep-alive 都是HTTP1.1的新特性，如果将 自己的HTTP协议版本设置为1.0，那么服务端将不会再返回chunked，而是以TCP分段的方式 直接返回整个文件内容，最后重组成一个完整的HTTP包。 14.mysql插入数据时,id字段不连续,数据也不连续12原因: id是唯一索引，冲突后id自增1,所以自增字段id不连续是正常的,中间的 空号很多且数据不连续,说明包含该插入语句的事务不成功的比例很多。 15.anaconda使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950查看所有的 packages：conda list如果你记不清 package 的具体名称，也可以进行模糊查询：conda search search_term如何管理Python环境？默认的环境是 root，你也可以创建一个新环境：conda create -n env_name list of packages其中 -n 代表 name，env_name 是需要创建的环境名称，list of packages 则是列出在新环境中需要安装的工具包。例如，当我安装了 Python3 版本的 Anaconda 后，默认的 root 环境自然是 Python3，但是我还需要创建一个 Python 2 的环境来运行旧版本的 Python 代码，最好还安装了 pandas 包，于是我们运行以下命令来创建：conda create -n py2 python=2.7 pandas 细心的你一定会发现，py2 环境中不仅安装了 pandas，还安装了 numpy 等一系列 packages，这就是使用 conda 的方便之处，它会自动为你安装相应的依赖包，而不需要你一个个手动安装。进入名为 env_name 的环境：source activate env_name退出当前环境：source deactivate另外注意，在 Windows 系统中，使用 activate env_name 和 deactivate 来进入和退出某个环境。删除名为 env_name 的环境：conda env remove -n env_name显示所有的环境：conda env list当分享代码的时候，同时也需要将运行环境分享给大家，执行如下命令可以将当前环境下的 package 信息存入名为 environment 的 YAML 文件中。conda env export &gt; environment.yaml同样，当执行他人的代码时，也需要配置相应的环境。这时你可以用对方分享的 YAML 文件来创建一摸一样的运行环境。conda env create -f environment.yaml至此，你已跨入 Anaconda 的大门，后续就可以徜徉在 Python 的海洋中了。 16.git添加公钥后报错sign_and_send_pubkey: signing failed: agent refused operation的解决办法1234在服务器添加完公钥后报错执行下面两句eval "$(ssh-agent -s)"ssh-add]]></content>
      <categories>
        <category>经验</category>
      </categories>
      <tags>
        <tag>经验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdawn使用笔记]]></title>
    <url>%2Fposts%2F13502%2F</url>
    <content type="text"><![CDATA[标题123456# 一阶标题 或者快捷键Ctrl+1## 二阶标题 或者快捷键Ctrl+2### 三阶标题 或者快捷键Ctrl+3#### 四阶标题 或者快捷键Ctrl+4##### 五阶标题 或者快捷键Ctrl+5###### 六阶标题 或者快捷键Ctrl+6 下划线1&lt;u&gt;下划线的内容&lt;/u&gt; 或按快捷键Ctrl+U 下划线的内容 字体加粗1**加粗内容** 或按快捷键Ctrl+B 加粗内容 斜体1*倾斜内容* 或按快捷键Ctrl+I 倾斜内容 删除线1~~删除线的内容~~ 或按快捷键Alt+Shift+5 删除线的内容 文字高亮1==我是最重要的== ==我是最重要的== 角标1x^2^ H~2~O 文本居中1&lt;center&gt;这是要居中的文本内容&lt;/center&gt; 这是要居中的文本内容 PS：Typora目前并不会直接预览居中效果——相应的效果只有输出文本的时候才会显现。 list有序1数字+英文小数点(.)+空格 策划目标 战前准备 开始行动 无序1+ 、- 、* 创建无序列，任意数字开始+空格创建有序列表 兔 马 Table1快捷键Ctrl+T弹出对话框 国籍 省份 市区 中国 山东 聊城市 分割线12***+回车 ---+回车 插入图片 将图片直接拖拽进来，自动生成链接 链接 内行式 百度一下，你就知道 1[百度一下，你就知道](https://www.baidu.com/) 参考式 [百度一下，你就知道][]https://www.baidu.com/ 1[百度一下，你就知道][]https://www.baidu.com/ # 第二个括号内可任意填写(不显) 快速链接 http://blog.csdn.net/wwwfrank2 1&lt;http://blog.csdn.net/wwwfrank2&gt; PS：按住Ctrl点击链接可直接打开。 数学公式(简) Typora支持加入用LaTeX写成的数学公式，并且在软件界面下用MathJax直接渲染。 行内公式(inline math) 可以在偏好设置中单独打开，由一个美元符号将公式围起来；name=将公式围起来；name=\prod \frac{1}{i^2}$ 行外公式，双$+回车 $$∏1i2∏1i2$$ 注：上标和下标可以使用数学表达式来获取 代码块 单行代码 PHP是世界上最好的语言 多行代码： ~~~markdown 1printf(&quot;Hello World!&quot;); 其余引用 引用 1&gt;空格 或按快捷键Ctrl+Shift+Q 表情🥇 1:1 目录1[TOC]]]></content>
      <tags>
        <tag>Markdawn</tag>
      </tags>
  </entry>
</search>
