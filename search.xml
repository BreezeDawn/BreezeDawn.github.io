<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[Python-代码实现]统计学习方法之感知机模型]]></title>
    <url>%2Fposts%2F29271%2F</url>
    <content type="text"><![CDATA[内容简介 感知机模型 - 手写 Coding 使用手写模型进行鸢尾花分类 使用 sklearn 中的感知机进行鸢尾花分类 感知机模型 - 手写 Coding1234567891011121314151617181920212223242526class Model:"""感知机模型""" def __init__(self, data): """选取初值 w, b, η""" self.w = np.zeros(len(data[0]) - 1, dtype=np.float32) # 参数 w 应与 x 等量 self.b = 0 self.η = 0.1 def sign(self, x): """感知机模型""" y = np.dot(self.w, x) + self.b return 1 if y &gt;= 0 else -1 def fit(self, x_train, y_train): """模型训练""" while True: for d, x in enumerate(x_train): # 取出一条数据 y = y_train[d] # 取出对应数据的 target if y * self.sign(x) &lt;= 0: # 分类不正确进行参数更迭 self.w = self.w + np.dot(self.η * y, x) self.b = self.b + self.η * y break # 发生更迭即存在分类错误，从头再来 else: # 没有发生更迭即全部分类正确，停止训练 break return self.w, self.b 使用手写模型进行鸢尾花分类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import pandas as pdimport numpy as npfrom sklearn.datasets import load_irisimport matplotlib.pyplot as pltdef main(): # 一、加载数据 iris = load_iris() # 二、提取输入与输出数据 # 为输入特征创建 Frame，并使用特征名称作为列标题(注意不是列索引) df = pd.DataFrame(iris.data, columns=iris.feature_names) # 添加输出列 target df['target'] = iris.target # 给 Frame 添加列索引(只有加了索引才可以使用索引) df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'target'] # 打印输出值分布情况 print(df.target.value_counts()) # 三、绘出数据并观察分布情况 # 通过 frame 能够看出数据是 50 间隔分布，因此可以以 50 间隔分别取出 plt.scatter(df[:50]['sepal length'], df[:50]['sepal width'], label='0') plt.scatter(df[50:100]['sepal length'], df[50:100]['sepal width'], label='1') plt.xlabel('sepal length') plt.ylabel('sepal width') plt.legend() plt.show() # 四、特征提取与目标值提取 # 使用 iloc 选取前 100 条数据的第 0, 1, -1 列，并转换为 array data = np.array(df.iloc[:100, [0, 1, -1]]) # 将 第 0, 1 列数据赋值给 x，将 第 -1 列数据赋值给 y x_train, y_train = data[:, :-1], data[:, -1] # 将 y 值进行 1, -1分类 y_train = np.array([i if i == 1 else -1 for i in y_train]) # 五、感知机模型训练 perceptron = Model(data) w, b = perceptron.fit(x_train, y_train) # 六、绘出判定边界 # 分离超平面为 w[0]x_1 + w[1]x_2 + b = 0 x_1 = np.linspace(4, 7, 10) x_2 = -(w[0] * x_1 + b) / w[1] plt.plot(x_1, x_2) plt.scatter(df[:50]['sepal length'], df[:50]['sepal width'], label='0') plt.scatter(df[50:100]['sepal length'], df[50:100]['sepal width'], label='1') plt.xlabel('sepal length') plt.ylabel('sepal width') plt.legend() plt.show()if __name__ == '__main__': main() 使用 sklearn 中的感知机进行鸢尾花分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import pandas as pdimport numpy as npfrom sklearn.datasets import load_irisfrom sklearn.linear_model import Perceptronimport matplotlib.pyplot as pltdef main(): # 一、加载数据 iris = load_iris() # 二、提取输入与输出数据 # 为输入特征创建 Frame，并使用特征名称作为列标题(注意不是列索引) df = pd.DataFrame(iris.data, columns=iris.feature_names) # 添加输出列 target df['target'] = iris.target # 给 Frame 添加列索引(只有加了索引才可以使用索引) df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'target'] # 三、特征提取与目标值提取 # 使用 iloc 选取前 100 条数据的第 0, 1, -1 列，并转换为 array data = np.array(df.iloc[:100, [0, 1, -1]]) # 将 第 0, 1 列数据赋值给 x，将 第 -1 列数据赋值给 y x_train, y_train = data[:, :-1], data[:, -1] # 将 y 值进行 1, -1分类 y_train = np.array([i if i == 1 else -1 for i in y_train]) # 四、使用SKlearn感知机进行模型训练 clf = Perceptron() clf.fit(x_train, y_train) w = clf.coef_[0] # w b = clf.intercept_ # b # 五、绘出判定边界 # 分离超平面为 w[0]x_1 + w[1]x_2 + b = 0 x_1 = np.linspace(4, 7, 10) x_2 = -(w[0] * x_1 + b) / w[1] plt.plot(x_1, x_2) plt.scatter(df[:50]['sepal length'], df[:50]['sepal width'], label='0') plt.scatter(df[50:100]['sepal length'], df[50:100]['sepal width'], label='1') plt.xlabel('sepal length') plt.ylabel('sepal width') plt.legend() plt.show()if __name__ == '__main__': main() 希望对你有所帮助，点个赞哇大兄dei！个人博客：http://xingtu.infoGitHub：https://github.com/BreezeDawn/MachineLearning]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>统计学习方法</tag>
        <tag>感知机</tag>
        <tag>sklearn</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百面机器学习-怎样处理类别型特征]]></title>
    <url>%2Fposts%2F17138%2F</url>
    <content type="text"><![CDATA[​ 知识点：序号编码（Ordinal Encoding）、独热编码（One-hot Encoding）、二进制编码（Binary Encoding） ​ 类别型特征主要是指性别、血型等只在有限选项内取值的特征。 ​ 类别型特征原始输入通常是字符串形式，除了决策树等少数模型能够直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。 如果被处理的类别型特征数据之间存在着大小关系 我们可以使用序号编码的方法，根据类别型特征之间的大小关系分别赋予一个数值 ID ，数值 ID 之间的大小关系与特征之间的大小关系相同。 如 高、中、低三档可以分别赋予数值 ID：3、2、1。 如果被处理的类别特征数据之间不存在大小关系 我们可以使用独热编码的形式，把每个数据都转换为 $N$ 维的稀疏向量来表示，$N$ 表示总的类别数目。如 A、B、AB、O 四种血型，可以分别使用四维稀疏向量 $[1,0,0,0]、[0,1,0,0]、[0,0,1,0]、[0,0,0,1]$ 来表示。 无论被处理的类别特征数据之间有无关系 我们可以使用二进制编码对特征数据进行转换。 进行转换时，首先我们需要新增一个递增的类别 ID 字段，如：1、2、3、4、…、n。然后利用二进制对 ID 字段进行哈希映射，最终得到 0/1 特征向量。 值得注意的是，二进制编码产生的向量维度是少于独热编码的，节省了存储空间。 如 A、B、AB、O 四种血型可以分别赋予类别 ID: 1、2、3、4，然后对类别ID进行二进制映射表示：$[0 0 1]、[0 1 0]、[0 1 1]、[1 0 0]$ 。]]></content>
      <categories>
        <category>机器学习</category>
        <category>百面机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三次握手与四次挥手-轻松理解]]></title>
    <url>%2Fposts%2F10091%2F</url>
    <content type="text"><![CDATA[给女朋友总结一个关于三次握手和四次挥手的文。 一、三次握手1. TCP三次握手的含义 TCP是面向连接的，无论哪一方向另一方发送数据之前，都必须先在双方之间建立一条连接。 三次握手的目的是同步连接双方的 序列号 和 确认号 并 交换TCP窗口大小信息 完成了三次握手，TCP协议就提供了可靠的连接服务，客户端和服务器端就可以开始传送数据。 2. 三次握手的步骤 1.第一次握手：客户端发送请求建立连接，请求报文段。2.第二次握手：服务器收到请求，发送同意并请求与客户端建立连接。3.第三次握手：客户端收到请求，发送同意与服务器建立连接。 最简单的理解如下图所示： 3. 三次握手的意义和作用 第一次和第二次握手 为了保证 服务端 能够接受到 客户端 的信息并能做出正确的应答。 第二次和第三次握手 为了保证 客户端 能够接收到 服务端 的信息并能做出正确的应答。 4. 怎样生动描述 TCP 的「三次握手」？ 「你瞅啥？」「瞅你咋地？」「来咱俩唠唠。」 然后就唠上了。 5. TCP 为什么是三次握手，而不是两次或四次？ 三次握手： “喂，你听得到吗？”“我听得到呀，你听得到我吗？” -&gt; 应答与请求同时发出“我能听到你，今天balabala……” 两次握手： “喂，你听得到吗？”“我听得到呀”“喂喂，你听得到吗？”“草，我听得到呀！！！！”“你TM能不能听到我讲话啊！！喂！”“……” 四次握手： “喂，你听得到吗？” “我听得到呀，你听得到我吗？” “我能听到你，你能听到我吗？” “……不想跟傻逼说话” 二、四次挥手1. 四次挥手的含义 当客户端和服务器通过三次握手建立了TCP连接以后，当数据传送完毕，相应的就要断开TCP连接。那对于TCP的断开连接，这里就有了“四次挥手”。 2. 四次挥手的步骤 1.第一次挥手：客户端发送断开请求2.第二次挥手：服务器收到断开请求，发送同意断开连接的请求3.第三次挥手：服务器发送请求断开连接4.第四次挥手：客户端收到，发送同意断开连接 3. 四次挥手的意义和作用 第一次挥手 当客户端发送断开请求，只是表示客户端已经没有数据要发送了，客户端告诉服务器，它的数据已经全部发送完毕了，但是，这个时候客户端还是可以接受来自服务器的数据。 第二次挥手 当服务器收到断开请求时，表示它已经知道客户端没有数据发送了并发送同意断开连接的请求，但是服务器还是可以发送数据到客户端的。 第三次挥手 当服务器发送同意断开连接的请求后，这个时候就表示服务器也没有数据要发送了，就会告诉客户端，我也没有数据要发送了。 第四次挥手 当客户端收到服务器发送请求断开连接后，再去告诉服务端我已经知道你没有数据要发给我了，同意断开连接请求。 4. 怎样生动描述 TCP 的「四次挥手」？ A:“走了昂！” B:“中！悠着点昂！” B:”我也走了昂！骨朵白！” A:”中！骨朵白！” 三、补充知识点1. 三次握手的第一次可以携带数据吗？为何？ 不可以，三次握手还没有完成。 2. 三次握手的第三次可以携带数据吗？为何？ 可以。 能够发出第三次握手请求的客户端，肯定接收得到服务器第二次握手请求，相当于被服务器钦定了。 所以，能够发出第三次握手报文的，应该是合法的用户。 尽管服务器还没有和该客户端建立连接，但接收到第三次握手的瞬间连接就会建立成功。 这时里面携带的数据刚好可以按照正常流程走。 3. 四次挥手的客户端第四次挥手后等待2msl的原因 为了保证客户端发送的最后一个确认断开连接的确认能够到达服务器。 因为这最后一个确认可能会丢失，然后服务器就会超时重传第三次挥手的报文。 如果没有这2msl，客户端发送完最后一个确认后直接关闭连接。 那么就接收不到服务器超时重传的报文，那么服务器就不能按正常步骤进入 close 状态。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>三次握手</tag>
        <tag>四次挥手</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百面机器学习-随机梯度下降法失效的原因]]></title>
    <url>%2Fposts%2F36134%2F</url>
    <content type="text"><![CDATA[​ 知识点：梯度下降法（Gradient Descent）、随机梯度下降法（Stochastic Gradient Descent） ​ 在深度学习中人们通常想到的优化方法就是随机梯度下降法。但是如果说只知道用随机梯度下降法来训练模型，那么当我们得到一个比较差的训练结果时就可能会放弃这个模型，但是造成训练效果差的原因可能并不是模型的问题，而是随机梯度下降法在优化过程中失效了，这就可能会错过一次新发现的机会。 批量梯度下降法与随机梯度下降法​ 首先我想先说一下随机梯度下降法相比于批量梯度下降法的劣势。在批量梯度下降法中，我们求梯度的时候为了获取准确的梯度，我们会在批量梯度下降法的每一步都会把整个训练集载入进行计算，因此批量梯度下降法能够很稳定的去逼近最低点。而在随机梯度下降法中我们在每一步仅仅是随机采样一个或者说是少量的样本来估计当前的梯度，由于每一步我们接收到的信息量有限，随意梯度下降法对梯度的估计就会常常出现偏差，目标函数曲线就会收敛的很不稳定并且会有很剧烈的波动现象，甚至还会出现不收敛的情况。这样批量梯度下降法就好比正常下山，而随机梯度下降法就好比蒙着眼睛下山。 随机梯度下降法失效的原因​ 在随机梯度下降法和批量梯度下降法中，我们都知道局部最优点是一个普遍存在的陷阱。但是对于随机梯度下降法来说，山谷和鞍点两种地形比局部最优更可怕。我们知道在山谷中准确的梯度方法方向是沿着山道向下，稍有偏离就会撞向山壁然后反弹，而粗糙的梯度估计就会使它在两个山壁之间来回的反弹，这样就会导致收敛不稳定和收敛速度慢。而在鞍点处，随机梯度下降法会走入一片平坦的地方，坡度不明显时就会很容易走错方向，同样的在梯度近乎为零的地方，随机梯度下降法也不能够准确的察觉出梯度的细微变化，这样梯度下降的过程就会停滞不前，随机梯度下降法就会失效。]]></content>
      <categories>
        <category>机器学习</category>
        <category>百面机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百面机器学习-为什么需要对数值类型的特征做归一化]]></title>
    <url>%2Fposts%2F6132%2F</url>
    <content type="text"><![CDATA[​ 知识点：特征归一化（Feature Scaling） ​ 如果存在特征的数值差别比较大的特征，那么分析出来的结果显然就会倾向于数值差别比较大的特征。 ​ 比如在学习速率相同的情况下，数值差别大的特征的更新速度就会大于数值差别小的特征，这样就需要较多的迭代才能找到最优解。如果将所有特征归一化到相同的数值区间后，优化目标的等值图就会变成圆形。这样所有的特征的更新速度就变得更为一致，就能够更容易且更快的通过梯度下降找到最优解。 ​ 因此为了消除数据特征之间的量纲影响，得到更为准确的结果，就需要进行特征归一化处理，使各指标处于同一数值量级，使得不同指标之间具有可比性，以便进行分析。 ​ 在实际应用中通过梯度下降法求解的模型通常都是需要归一化的，比如线性回归、逻辑回归、支持向量机、神经网络等模型。但是数据归一化并不是万能的，对于决策树模型它是不适用的，决策树在进行节点分裂时，主要依据的是数据集关于特征的信息增益比，而信息增益比与特征是否经过归一化是无关的，因为归一化并不会改变样本在特征上的信息增益。]]></content>
      <categories>
        <category>机器学习</category>
        <category>百面机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百面机器学习-能否说出几种降低过拟合和欠拟合风险的方法]]></title>
    <url>%2Fposts%2F9551%2F</url>
    <content type="text"><![CDATA[知识点：过拟合（over fitting）、欠拟合（under fitting） 一、降低过拟合风险的方法 增加训练数据​ 首先，我们知道的是，使用更多的训练数据是解决过拟合问题最有效的手段。因为如果说我们有更多的样本，也就是有更多的训练数据的话，我们就能够让模型学习到更多更有效的特征，从而就能减小噪声的影响。所以我们能够从数据入手来获得更多的训练数据来解决过拟合问题。 ​ 但是我们也知道直接增加这个数据一般来讲是很困难的，那么对此我们就可以通过一定的规则来扩充训练数据，比如说我们可以使用这个GAN来合成大量的新数据，再一个如果说我们在图像分类上面的话，我们可以让图像进行平移，旋转或者缩放等方式来扩充数据。 降低模型的复杂度​ 其次我们也知道在数据较少的时候，如果说模型是过于复杂的，也很容易产生这个过拟合问题，所以我们也可以适当的降低模型的复杂度来避免模型拟合过多的采样噪声。​ 比如说我们在这个神经网络模型里边儿我们就可以减少神经元的个数就是每层的神经元个数，也可以减少神经网络模型的网络层数来解决过拟合问题。当然我们如果说在这个决策树模型中的话，我们可以通过降低树的高度，或者说对树进行剪枝等操作，也能够降低模型的复杂度。​ 然后我们就能够通过降低模型的复杂度来解决过拟合的问题。 增大正则化系数​ 然后我们还能用最常用的方法就是正则化来解决。就是给模型里边儿的所有参数都加上一定的正则约束来避免因为权值过大产生过拟合问题。 减少特征的数量​ 我们还能够想的到当出现过拟合现象的时候。也有可能是因为我们的特征数量太多导致的。我们就可以通过减少相关性系数较小的特征来减少特征数量，防止过拟合的情况发生。 集成学习方法​ 最后就是还有一个集成学习方法。集成学习就是我们把多个模型集成到一起，然后来降低这个使用单一模型的时候产生的过拟合问题。不过这个方法我只是了解了一下，嗯，有一个叫做Bagging的方法。 ​ 关于Bagging：https://baijiahao.baidu.com/s?id=1581775182037963646 二、降低欠拟合风险的方法 添加新特征​ 首先，当出现欠拟合问题的时候我们首先要想到的就是特征值不足导致的，或者说也可能是现有的特征和样本标签的相关性不是那么强。​ 这样的话我们就可以通过“上下文特征”“ID类特征”或者“组合特征”来获取更多的特征，然后这样往往就能够取得更好的结果来降低欠拟合风险。​ 如果是在深度学习里的话，我们可以通过很多模型来来帮助丰富特征。比如因子分解机(Factorization Machine, FM)、GBDT(梯度提升决策树)、Deep-crossing 等。 关于FM：https://blog.csdn.net/liruihongbob/article/details/75008666 关于GBDT：https://blog.csdn.net/google19890102/article/details/51746402/ 关于Deep-crossing：https://blog.csdn.net/m0_37721099/article/details/79265958 增加模型的复杂度​ 如果说现有的特征和样本标签的相关性挺强的，但是还是出现了欠拟合的情况，而且特征也特别多的时候，我们就能够通过另一个方式：增加模型的复杂度来使模型具有更强的拟合能力。​ 比如说在线性模型中，我们可以添加高次项。在神经网络模型中可以增加网络层数或者每一层的神经元个数，来增加模型复杂度使模型拥有更强的拟合能力。 减小正则化系数​ 最后，我们还能通过减小正则化系数来这个降低欠拟合风险。正则化本来是用来防止过拟合的，但是当模型出现欠拟合现象的时候，我们也是可以有针对性地减小正则化系数来防止欠拟合情况发生。]]></content>
      <categories>
        <category>机器学习</category>
        <category>百面机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百面机器学习-逻辑回归相比线性回归，有何异同]]></title>
    <url>%2Fposts%2F32576%2F</url>
    <content type="text"><![CDATA[知识点：逻辑回归（Logistic Regression）、线性回归（Linear Regreesion） 异 首先，逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者的最本质的区别。 其次，逻辑回归与线性回归最大的区别就是逻辑回归中的因变量是离散的，而线性回归中的因变量是连续的。 在自变量 $x$ 与超参数 $θ$ 确定的情况下，逻辑回归可以看作广义线性模型在因变量 $y$ 服从二元分布时的一种特殊情况。而使用最小二乘法求解线性回归时，我们认为因变量y服从正态分布。也就是逻辑回归中因变量取值是一个二元分布，而线性回归中的因变量是服从正态分布的。 同虽然逻辑回归与线性回归的本质是大相迳庭的，但是他们也存在着相同之处。 首先，我们可以认为他们两个都使用了极大似然估计来对训练样本进行建模。线性回归使用最小二乘法实际上就应该是极大似然估计的一个简化，而逻辑回归中也是通过对似然函数去学习才得到的最佳参数 $θ$ 。 其次，他们两个在求解超参数的过程中都可以使用梯度下降的方法，这也是监督学习中一个常见的相似之处。]]></content>
      <categories>
        <category>机器学习</category>
        <category>百面机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
        <tag>经典算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录-统计学习方法（第一章）]]></title>
    <url>%2Fposts%2F24404%2F</url>
    <content type="text"><![CDATA[一、统计学习统计学习的目的 统计学习的方法有哪些 统计学习方法的三要素 统计学习方法的使用步骤 统计学习方法、统计学习理论、统计学习应用的概念 统计学习在科学技术中的重要性 二、监督学习输入空间、输出空间以及特征空间 回归问题、分类问题以及标注问题 联合概率分布 假设函数 监督学习的基本模型 什么是联合概率分布? 如P(x，y)，x与y同时发生的概率分布 三、统计学习三要素方法=模型+策略+算法 模型模型的假设空间以及模型的两种表现形式 策略损失函数与风险函数(期望损失) ​ 损失函数度量模型一次预测的好坏 ​ 损失函数的期望是风险函数，风险函数度量平均意义下模型预测的好坏 经验风险函数与结构风险函数 什么是期望? 什么是泛函? 泛函，泛函是函数的函数，函数的值由自变量的选取而确定，泛函的值是由自变量函数确定的，泛函的自变量称为宗量。 什么是先验概率以及后验概率，贝叶斯估计中的最大后验概率估计(MAP)又是什么? 算法考虑用什么样的计算方法求解最优模型 四、模型评估与模型选择训练误差与测试误差 过拟合和模型选择 五、正则化与交叉验证 正则化 交叉验证 简单交叉验证 S折交叉验证 留一交叉验证 什么是范数? 什么是奥卡姆剃刀原理? 为什么正则化项对应于先验概率? 六、泛化能力 泛化误差 期望风险等于泛化误差 泛化误差上界 从假设空间中选取一个函数，当所选参数不同时，该函数的泛化误差会不同。但我们知道我们期望得到的泛化误差一定小于或等于我们选定函数的泛化误差的最大值，这个最大值与训练误差成正比，训练误差越小，它也越小，但如果过拟合时训练误差足够的小时，它不应该越小而应该越大，或者如果当训练集数据足够的多的情况下，它一定会趋于0。因此泛化能力上界和训练误差、复杂度成正比，和训练集数量成反比。 七、生成模型与判别模型 监督学习又分为生成方法和判别方法，所学到的模型分别称为生成模型和判别模型。 什么是生成模型与判别模型。 从概率分布的角度考虑，对于一堆样本数据，每个均有特征Xi对应分类标记yi。 生成模型：学习得到联合概率分布P(x,y)，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。 判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。 数据要求：生成模型需要的数据量比较大，能够较好地估计概率密度；而判别模型对数据样本量的要求没有那么多。 生成模型与判别模型的优缺点 八、分类问题分类器 准确率、精确率以及召回率 $F_1$ 值 九、标注问题 什么是标注问题? 标注问题是分类问题的一个推广，它的输入与输出皆为序列。标注学习的目标类似于给一个物品打上相应的多个标签，比如一个长JJ的人你能够打上多个标签：男人、雄性哺乳动物。 十、回归问题一元回归与多元回归 线性回归与非线性回归]]></content>
      <categories>
        <category>机器学习</category>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MachineLearning-聚类算法]]></title>
    <url>%2Fposts%2F37668%2F</url>
    <content type="text"><![CDATA[在无监督学习中，我们的训练集可以写成只有$x^{(1)}$,$x^{(2)}$…..一直到$x^{(m)}$。我们没有任何标签 $y$。 我们希望有一种算法能够自动的把这些数据分成有紧密关系的子集或是簇。 K-均值算法(K-Means)算法步骤综述K-均值是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为: 首先选择k个随机的点，称为聚类中心(cluster centroids); 簇分配(cluster assignment) 对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。 移动聚类中心(move centroids) 计算 每一个组 的平均值，将该组所关联的中心点移动到平均值的位置。 重复步骤 2-4 直至中心点不再变化。 定义损失函数变量 假设有K个簇，$c^{(i)}$表示样本$x^{(i)}$ 当前所属的簇的索引编号 ，$c^{(i)}∈(1,2,3…K)$ $μ_k$ 表示 第k个聚类中心 的位置，其中 $k∈1,2,3,4…K$ 根据以上定义:则$μ_c(i)$ 表示样本$x^(i)$所属簇的中心的 位置坐标 K-means算法的优化目标损失函数为 每个样本到其所属簇的中心的距离和的平均值 ，优化函数的输入参数为 每个样本所属的簇的编号$c^{(i)}$和每个簇中心的坐标$μ_k$ 这两个都是在聚类过程中不断变化的变量。此代价函数也被称为 畸变函数(Distortion function) K-means算法步骤与优化函数 对于K-means算法中的 簇分配(将每个样本点分配到距离最近的簇) 的步骤实际上就是在最小化代价函数 $J$，即在$μ_1,μ_2,μ_3,μ_4…μ_K$固定的条件下调整 $c^{(1)},c^{(2)},c^{(3)},…c^{(m)}$的值以使损失函数的值最小。 对于K-means算法中的 移动聚类中心(将聚类中心移动到分配样本簇的平均值处) ，即在$c^{(1)},c^{(2)},c^{(3)},…c^{(m)}$固定的条件下调整 $μ_1,μ_2,μ_3,μ_4…μ_K$的值以使损失函数的值最小。 K均值算法簇中心的随机初始化 Random initialization随机初始化遵循法则 我们应该选择 K小于m，即聚类中心点的个数要小于所有训练集实例的数量 随机选择 K 个训练实例，然后令 K 个聚类中心分别与这 K 个训练实例相等 随机初始化的局限性 随机初始化很容易把 初始化簇中心 分到相近的样本中，这种初始化方式有其局限性。 K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。 改进初始化方式–多次随机初始化 假如随机初始化K-means算法100 (一般是50-1000) 次之间，每次都使用不同的随机初始化方式，然后运行K-means算法，得到100种不同的聚类方式，都计算其损失函数，选取代价最小的聚类方式作为最终的聚类方式。 这种方法在 K 较小的时候（2–10）还是可行的，但是如果 K 较大，这么做也可能不会有明显地改善。(不同初始化方式得到的结果趋于一致) K均值算法聚类数K的选择 Choosing the Number of Cluters 没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。 肘部法则(Elbow method) 改变聚类数K，然后进行聚类，计算损失函数，拐点处即为推荐的聚类数 (即通过此点后，聚类数的增大也不会对损失函数的下降带来很大的影响，所以会选择拐点) 但是也有损失函数随着K的增大平缓下降的例子，此时通过肘部法则选择K的值就不是一个很有效的方法了(下图中的拐点不明显，k=3,4,5有类似的功能) 目标法则 通常K均值聚类是为下一步操作做准备，例如：市场分割，社交网络分析，网络集群优化 ，下一步的操作都能给你一些评价指标，那么决定聚类的数量更好的方式是：看哪个聚类数量能更好的应用于后续目的]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-支持向量机（SVM）]]></title>
    <url>%2Fposts%2F11763%2F</url>
    <content type="text"><![CDATA[支持向量机的代价函数$$J(\theta) = \min_{\theta} C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta^Tx^{(i)})] + \frac{1}{2}\sum_{i=1}^n\theta_j^{2}$$ C 可以看作 $\frac{1}{\lambda}$ 支持向量机的作用 人们有时将支持向量机看作是大间距分类器 支持向量机能够努力的将正样本和负样本用最大的间距分开。这也是支持向量机具有鲁棒性的原因，鲁棒是Robust的音译，也就是健壮和强壮的意思。 支持向量机（SVM）实际上是一种凸优化问题，因此它总是能找到全局最小值或者接近它的值从而不用担心局部最优 关于内积和范数1. 内积：设有n维向量 ​ 令 ， 则称[x,y]为向量x与y的内积。 2. 范数：称 为向量x的范数(或长度)。 支持向量机产生大间距分类的原因由内积和范数引起变化，具体先不写。。。 核函数公式$$exp(-\frac{||x-l^{i}||^2}{2\sigma^2})$$ 目的使用核函数构造复杂的非线性分类器，能够根据数据的相似与否定义许多新的特征值 相似度函数就是核函数就是高斯核函数，$\sigma$ 是高斯核函数的参数 我们通过标记点和核函数来定义新的特征变量从而训练复杂的非线性边界 如何使用我们通过核函数能够得到 如何选取标记点每一个标记点的位置都与样本点的位置精确对应，选出 $m$ 个标记点。这样就说明特征函数基本上是在描述每一个样本距离样本集中其他样本的距离 支持向量机如何通过核函数有效的学习复杂非线性函数如果我们要进行预测，首先我们需要计算特征向量 $f_{(m+1)×1}$ ，内部值都是 传入 $x$ 与 标记点 通过核函数 与m个样本点进行相似度比较产出的。 我们再使用参数转置乘特征向量： $\theta^Tf = \theta_0f_0 + \theta_1f_1 + \theta_2f_2 + ……+ \theta_mf_m$ 如果结果 大于等于零，预测结果为 1。 但是我们怎么获得参数 $\theta$ 的值，我们通过最小化下式就能得到支持向量机的参数$$J(\theta) = \min_{\theta} C\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)}) + (1-y^{(i)})cost_0(\theta^Tf^{(i)})] + \frac{1}{2}\sum_{i=1}^n\theta_j^{2}$$ 这里的 $n = m$ ，这里我们仍然不对 $\theta_0$ 做正则化处理 最后的 $\sum_{i=1}^n\theta_j^{2}$ 还能够被写为 $\theta^T\theta$ 或是别的比如 $\theta^TM\theta$ ，这取决于我们使用的是什么核函数，这能够使支持向量机更有效率的运行，这样修改能够适应超大的训练集，那时 求解m维参数的成本会非常高，主要为了计算效率。 核函数虽然也能用在逻辑回归上，但是它毕竟是为支持向量机开发的，用在逻辑回归上会十分缓慢。 使用支持向量机时，怎么选择支持向量机里的参数参数 $C$在使用支持向量机时，其中一个要选择的事情是目标函数中的参数 $C$ 。 我们知道 $C$ 的作用类似于 $\frac{1}{\lambda}$ 。 如果使用较大的 $C$ ，这意味着我们没有使用正则化，这可能使我们可能得到一个低偏差高方差的模型。 如果使用较小的 $C$ ，这相当于我们在逻辑回归中用了一个大的 $\lambda$，这可能使我们可能得到一个高偏差低方差的模型。 高斯核函数中的参数 $\sigma^2$当 $\sigma^2$ 偏大时，由核函数得到的相似度会变化的很平缓，这会给模型带来较高的偏差和较低的方差。 当 $\sigma^2$ 偏小时，由核函数得到的相似度会变化的很剧烈，会有较大斜率和较大的导数，这会给模型带来较低的偏差和较高的方差。 运用SVM时的一些细节 使用不带有核函数的支持向量机就叫做线核的SVM，即没有 $f$ ，你可以把它想象为给了你一个线性分类器 线核SVM如果有大量的特征值（N很大），且训练的样本数很小（M很小），那么是不会去想着拟合拟合一个非常复杂的非线性函数的，因为没有足够多的数据很有可能过度拟合。 高斯核函数如果有少量的特征值（N很小），且训练的样本数很大（M很大）， 提供核函数高斯核函数和线性核函数是最普遍的核函数， 注意事项：如果你有大小不一样的特征变量，为了不使间距被大型特征操控（小的特征都被忽略掉），在使用高斯核函数前最好将这些特征变量的大小按比例归一化。 warning：所有的核函数都已经满足一个技术条件，它叫做莫塞尔定理。 吴恩达很少很少很少使用其他核函数， 多类分类（K分类）中如何使用支持向量机现成的多类分类的函数包， 逻辑回归算法于支持向量机的选择$n&gt;=m$ 逻辑回归或者线核SVM，因为没有更好更多的数据拟合复杂的非线性函数 $n&lt;=m$ 高斯SVM $n&lt;&lt;=m$ 增加或者创建更多的特征变量，然后使用逻辑回归或者线核SVM 为什么不使用神经网络训练起来会特别的慢 最后算法确实很重要，但更重要的是我们有多少数据，我们是否擅长做误差分析和诊断学习算法来指出设定新的特征变量，或找出其他能够决定我们学习算法的变量等方面，通常这些方面会比我们使用逻辑回归还是SVM这方面更加重要]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-如何有效使用机器学习算法]]></title>
    <url>%2Fposts%2F16394%2F</url>
    <content type="text"><![CDATA[怎么改进算法当使用训练好的模型时，新样本输出的数据产生了巨大的误差，如何改进算法的性能。 使用更多的训练样本，但通常来讲并没有什么卵用 尝试选用更少的特征集，来防止过拟合 或许也需要更多的特征集，当目前的特征集对你没有多大用处时，可以从更多的特征角度去收集更多的特征 增加多项式特征 减小正则化中的 $\lambda$ 的值 增大正则化中的 $\lambda$ 的值 我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。 怎么评估算法的性能（机器学习诊断法[machine learning diagnostics]） “诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。 标准方法 将所有数据按照 7：3 的比例分成训练集和测试集，使用 $(x_{test}^{(i)}，y_{test}^{(i)})$ 表示测试集数据。 如果所有的数据存在规律或顺序，最好先打乱顺序再按比例分割。 测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算测试集误差： 对于线性回归模型，我们利用测试集数据计算代价函数$J_{test}(\theta)$ 对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外：$$J_{test}{(\theta)} = -\frac{1}{m}{test}\sum{i=1}^{m_{test}}\log{h_{\theta}(x^{(i)}{test})}+(1-{y^{(i)}{test}})\log{h_{\theta}(x^{(i)}_{test})}$$ 误分类的比率，对于每一个测试集样本，计算： ​ 然后对计算结果求平均。 怎么选择模型通过交叉验证，选择能最好的拟合数据的多项式次数的模型 定义多个模型，每个模型的次数不同，使用 $d$ 表示模型的多项式最高次数。 将所有数据按照 6：2：2 的比例分成训练集、交叉验证集和测试集，使用 $(x_{cv}^{(i)}，y_{cv}^{(i)})$ 表示验证集数据，使用 $(x_{test}^{(i)}，y_{test}^{(i)})$ 表示测试集数据。 如果所有的数据存在规律或顺序，最好先打乱顺序再按比例分割。 $m_{cv}$ 表示验证集总数，$m_{test}$ 表示测试集总数。 同样的，我们能够定义训练集误差 $J_{train}{(\theta)}$、验证集误差 $J_{cv}{(\theta)}$、测试集误差 $J_{test}{(\theta)}$ 使用训练集代入所有模型并通过训练使得最终代价函数 $J(\theta)$ 最小，再使用验证集代入训练后的所有模型来算出$J_{cv}{(\theta)}$，选出能最好的对交叉验证集进行预测的模型（$J_{cv}{(\theta)}$ 最小的模型），确定最终模型的最高次数 $d$。 使用测试集，预测或估计，通过学习算法得出的模型的泛化误差。 最好按比例分出三份不一样的数据，如果只分为两份，让其中一份既作为验证集又作为测试集，并不好。 模型出现问题，是欠拟合还是过拟合 偏差比较大（欠拟合） 方差比较大（过拟合） 高偏差（欠拟合）： 当 训练集数据 和 验证集数据 出现的误差都很大时，且两个误差可能很接近或者可能验证误差稍大一点。 高方差（过拟合）： 当 训练集数据 出现的误差很小， 验证集数据 出现的误差很大时，且 $J_{cv}{(\theta)} &gt; &gt; J_{train}{(\theta)}$ 误差即 $J(\theta)$ $&gt;&gt;$ 远大于 如何选取正则化参数 $\lambda$之前通过交叉验证后我们已经选择了一个合适的模型，但是我们还没有正则化项。 这次我们仍然通过交叉验证，来进行选择一个合适的正则化参数 $\lambda$ 。 定义多个正则化参数 $\lambda$，每个模型的$\lambda$不同。 将所有数据按照 6：2：2 的比例分成训练集、交叉验证集和测试集，使用 $(x_{cv}^{(i)}，y_{cv}^{(i)})$ 表示验证集数据，使用 $(x_{test}^{(i)}，y_{test}^{(i)})J_{cv}{(\theta)}$ 表示测试集数据。 如果所有的数据存在规律或顺序，最好先打乱顺序再按比例分割。 $m_{cv}$ 表示验证集总数，$m_{test}$ 表示测试集总数。 同样的，我们能够定义训练集误差 $J_{train}{(\theta)}$、验证集误差 $J_{cv}{(\theta)}$、测试集误差 $J_{test}{(\theta)}$ 使用训练集代入所有模型并通过训练使得最终代价函数 $J(\theta)$ 最小，再使用验证集代入训练后的所有模型来算出$J_{cv}{(\theta)}$，选出能最好的对交叉验证集进行预测的模型（$J_{cv}{(\theta)}$ 最小的模型），确定最终正则化参数 $\lambda$ 。 使用测试集，预测或估计，通过学习算法得出的模型的对新样本的泛化能力。 关于学习曲线从学习曲线，我们能够看出模型面临的是什么问题。 当模型处于高偏差时，误差会趋于水平不会再降，哪怕添加再多的训练集数据模型产生的误差也不会有所改善。 当模型处于高方差时，训练集误差会始终很小，验证集误差会始终很大，但是如果继续增大训练集数据，是能够改进模型的泛化能力的。 因此，画出模型的学习曲线，搞清楚当前算法是否存在高偏差或是高方差，对于改善算法来讲是非常有意义的，我们能够选择是否添加更多的训练及数据来应对问题。 应该采用哪种方法改进算法 获得更多的训练样本——解决高方差 尝试减少特征的数量——解决高方差 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 尝试增加正则化程度λ——解决高方差 1234567使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络，然后选择交叉验证集代价最小的神经网络。 误差分析构建一个学习算法的推荐方法为： 首先使用简单快速的方式实现算法，然后使用交叉验证集数据验证这个算法。 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他方式完善算法。 进行误差分析：人工检查交叉验证集在我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势。 数值评估误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 通过一个量化的数值评估，我们可以看看这个数字，误差是变大还是变小了，它可以直观地告诉我们：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高我们实践算法时的速度。 类偏斜类偏斜情况表现为在训练集中同一种类的样本特别多，其他类的样本特别少。 在类偏斜的情况下，模型的准确率的提升并不能说明我们的模型质量得到了提升，比如当我们的模型准确率为98时，我们完善了模型后，把准确率提升到了99％，但是训练集中的99％的数据是属于同一类的，我们并不知道我们的算法究竟是得到了完善还是变成了碰到数据就把它归为99％那一类的撒比算法。 所以我们需要一个不同寻常的评估度量值：查准率 和 召回率。 查准率：预测出的实际结果数量 占 预测结果总数量 的百分比 召回率：预测出的实际结果数量 占 实际结果总数量 的百分比 这两个数字越高，就说明模型质量越好。 查准率和召回率之间的权衡我们选取的阈值决定了我们最终预测结果的走向是0还是1，所以阈值是影响查准率和召回率的重要因素，那么我们应该选取 $0-1$ 之间哪个数为阈值呢? 不同的阈值对应着不同的查准率和召回率，查准率和召回率都是越高越好，但是我们无法判断究竟查准率 $0.7$ 召回率 $0.3$ 好 ，还是查准率 $0.4$ 召回率 $0.6$ 好 。 这时我们就需要一个评估度量值来综合的考虑两个因素给定一个结果：阈值设为多少合适。 这个评估度量值的计算公式有很多个，我们现在看在机器学习中经常用到的一个：$${F}_{1} = \frac{PR}{P+R}$$ P 查准率 R 召回率 机器学习的数据123事实上，如果你选择任意一个算法，可能是选择了一个&quot;劣等的&quot;算法，如果你给这个劣等算法更多的数据，那么从这些例子中看起来的话，它看上去很有可能会其他算法更好，甚至会比&quot;优等算法&quot;更好。由于这项原始的研究非常具有影响力，因此已经有一系列许多不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现，这还取决于一些细节，但是真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识：&quot;取得成功的人不是拥有最好算法的人，而是拥有最多数据的人&quot;。现在假设我们使用了非常非常大的训练集，在这种情况下，尽管我们希望有很多参数，但是如果训练集比参数的数量还大，甚至是更多，那么这些算法就不太可能会过度拟合。也就是说训练误差有希望接近测试误差。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-see，HexoGUI，发布文章-提交仓库-重置静态文件-开启本地服务]]></title>
    <url>%2Fposts%2F48526%2F</url>
    <content type="text"><![CDATA[一、hexo-see简介Python3 实现。 Hexo的可视化界面，摆脱命令行。 很粗糙，请见谅。 目前有个BUG：只要是GUI界面创建的文章，在每次打开后保存(或自动保存)时，所有内容会丢失，但只要撤回一下内容就回来了。这个BUG可能由于使用了动态生成链接的插件导致的，也可能都会出现。 二、功能1、界面化创建文章！ 2、创建文章后可选择直接打开 3、提交至远程仓库 4、清除本地public文件 5、开启本地服务 三、所用包 包 操作 tkinter 实现GUI界面 os 进行命令操作 threading 进行多线程操作 win32api 实现界面居中 四、按钮与命令的映射关系 按钮名称 对应命令 重新生成静态文件 hexo g 清除本地public文件 hexo clean 创建文章 hexo n post 提交仓库 hexo d 本地预览 hexo s 退出 退出本程序 五、使用配置 tkinter、os、threading 都是内置包，因此仅需安装 win32api， Python3 使用 pip3 install pypiwin32安装即可。 如安装失败，请手动安装whl文件。 whl文件源地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/。 更改 if __name__ == ‘__main__‘: 里初始化 Hexo 时的路径输入。 改为自己博客 站点配置根路径 即可使用！ 使用说明 输入 标题、标签、分类 直接创建！ 标题 不可为空！，标签和分类 可以为空。 如果标题中出现 空格 会被替换掉。出现 、?/\&lt;&gt;*都会被替换为 - 。 多个标签/多个分类使用空格分割！ 多个标签/多个分类使用空格分割！ 多个标签/多个分类使用空格分割！ 如果想要使用 .exe 可执行文件，需自行转换（因为需要配置自己的路径）。 可使用 pyinstaller 包进行转换，pip install pyinstaller。 下面有关于本工具的打包说明。 除本地预览为后台开启，其他都会有控制台出现，方便查看执行过程。 本地预览暂时不支持关闭（因为是后台执行，虽然也不需要关，毕竟可以一直本地访问）， 即使程序退出，本地服务也不会关闭。 因为本地服务有可能在后台运行，因此点击本地预览时将会使用taskkill杀掉 占用4000端口的服务， 然后才开启Hexo本地服务。 六、exe 可执行程序转换说明pyinstaller的参数说明123456789101112131415-c 参数 使用控制台，无界面(默认)-w 参数 使用窗口，无控制台.如果程序里有使用到控制台(如print)的就不可以使用-w, 否则会报错 '''failed to excute script xxx''' 如果想要捕捉错误信息可以先用控制台捕捉,没有报错后再使用无控制台. -D 参数 创建一个目录，包含exe文件，但会依赖很多文件（默认选项）。-F 参数 打包成一个exe文件-p 多文件打包时,以-p [其他.py] 的形式跟在主文件后 '''如:pyinstaller -w -F main.py -p view.py -p other.py'''-i 参数 修改打包后的exe图标,图标应放在py同级目录下,需要是ico格式,只改后缀不可用. '''如:pyinstaller -w -F -i zzz.ico main.py -p view.py -p other.py''' 本程序的打包说明 将配置完毕的 Hexo.py 与 favicon.ico 放在同一文件目录 使用命令行进入文件目录 pyinstaller -w -F -i favicon.ico Hexo.py 愉快使用 七、额外说明本工具开源协议为 不知道协议，因为我还没有区分这些协议的意思…… 总之，随便用，欢迎 star、 fork、issue。]]></content>
  </entry>
  <entry>
    <title><![CDATA[MachineLearning-神经网络(二)]]></title>
    <url>%2Fposts%2F36766%2F</url>
    <content type="text"><![CDATA[神经网络的代价函数 符号 意义 $L$ 神经网络结构总层数 $S_l$ 第 $l$ 层的单元数量 (不包括偏差单元) $K = S_L$ 输出层的单元数量 (日了个仙人板板，手写一直渲染错误只能贴图了，: ) 浪费好久时间 ) 代价函数中 $\theta_0$ 总是被忽略的，因为我们并不想把 $\theta_0$ 加入到正则化里，也不想使它为 0，即不把偏差项正则化。 反向传播为什么使用反向传播在神经网路中，我们的 $\theta$ 数量居多，如果一个一个计算代价函数的偏导项再进行梯度下降计算，计算量实在是太大了，在使用梯度下降算法进行训练时速度会特别慢。因此，为了计算代价函数的偏导项，我们选择使用反向传播计算每一个神经节点激励值与期望神经节点激励值的误差，然后通过误差与神经元的激励值再次计算得出偏导项的计算结果。 思想我们能够明白，如果输出层的输出与期望得到的输出 存在误差，那么当下的每个神经元的激励值必定与得到期望输出时的每个神经元的激励值 也存在误差，我们将使用 $\delta_j^{(l)}$ 代表第 $l$ 层的第 $j$ 个神经元当下激励值与期望神经元的激励值之间存在的误差。 而反向传播算法从直观上说，就是从输出层开始到输入层为止，反向推导出每一个神经节点的激励值的误差$\delta$。 方法使用反向传播前，也就是求代价函数的导数前，首先需要使用前向传播将每一个神经节点的激励值算出，然后从后向前计算每一个神经节点的 $\delta$。我们还要明白的就是：我们此时只知道输出层神经节点的期望激励值，因此我们只能够从输出层开始计算。 那么，假设我们有一层输入层，一层输出层，两层隐藏层，一共四层，我们能够以 $\delta_2^{(4)}$ 表示输出层的第二个神经节点的激励值误差， 并且它的值能够通过计算得出： $\delta_2^{(4)} = a_2^{(4)} - y_2$ ，通常我们会以向量化的形式表示整个一层的误差值 即 $\delta^{(4)}$ = $a^{(4)} - y$。 而我们会使用这样一个公式，反向计算上一层的误差值：$$\delta^{(l-1)} = (\Theta^{(l-1)})^T\delta^{(l)} .* g’(z^{(l-1)})$$ 如 第 3 层： $$\delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)} .* g’(z^{(3)})$$ $.*$ 代表两个向量(矩阵)对应值两两相乘。 $g$ 代表激励函数，通过计算能够得出 $g’(z^{(l)}) = a^{(l)} .*(1 - a^{(l)})$ 同样的，我们能够计算出 第 2 层 $\delta^{(2)}$ ，但是我们并不需要计算 $\delta^{(1)}$，因为输入层是明确的已知值。 如何计算代价函数的导数项不使用求导的方法，我们能够通过以下公式得到导数项的最终结果（忽略正则化）：$$\frac{\partial}{\partial(\Theta_{ij}^{(l)})}J(\Theta) = a_j^{(l)} \delta_i^{(l+1)}$$ 也就是代价函数 $J(\theta)$ 对 第 $l$ 层 第 $i$ 行 $j$ 列 的 $\theta$ 求偏导 $=$ 第 $l$ 层 第 $j$ 个 神经节点的激励值 $a$ × 第 $l+1$ 层 第 $i$ 个 神经节点的误差值 $\delta$ 由此，我们能够很快求出 所有参数 $\theta$ 的偏导数。 但是由于每条训练集数据都不相同，因此针对与每条数据，得到的输出层结果与期望结果也总是不同的，那么 虽然每层的参数矩阵 $\Theta$ 一直不变，但由于每条数据的每层的误差 $\delta$ 各不相同，那么每条数据的代价函数求导自然得出的值也不相同。 所以我们需要计算出针对于每层 $\Theta$ 的每条数据的代价偏导 $\frac{\partial}{\partial(\Theta_{ij}^{(l)})}J(\Theta)$，然后进行相加，最终得出针对 $m$ 条数据算出的第 $l$ 层的总体代价函数偏导值：$\Delta_{ij}^{(l)}$ 。 所以我们能够得出代价函数针对每一个参数的平均偏导数 $D_{ij}^{(l)}$ ：$$\frac{\partial}{\partial(\Theta_{ij}^{(l)})}J(\Theta) = D_{ij}^{(l)} = \frac{1}{m}\Delta_{ij}^{(l)}$$ 当 $j = 0$ 时，最终结果为上式。 当 $j \neq 0$ 时，最终结果应为 $\frac{\partial}{\partial(\Theta_{ij}^{(l)})}J(\Theta) = D_{ij}^{(l)} = \frac{1}{m}\Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(j)}$ 为什么取平均值：稳定性。 矩阵向量化其实就是把矩阵写为一行。 梯度检验本质上就是使用求斜率的方法计算出偏导项结果，然后与反向传播算法计算出的偏导项结果进行比较校验 随机初始化为了训练神经网络，应该对权重进行随机初始化，初始化为 $-\epsilon &lt; \theta &lt; \epsilon$ 接近于0的小数，然后进行反向传播，执行梯度检验，使用梯度下降或者使用更好的优化算法试着使 $J$ 最小。作为参数 $\theta$ 的需要使用随机的初始值来打破对称性，使得梯度下降或是更好的优化算法找到 $\theta$ 的最优值。 关于隐藏层的层数设定与每层隐藏单元的个数设定 第一层的单元数即我们训练集的特征数量。 最后一层的单元数是我们训练集的结果的类的数量。 普遍地，我们会设定隐藏层的层数为 1 层。 如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。 一般来讲，每个隐藏层所包含的单元数量还应该和输入 $x$ 的维度相匹配，也要和特征的数目相匹配。 可能隐藏单元的单元数量和输入特征的数量相同，一般来说，隐藏单元的数目取为稍大于输入特征数目。 关于神经网络的使用过程总结 参数的随机初始化 利用正向传播方法计算所有的$h_{\theta}(x)$ 编写计算代价函数 $J$ 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数 ps 代价函数 $J$ 度量的就是这个神经网络对训练数据的拟合情况。 BP算法的基本思想是，学习过程由信号的正向传播与误差的反向传播两个过程组成。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>代价函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Hexo+Github搭建个人博客(详细)]]></title>
    <url>%2Fposts%2F54972%2F</url>
    <content type="text"><![CDATA[搭建环境准备： 下载Node.js 安装Git 拥有github账号 下载完成后（全部按NEXT就好），按下WIN+R，调出运行窗口，打cmd回车进入命令行，验证node和Git是否安装正确，输入下面指令: 123node -vnpm -vgit --version 如果都安装成功就会显示对应的软件版本号。 安装Hexo进去你要放置博客文件夹的目录，点击鼠标右键，点击 Git Bash Here，然后依次输入并执行下面的代码。 12345$ npm install hexo-cli -g $ hexo init Hexo$ cd Hexo$ npm install$ hexo server 第一行是安装 hexo 扩展插件。 第二行是创建一个为 Hexo 的文件夹，我们要把 hexo 相应的代码下载到该文件中下。 第三行进入到新创建的文件夹内。 第四行是安装 hexo 相关的代码。 第五行启动本地服务，启动完成后，在浏览器输入 http://localhost:4000/ 就可以访问刚刚创建的博客了。 如果报错: 执行 npm cache clean --force 清缓存再安装 Hexo目录里会多出很多文件，此时的目录情况如下: 123456node_modules npm 文件缓存目录scaffolds 文夹件下存放文章和页面模版scource 文夹件下存放资源文件themes 文夹件下存放主题文件package.json 站点版本和站点所需的依赖文件_config.yml 站点配置文件 hexo 常用命令 ： 12345678$ hexo g 生成静态文件$ hexo s 启动本地服务$ hexo d 提交到远程仓库$ hexo n page 创建页面 $ hexo n &quot;&quot; 创建文章$ hexo d -g 生成静态并提交到远程仓库$ hexo s -g 生成静态文件并启动本地预览$ hexo clean 清除本地 public 文件 外网访问第一步 - 创建仓库在 Github 创建一个名字为 username.github.io 的仓库，这里的 username 必须是你的github用户名。 第二步 - 修改配置注意所有的命令都在git-bash里敲! 注意所有的命令都在git-bash里敲! 注意所有的命令都在git-bash里敲! 注意所有的配置冒号后都要有空格! 注意所有的配置冒号后都要有空格! 注意所有的配置冒号后都要有空格! 修改depoly ，这里的 username 仍然是你的 Github 用户名： 1234deploy: type: git repo: git@github.com:username/username.github.io.git // 后面对应的是仓库链接 branch: master 示例： 1234deploy: type: git repo: git@github.com:BreezeDawn/BreezeDawn.github.io.git branch: master 修改 site 相关信息 ： 1234567title: xxsubtitle: description: 中文最好用编辑器不要用记事本打开keywords:author: xxlanguage: zh-Hanstimezone: 注 ：网站名称（title），作者 (author)，语言 (language)，签名(description) 第三步 - 给本地 Git 添加 ssh免密登陆首先我们需要修改 Git 的全局配置 user.name 和user.email 12git config –-global user.name “xxxxxx” // 自己的 github 用户名git config –-global user.email “xxxxxx” // 自己的 github 里绑定的邮箱 在这里 user 不需要替换成自己…..只需要修改双引号里的内容 然后我们进入存放密匙的文件夹，检查本地是否有 SSH key 12$ cd ~/.ssh$ ls 如果 SSH key 存在，就会显示 id_rsa、id_rsa.pub、know_hosts 三个文件 。 没有的话我们就来创建 SSH key 1$ ssh-keygen -t rsa -C &quot;你的邮箱&quot; 然后点击回车，接着会让你输入文件名，点击回车直接忽略，再然后会让设置密码并确认密码，我们点击两次回车，直接把密码设置为空，不用输入 。然后你会看到一堆泡泡，说明密匙创建成功。 创建成功后，可以通过如下命令拷贝 SSH key 的内容 ： 1clip &lt; ~/.ssh/id_rsa.pub&quot; 你也可以手动打开~/.ssh/目录下的id_rsa.pub文件进行拷贝，所有内容一字不漏的拷贝! 然后我们打开 GitHub 点击右上角头像进入个人资料，点击Settings -&gt; 左边 SSH and GPG keys，然后点击 New SSH key，title随便填，把之前拷贝的内容粘贴到 key 里面，然后点击 Add SSH key。 怎么去验证是否已经添加成功了呢 ？通过如下命令 ： 1$ ssh -T git@github.com 验证成功，你会看到 successfully !! 但是我们还差一步~ 第四步 - 更新静态文件，提交到 github 仓库执行 $ npm install hexo-deployer-git --save - 安装关联 Github 的插件 执行 hexo d -g - 更新静态文件并提交到你的 github 仓库。 然后使用浏览器打开 https://username.github.io.git ，是不是可以外网访问了呢? 记住一定是https的哦，不然访问不到 更换主题下载主题如果不喜欢现在的主题，我们可以在 github 中搜索 hexo theme 寻找自己喜欢的主题。 我用的是人气最高的 Next 主题，它提供非常详尽的官方文档，并且支持很多第三方插件，十分的友好。 Next Github：https://github.com/iissnan/hexo-theme-next/ Next 官方文档：http://theme-next.iissnan.com/ 我们现在通过 git 方式下载 Next，命令如下 ： 12$ cd themes$ git clone https://github.com/iissnan/hexo-theme-next next hexo 的主题文件都放在 themes 文件夹下，所以我们要进入主题文件夹 下下载 Next。 下载完成后，我的博客 themes 下就多了一个 next 文件夹。 配置主题首先我们要区分两个文件。 第一个是我们网站的 站点配置文件 _config.yml，它在我们的博客根目录 Hexo/下，Hexo 为 hexo init 初始化时自动创建的文件夹名称。 第二个是我们网站的 主题配置文件 _config.yml，它在我们的主题目录 Hexo/themes/next下。 然后我们修改 站点配置文件 : 1theme : next 注 ：把默认主题 landscape 切换成 next。 此时我们的博客主题已经修改为 Next 主题，但 Next 主题其实有四个风格，Muse、Mist、Pisces、Gemini，且这里默认为 Muse。 如果我们想要修改，就需要打开 主题配置文件， 修改 Schemes ： 1234scheme: Muse#scheme: Mist#scheme: Pisces#scheme: Gemini 根据自己的选择进行注释。 修改完毕后我们需要把静态文件按照新的配置重新生成，还记得重新生成静态文件的命令吗? 1$ hexo g 然后我们就可以提交给远程仓库了 1$ hexo d 完结接下来我们就可以在 github 上看见我们提交的静态文件了，也可以通过 https://username.github.io/ 访问我们的博客了，username 改成你的github 用户名，一定要记得是https!! 如果你想发布文章、生成新页面、增加搜索或其他功能，在 Next 官方文档中你都能找到~ 再放一下官方文档： Next 官方文档：http://theme-next.iissnan.com/ 当然还有一些不在官方文档上的骚操作，等我更新吧… 如果在进行操作时有 bug ，欢迎留言~~]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习-神经网络(一)]]></title>
    <url>%2Fposts%2F6267%2F</url>
    <content type="text"><![CDATA[为什么要使用神经网络当特征太多时，计算的负荷会特别大，而普通的线性回归/逻辑回归都无法有效地处理这么多的特征，这个时候我们需要神经网络。 神经网络的模型表示首先，我们为神经网络里的每一层都增加了一个偏差单元，即每一层的0号下标的单元，它的值永远为1，而偏差单元我们只在当作输入时使用。 这时，我们把输入的样本特征$x_0x_1x_2x_3$看作第一层输入，$a_1a_2a_3$看作第一层的输出，把$a_0a_1a_2a_3$看作第二层的输入，$h_\theta(x)$看作第二层的输出。 PS：输出的意思就是经过一个激励函数$g(z)$的运算得出的，这里：$$g(z)=\frac{1}{1+e^{-z}}$$ 输出怎么得到从线性回归中我们能够知道：$h_\theta(x) = \theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3 = y$ 如果我们想要使用相同的输入 $x_0x_1x_2x_3$ 得出一个不一样的 $y_1$ ，我们必须要改变 $\theta$的值使得与第一次运算的 $\theta$ 不一样。 同理，当我们把输入的样本特征 $x_0x_1x_2x_3$ 看作第一层输入时，我们就需要三组不同的 $\theta$ 值，使得经过激励函数后得到三个不同的值 $a_1a_2a_3$。因此，我们就有了关于第一层输入的 $\theta$ 矩阵 $\Theta^{(1)}$，它的尺寸为 3*4。那么第二层输入的 $\theta$ 矩阵 $\theta^{(2)}$ 的尺寸则为 1*4。当然我们也可能会有许多次输入输出，如果我们进行多次的输入输出，$a_{i}^{\left( j \right)}$ 则代表第 $j$ 层的第 $i$ 个激活单元(输入)。${\theta }^{\left( j \right)}$代表从第 $j$ 层映射到第 $ j+1$ 层时的权重的矩阵，例如 ${\theta }^{\left( 1 \right)}$ 代表从第一层映射(输出)到第二层的权重的矩阵。其尺寸为：以第 $j+1$层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵。 输入-输出的过程因为 $a_1a_2a_3$ 是样本特征 $x_0x_1x_2x_3$ 与 $\Theta^{(1)}$ 经过激励函数后得到的值，因此此过程可写为： $a_{1}^{2}=g(\Theta_{10}^{1}x_{0}+\Theta_{11}^{1}x_{1}+\Theta_{12}^{1}x_{2}+\Theta_{13}^{1}x_{3})$ $a_{2}^{2}=g(\Theta_{20}^{1}x_{0}+\Theta_{21}^{1}x_{1}+\Theta_{22}^{1}x_{2}+\Theta_{23}^{1}x_{3})$ $a_{3}^{2}=g(\Theta_{30}^{1}x_{0}+\Theta_{31}^{1}x_{1}+\Theta_{32}^{1}x_{2}+\Theta_{33}^{1}x_{3})$ $h_\Theta(x)=g(\Theta_{10}^{2}a_{0}^{2}+\Theta_{11}^{2}a_{1}^{2}+\Theta_{12}^{2}a_{2}^{2}+\Theta_{13}^{2}a_{3}^{2})$ 上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。 我们可以知道：每一个 $a$ 都是由上一层所有的$x$和每一个$x$所对应的决定的。 （我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )） 把$x$, $\theta$, $a$ 分别用矩阵表示： 我们可以得到 $g(\theta \cdot X)=a$ 。 如果细分下去，我们就能够得到向量化的结果：$$g(\Theta^{(1)}\cdot{X^T})=a^{(2)}$$ 即： 以上是以第一层为例进行的说明，那么现在我们看第二层，则有$$g(\Theta^{(2)}\cdot a^{(2)})=h_\theta(x)$$我们令 ${z}^\left( 3 \right)={\theta }^{( 2 )}{a}^{( 2 )}$，则 $h_\theta(x)={a}^{\left(3\right)}=g({z}^{\left(3\right)})$。 更好的理解 其实神经网络就像是logistic regression，只不过我们把logistic regression中的输入向量$[ x_1\sim {x_3} ]$ 变成了中间层的$[ a_1^{(2)}\sim a_3^{(2)} ]$, 即: $h_\theta(x)=g( \Theta_0^{ 2 }a_0^{ 2 }+\Theta_1^{ 2 }a_1^{ 2 }+\Theta_{2}^{ 2 }a_{2}^{ 2 }+\Theta_{3}^{ 2 }a_{3}^{ 2 } )$ 我们可以把 $a_0, a_1, a_2, a_3$ 看成更为高级的特征值，也就是 $x_0, x_1, x_2, x_3$ 的进化体，并且它们是由 $x$与$\theta$决定的，因为是梯度下降的，所以 $a$ 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 $x$次方厉害，也能更好的预测新数据。这就是神经网络相比于逻辑回归和线性回归的优势。从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征 $x_1,x_2,…,{x}_{n}$ ，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 单层神经元计算的简化理解神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。当输入特征为布尔值（0或1）时，我们可以用一个单一的激活层可以作为二元逻辑运算符，为了表示不同的运算符，我们只需要选择不同的权重即可。 在理解之前，我们再复习一下关于激励函数与判定边界。当$g(z)$中 $z &gt; 0$ 时，$g(z)$ &gt; 0.5 ，假如我们的阈值就是 0.5，那么我们此时就把 $g(z)$ 的结果归为 1 ，反之则为 0。 逻辑与那么，如果我们现在有输出函数 $h_\theta(x) = g(\theta_0x_0+\theta_1x_1+\theta_2x_2)$，且 $\Theta = [-30,20,20]$，此时 $h_\theta(x) = g(-30+20x_1+20x_2)$，我们就能够得到$x_1x_2$分别取值时的结果对照表： 经过复习和上表的对照我们能够很轻松的理解。 此时$h_\theta(x)$ 得出的结果 等于 $x_1 AND x_2$ 得出的结果，因此我们就能够把此时的输出函数中进行的计算简化理解为：对输入做 逻辑与(AND)​ 运算。 逻辑或而当 $\Theta = [-10,20,20]$ 时，此时 $h_\theta(x) = g(-10+20x_1+20x_2)$ ，我们就能够得到$x_1x_2$分别取值时的结果对照表： 此时$h_\theta(x)$ 得出的结果 等于 $x_1 OR x_2$ 得出的结果，因此我们就能够把此时的输出函数中进行的计算简化理解为：对输入做 逻辑或(OR) 运算。 逻辑非当 $\Theta = [10,-20,0]$ 时，此时 $h_\theta(x) = g(10-20x_1)$ ，我们就能够知道$x_1$取 0 时，结果为 1，$x_1$取 1 时，结果为 0 。我们就能够把此时的神经元的作用等同于 逻辑非(NOT) XNOR(输入两个值相等时，结果为 1 )那么我们如何表示呢? 首先我们构造一个能表达 $(NOT x_1) AND (NOTx_2)$ 的神经元进行第一层计算，如图： 然后构造一个能表示 OR 的神经元进行第二层计算，再让这两层组合在一起： 这样我们就得到了一个能实现 $\text{XNOR}$ 功能的神经网络。 因此，我们能够组合三种简单的运算来逐渐构造出复杂的函数，这样我们也能得到更加复杂有趣的特征值。 这就是神经网络的厉害之处。 多类处理的神经网络我们在上面的神经网络仅仅输出了一个结果，也就是我们只做了二分类问题，那么我们如果想要进行多个类的分类呢? 很简单，我们只需要让结果值的数目等于你要分类的类数目。比如说：如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车，第三个值用于预测是否是摩托车，第四个值用于判断是否为卡车。 如果我们构造两个中间层进行，输出层4个神经元分别用来表示4类，那么神经网络图可能如下所示： 我们希望当输入人的图片时，输出的结果为 [1 , 0, 0, 0]，输入卡车图片时，输出的结果为 [0 , 0, 0, 1]。 这样，我们就实现了神经网络的多分类处理。 查看我的CSDN: https://blog.csdn.net/qq_28827635]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-正则化]]></title>
    <url>%2Fposts%2F1179%2F</url>
    <content type="text"><![CDATA[1. 正则化它可以改善或者减少过度拟合问题 2. 欠拟合(模型的高偏差)欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或者模型没有很好地捕捉到数据特征，不能够很好地拟合数据。 3. 过拟合(模型的高方差)为什么出现过拟合 特征过多 训练集数据较少 模型复杂 对过拟合的理解如果我们拟合一个高阶多项式，那么这个函数能很好的拟合训练集能拟合几乎所有的训练数据，这就面临可能函数太过庞大的问题，即变量太多。同时如果我们没有足够的数据去约束这个变量过多的模型，就会出现过度拟合的情况。虽然训练出的模型能够很好的拟合训练集的样本数据，但很有可能无法泛化新样本。 如何解决过拟合 尽量减少选取特征数量。后面会学到模型选择算法 ，它能够自动选择采用哪些特征变量，且自动舍弃不需要的变量 正则化保留所有特征变量，但是减少参数$\theta_j$的大小 4. 怎么应用正则化思想与做法修改代价函数，从而收缩(惩罚)所有的参数值，因为我们并不知道具体的去收缩(惩罚)哪些参数， 修改后的代价函数如下：$$J\left(\theta\right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{({h_\theta}({x}^{(i)})-{y}^{(i)})^{2}+\lambda \sum\limits_{j=1}^{n}\theta_{j}^{2}]}$$$\lambda $又称为正则化参数（Regularization Parameter），它能够平衡代价函数，使$\theta_j$尽可能的小。 注：根据惯例，我们的$j$是从1开始的，也就是我们不对${\theta_{0}}$ 进行惩罚。 举一个例子我们看这个假设函数: $h_\theta\left( x \right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^2+\theta_{3}x_{3}^3+\theta_{4}x_{4}^4$ 。通常地，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。于是我们将修改代价函数，在其中${\theta_{3}}$和${\theta_{4}}$ 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的${\theta_{3}}$和${\theta_{4}}$。修改后如下 :$$\underset{\theta}{\mathop\min }\,\frac{1}{2m}[\sum\limits_{i=1}^{m}{\left({h}_{\theta }\left( {x}^{(i)} \right)-{y}^{(i)} \right)^{2}+1000\theta _{3}^{2}+10000\theta _{4}^{2}]}$$但是正是因为我们并不知道具体的哪一个$\theta$是高次项，因此我们只能去收缩(惩罚)所有参数。 但是如果我们令 $\lambda$ 的值很大的话，那么$\theta $（不包括${\theta_{0}}$）都会趋近于0，这样我们所得到的只能是一条平行于$x$轴的直线。所以对于正则化，我们要取一个合理的 $\lambda$ 的值，这样才能更好的应用正则化。 5. 正则化线性回归正则化代价函数$$J\left(\theta\right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[(({h_\theta}({x}^{(i)})-{y}^{(i)})}^{2}+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2})]}$$ 正则化梯度下降要使梯度下降法令正则化后的线性回归代价函数最小化，因为我们没有对$\theta_0$进行正则化，所以梯度下降算法有两种情形：$${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}(({h_\theta}({x}^{(i)})-{y}^{(i)})x_{0}^{(i)})$$ $${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left(i\right)}+\frac{\lambda }{m}{\theta_j}]$$ 对第二个式子进行变化后，可得:$${\theta_j}:={\theta_j}(1-a\frac{\lambda }{m})-a\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left(i\right)}$$可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta $值减少了一个额外的值。 PS：梯度下降仍然是对$J(\theta)$进行最小化，通过求导，得出梯度下降算法 正则化正规方程 注：图中的矩阵尺寸为 $(n+1)*(n+1)$。 值得一提的是，哪怕此时$X$不可逆，经过$\lambda$相加变化后的矩阵将是可逆的。 6. 正则化逻辑回归正则化代价函数$$J\left(\theta\right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{y}^{(i)}\log \left({h_\theta}\left({x}^{(i)}\right)\right)-\left(1-{y}^{(i)} \right)\log\left(1-{h_\theta}\left({x}^{(i)}\right) \right)]}+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta _{j}^{2}$$ ​ ps：注意这里$\lambda$的仍为$\frac{1}{2m}$ 正则化梯度下降类似地，因为我们没有对$\theta_0$进行正则化，所以梯度下降算法有两种情形：$${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}(({h_\theta}({x}^{(i)})-{y}^{(i)})x_{0}^{(i)})$$ $${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left( i \right)}+\frac{\lambda }{m}{\theta_j}]$$ 看起来和线性回归的一模一样，实际上我们知道这里 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$，所以与线性回归不同。 PS：值得注意的是，${\theta_{0}}$仍然不参与其中的任何一个正则化。 注:泛化能力（generalization ability）泛化能力是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的模型也能给出合适的输出，该能力称为泛化能力。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-逻辑回归]]></title>
    <url>%2Fposts%2F12210%2F</url>
    <content type="text"><![CDATA[什么是逻辑回归逻辑回归算法是分类算法，可能它的名字里出现了“回归”让我们以为它属于回归问题，但逻辑回归算法实际上是一种分类算法，它主要处理当 $y$ 取值离散的情况，如：1 0 。 为什么不使用线性回归算法处理分类问题假设我们遇到的问题为 二分类问题，那么我们可能将结果分为负向类和正向类，即$y\in0,1$ ，其中 0 表示负向类，1 表示正向类。如果我们使用线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，但是我们需要的假设函数输出值需要在0到 1 之间，因此我们需要用到逻辑回归算法。 逻辑回归的假设函数与理解逻辑回归的假设函数 sigmoid function 表示方法 :$$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$$ 理解记忆:其实里面的$\theta_Tx$就是线性回归时的假设函数 h(x) ，$$h(x) = \theta^Tx = \sum_{j=0}^{n}{\theta_jx_j}$$而逻辑回归的假设函数其实就是将线性回归的表达式 h(x) 以 z 的形式代入到了 S 型函数(sigmoid function) 中 :$$g(h(x)) = g(z) = \frac{1}{1+e^{-z}}$$ps: 这里我们用$h(x)$表示的是线性回归的假设函数，之后的$h$都将表示 逻辑回归的假设函数。值得一提的是S型函数和我们的假设函数没关系，它只是一个输出值在0~1之间的函数，仅此而已。我们做的只是把之前得到的线性回归假设函数给代入进去形成逻辑回归的假设函数，这样 对假设函数的解释 :给定 x ，根据选择的参数计算出y = 1 的概率 ，具体的概率公式如下 :$$h_\theta(x) = P(y=1|x; \theta)$$ Sigmoid - Python:1234import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z)) 判定边界(decision boundary)如何得出判定边界 :在 逻辑回归的假设函数中，但凡输出结果 $h_\theta(x)$大于 0.5 的，我们都将预测结果 $y$ 收敛于 1 ；小于 0.5 的，收敛于 0 ；而恰好等于 0.5 的，收敛1 或 0 都可以，我们可以自己设定它如何收敛。由此，我们的输出值就都在 0 到 1 之间了。而当 $h_\theta(x)$ 大于 0.5 时，$\theta^Tx$ 大于 0.5， $h_\theta(x)$ 小于 0.5 时，$\theta^Tx$ 小于 0.5， $h_\theta(x)$ 等于 0.5 时，$\theta^Tx$ 等于 0.5。当然，具体的阈值是可以调整的，比如说你是一个比较保守的人，可能将阈值设为 0.9 ，也就是说有超过 90% 的把握，才相信这个$y$收敛于 1 。 由此，我们能够绘制出判定边界 :$$\theta^Tx = 0$$ 关于判定边界 : 决策边界不是训练集的属性，而是假设本身及其参数的属性 只要给出确定的参数$\theta$，就确定了我们的决策边界 高阶多项式(多个特征变量)能够让我们得到更复杂的决策边界 逻辑回归的代价函数，梯度下降自动拟合$\theta$，以及代价函数的推导过程逻辑回归的代价函数 :$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})$$其中$Cost$ :$$Cost(h_\theta(x),y) = -ylog(h_\theta(x)) - (1-y)log(1-h_\theta(x))$$因此$J(\theta)$ :$$J(\theta) =-\frac{1}{m}[\sum_{i=1}^{m} y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$ 使用对数几率的原因: 代价函数 $J(\theta)$ 会是一个凸函数，并且没有局部最优值。否则我们的代价函数将是一个非凸函数。 逻辑回归的梯度下降算法 :Repeat {$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$$(simultaneously update all )} 求导后得到： Repeat {$$\theta_j := \theta_j - \alpha\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$ (simultaneously update all )} ps : 逻辑回归梯度下降结果与线性回归梯度下降结果一致，但其中的$h_\theta(x)$并不一样，因此本质上是不同的。 关于特征缩放和均值归一化:思想:在有多个特征的情况下，如果你能确保这些不同的特征都处在一个相近的范围，这样梯度下降法就能更快地收敛。使代价函数$J(θ)$的轮廓图的形状就会变得更圆一些。 做法:一般地，我们执行特征缩放时，我们通常将特征的取值约束到接近−1到+1的范围。其中，特征x0总是等于1，因此这已经是在这个范围内了，但对于其他的特征，我们需要通过除以不同的数来让它们处于同一范围内。除了在特征缩放中将特征除以最大值以外，有时候我们也会进行一个称为均值归一化的操作:$$x_n = \frac{x_n-μ_n}{s_n}$$其中，$μ_n$是平均值，$s_n$是标准差 好处: 更好的进行梯度下降，提高代价函数的收敛速度 提高代价函数求解的精度 更适合解决大型机器学习的问题​ 其他相较于梯度下降算法更好的的令代价函数最小的算法(高级优化[超纲])常用算法: 共轭梯度(Conjugate Gradient) 局部优化法(BFGS - Broyden fletcher goldfarb shann) 有限内存局部优化法(LBFGS) 好处: 这些算法内部有一个智能的内部循环(线性搜索算法)，能够尝试不同的 $\alpha​$ 并自动的选择一个好的学习速率 $\alpha​$ ，这样就不需要手动选择 $\alpha​$ 收敛速度通常比梯度下降算法更快速 缺点: 比梯度下降算法更加复杂 使用逻辑回归算法解决多类别问题思想:将多分类问题拆分成多个二分类问题并得出多个模型。最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 做法:我们将多个类中的一个类标记为正向类（$y=1$），然后将其他所有类都标记为负向类，这个模型记作$h_\theta^{\left( 1 \right)}\left( x \right)$。接着，类似地我们选择另一个类标记为正向类（$y=2$），再将其它类都标记为负向类，将这个模型记作 $h_\theta^{\left( 2 \right)}\left( x \right)$,依此类推。最后我们得到一系列的模型简记为： $h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)$其中：$i=\left( 1,2,3….k \right)$ 。然后我们将这多个逻辑回归分类器进行训练并得出最终模型：$h_\theta^{\left( i \right)}\left( x \right)$， 其中 $i$ 对应每一个可能的 $y=i$，最后，当我们需要进行预测时，输入一个新的 $x$ 值，我们要做的就是在这多个分类器里面输入 $x$，然后在多个分类器得出的结果中，选出一个最大的$ i$，即$\mathop{\max}\limits_i\,h_\theta^{\left( i \right)}\left( x \right)$。 逻辑回归梯度下降中代价函数求导过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-向量化]]></title>
    <url>%2Fposts%2F16142%2F</url>
    <content type="text"><![CDATA[向量化 - 传统累加运算 - 代码实现:1234567891011121314151617181920import timeimport numpy as np# 定义两组向量vector1 = np.random.rand(100000)vector2 = np.random.rand(100000)# 使用向量化start_time = time.time() # 开始时间res = np.dot(vector1, vector2) # 向量直接相乘得到最终结果end_time = time.time() # 结束时间print("Vectorized: " + str((end_time - start_time)*1000) + "ms" + " res =" + str(res))# 使用for循环res = 0start_time = time.time() # 开始时间for i in range(100000): # 传统的累加运算,需要累加100000次 res += vector1[i] * vector2[i]end_time = time.time() # 结束时间print("For loop: " + str((end_time - start_time)*1000) + "ms" + " res =" + str(res)) 结果对比:12Vectorized :1.0001659393310547ms res =24969.775960643143For loop:79.94818687438965ms res =24969.775960642968 ​ 从执行结果来看向量化的运算速度要比非向量化的运算快了近80倍，而这个对比结果还会随着运算集的数目增加而增加。 为什么:​ CPU 与 GPU 都能够使用 SIMD 指令进行并行化操作，即以同步方式，在同一时间内执行同一条指令。一般来讲可扩展的深度学习都在 GPU 上做，但其实 CPU 也不是太差，只是没有 GPU 擅长。 ​ 而 Python 的 numpy 的一些内置函数能够充分利用并行化来加速运算，比如 np.dot，因此，不到逼不得已，还是不要使用 for 循环吧 注:​ GPU - 图形处理器也，叫做图像处理单元，显卡的处理器。与 CPU 类似，只不过 GPU 是专为执行复杂的数学和几何计算而设计的，这些计算是图形渲染所必需的。​ SIMD - 单指令多数据流，以同步方式，在同一时间内执行同一条指令。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（十七）—sklearn之无监督学习-K-means算法]]></title>
    <url>%2Fposts%2F26879%2F</url>
    <content type="text"><![CDATA[一、无监督学习概述 什么是无监督学习之所以称为无监督，是因为模型学习是从无标签的数据开始学习的。 无监督学习包含算法 聚类 K-means(K均值聚类) 降维 PCA 二、K-means原理 K-means聚类步骤 随机设置K个特征空间内的点作为初始的聚类中心 对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别 接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值） 如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程 图片助解 三、K-means - APIsklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’) k-means聚类 n_clusters 开始的聚类中心数量 init 初始化方法，默认为’k-means ++’ labels_ 默认标记的类型，可以和真实值比较（不是值比较） 四、K-means性能评估指标1. 轮廓系数​$$sc_i = \frac{b_i-a_i}{max(b_i,a_i)}$$ 注： 对于每个 i 点是已聚类数据中的样本点 ，$b_i$ 为 i 点到其它簇中所有样本点的距离的最小值，$a_i$ 为 i 到自身簇的所有样本点的距离的平均值。最终计算出所有的样本点的轮廓系数平均值 2. 轮廓系数值分析 根据公式极端值考虑： 如果 $b_i &gt;&gt;a_i$ 那么公式结果趋近于 1，效果好。 如果 $a_i&gt;&gt;b_i$ 那么公式结果趋近于 -1，效果不好。 轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。 3. 轮廓系数 - APIsklearn.metrics.silhouette_score(X, labels) 计算所有样本的平均轮廓系数 X 特征值 labels 被聚类标记的目标值 未完待续….]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（十六）—sklearn之模型保存和加载]]></title>
    <url>%2Fposts%2F64982%2F</url>
    <content type="text"><![CDATA[一、sklearn - 模型的保存和加载 - APIfrom sklearn.externals import joblib 保存 joblib.dump(rf, ‘test.pkl’) 加载 estimator = joblib.load(‘test.pkl’) 二、示例助解 保存1234567# 使用线性模型进行预测# 使用正规方程求解lr = LinearRegression()# 进行训练lr.fit(x_train, y_train)# 保存训练完结束的模型joblib.dump(lr, "test.pkl") 加载123# 通过已有的模型去预测model = joblib.load("test.pkl")model.predict(x_test)]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（十五）—sklearn之分类算法-逻辑回归、精确率、召回率、ROC、AUC]]></title>
    <url>%2Fposts%2F46965%2F</url>
    <content type="text"><![CDATA[逻辑回归虽然名字中带有回归两字，但它实际是一个分类算法。 一、逻辑回归的应用场景 广告点击率 是否为垃圾邮件 是否患病 金融诈骗 虚假账号 看到上面的例子，我们可以发现其中的特点，那就是都属于两个类别之间的判断。 逻辑回归就是解决二分类问题的利器 二、逻辑回归的原理 输入逻辑回归的输入其实就是线性回归 即：$$h_\theta(x)=\theta^Tx$$ 激活函数(sigmoid)逻辑回归的本质就是把输入到线性回归产生的结果再输入到激活函数中然后输出。 即：$$g(h_\theta(x)) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}$$输出的结果为：[0,1]区间中的一个概率值，默认的阈值为 0.5。 如：假设有两个类别A，B，并且我们认为阈值为0.5，输出结果超过阈值则预测为 A 类。那么现在有一个样本的输入到逻辑回归输出结果 0.6，这个概率值超过 0.5，意味着我们训练或者预测的结果就是A类别。那么反之，如果得出结果为 0.3 那么，训练或者预测结果就为B类别。 三、损失以及优化那么如何去衡量逻辑回归的预测结果与真实结果的差异呢？ 损失逻辑回归的损失，称之为对数似然损失，公式如下：$$Cost(h_\theta(x),y) = -ylog(h_\theta(x)) - (1-y)log(1-h_\theta(x))$$上式为针对单条数据的损失函数， 那么，我们能够得出总的损失函数，公式如下：$$\sum_{i=1}^{m} y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)})$$ 优化我们同样可以使用梯度下降优化算法，去减少损失函数的值。 这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。 四、逻辑回归API sklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0) solver 优化求解方式（默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数） 默认使用的是 sag，即根据数据集自动选择，随机平均梯度下降 penalty 正则化的种类 默认为 L2 C 正则化力度 使用 SGDClassifier 实现逻辑回归 API，SGDClassifier(loss=”log”, penalty=” “) SGDClassifier实现了一个普通的随机梯度下降学习，也可以通过设置average=True，实现随机平均梯度下降。 loss，设置 log ，即逻辑回归中的对数损失函数 五、案例：癌症分类预测-良／恶性乳腺癌肿瘤预测 数据介绍原始数据的下载地址：https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/ 下载数据：breast-cancer-wisconsin.data 数据描述 699条样本，共11列数据，第一列用语检索的id，后9列分别是与肿瘤 相关的医学特征，最后一列表示肿瘤类型的数值。 包含16个缺失值，用 ”?” 标出。 步骤分析 缺失值处理 标准化处理 逻辑回归预测 完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344import pandas as pdimport numpy as npfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler# 获取数据并添加字段名column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']cancer=pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data",names=column_name)cancer.head()# 缺失值处理cancer=cancer.replace(to_replace="?",value=np.nan)cancer=cancer.dropna()# 数据集划分# 1&gt; 提取特征数据与目标数据x=cancer.iloc[:,1:-2]y=cancer.iloc[:,-1]# 2&gt; 划分数据集x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)# 标准化处理transfer=StandardScaler()x_train=transfer.fit_transform(x_train)x_test=transfer.transform(x_test)# 模型训练# 创建一个逻辑回归估计器estimator=LogisticRegression()# 训练模型，进行机器学习estimator.fit(x_train,y_train)# 得到模型，打印模型回归系数，即权重值print("logist回归系数为:\n",estimator.coef_)# 模型评估# 方法1：真实值与预测值比对y_predict=estimator.predict(x_test)print("预测值为:\n",y_predict)print("真实值与预测值比对:\n",y_predict==y_test)# 方法2：计算准确率print("直接计算准确率为:\n",estimator.score(x_test,y_test)) 六、二分类 - 模型评估 - 精确率、召回率 与 $F_1-score$ 混淆矩阵在分类任务下，预测结果与正确标记之间存在四种不同的组合，构成混淆矩阵(适用于多分类) | | 预测为正例 | 预测为假例 || :——–: | :——–: | :——–: || 真实为正例 | 真正例(TP) | 伪反例(FN) || 真实为假例 | 伪正例(FP) | 真反例(TN) | 精确率$$\frac{真正例}{预测为正例}$$ 召回率 - (查得全，对正样本的区分能力)$$\frac{真正例}{真实为正例}$$ $F_1 - score$ - (反映了模型的稳健型)$$F_1 = \frac{2TP}{2TP+FP+FN}$$ 分类评估报告 - APIsklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None ) y_true 真实目标值 y_pred 估计器预测值 labels 指定类别对应的数字 target_names 目标类别名称 return 每个类别精确率、召回率、$F_1$ 系数以及该类占样本数 如：classification_report(y_test, lr.predict(x_test), labels=[2, 4], target_names=[&#39;良性&#39;, &#39;恶性&#39;]) 七、二分类 - 模型评估 - ROC曲线与AUC指标 如何衡量样本不均衡下的评估？ ​ 假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例),准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题。 TPR$$TPR = \frac{TP}{TP + FN}$$ 真实为真时预测为真 占 真实为真的 比例 FPR$$FPR = \frac{FP}{FP + TN}$$ 真实为假时预测为真 占 真实为假的 比例 ROC曲线ROC 曲线的横轴就是FPR，纵轴就是TPR AUC指标 AUC 的概率意义是随机取一对正负样本，正样本得分大于负样本的概率 AUC 的最小值为0.5，最大值为1，取值越高越好 AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。 0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 AUC - APIfrom sklearn.metrics import roc_auc_score sklearn.metrics.roc_auc_score(y_true, y_score) 计算ROC曲线面积，即AUC值 y_true 每个样本的真实类别，必须为0(反例),1(正例)标记 y_score 预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值 return AUC值 关于AUC AUC只能用来评价二分类 AUC非常适合评价样本不平衡中的分类器性能 八、案例 - 精确率、召回率、AUC值12345678910# 接上面的肿瘤预测代码#打印精确率、召回率、F1 系数以及该类占样本数print("精确率与召回率为:\n",classification_report(y_test,y_predict,labels=[2,4],target_names=["良性","恶性"]))###模型评估#ROC曲线与AUC值# 把输出的 2 4 转换为 0 或 1y_test=np.where(y_test&gt;2,1,0) # 大于2就变为1，否则变为0print("AUC值:\n",roc_auc_score(y_test,y_predict))]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（十四）—sklearn之岭回归（线性回归的改进）]]></title>
    <url>%2Fposts%2F15321%2F</url>
    <content type="text"><![CDATA[带有 L2 正则化的线性回归就是岭回归。 岭回归，其实也是一种线性回归。 只不过在算法建立回归方程时候，加上正则化的限制，从而达到解决过拟合的效果。 加上正则化，也就是使权重满足划分正确结果的同时尽量的小 一、岭回归 - API 岭回归 - APIsklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver=”auto”, normalize=False) 具有 L2 正则化的线性回归 alpha 正则化力度，也叫 λ λ 取值在0~1或者1~10 fit_intercept 偏置，默认True solver 会根据数据自动选择优化方法 如果数据集、特征都比较大，可以设置为 ‘sag’ ，进行随机梯度下降优化 normalize 数据是否进行标准化 normalize=True 时会进行标准化操作，我们就可以不使用 StandardScaler 进行标准化操作啦 Ridge.coef_ 回归权重 Ridge.intercept_ 回归偏置 All last four solvers support both dense and sparse data. However,only ‘sag’ supports sparse input when fit_intercept is True. SGDRegressor-API 实现岭回归SGDRegressor(penalty=’l2’, loss=”squared_loss”) 使用梯度下降API实现岭回归 penalty 乘法的意思，表示使用L2 loss 损失函数使用什么 squared_loss - 最小二乘 推荐使用 Ridge，因为它实现了 SAG 优化 岭回归 - 实现了交叉验证 - APIsklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin) 具有 L2 正则化的线性回归，可以进行交叉验证 coef_ 回归系数 二、案例 - 岭回归 - 波士顿房价预测 在之前案例基础上使用岭回归 完整代码1234567891011121314151617181920212223242526272829303132from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import Ridgefrom sklearn.datasets import load_bostonfrom sklearn.metrics import mean_squared_error# 获取数据boston = load_boston()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=6)# 特征工程：标准化# 1）实例化一个转换器类transfer = StandardScaler()# 2）调用fit_transformx_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 岭回归的预估器流程estimator = Ridge()estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print("岭回归求出模型参数的方法预测的房屋价格为：\n", y_predict)# 打印模型相关属性print("岭回归求出的回归系数为：\n", estimator.coef_)print("岭回归求出的偏置为：\n", estimator.intercept_)# 模型评估——均方误差error = mean_squared_error(y_test, y_predict)print("岭回归的均方误差为：\n", error)]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（十三）—sklearn之欠拟合与过拟合]]></title>
    <url>%2Fposts%2F47108%2F</url>
    <content type="text"><![CDATA[当训练数据训练的很好误差也不大的时候，为什么在测试集上面进行预测会有较大偏差呢？ 当算法在某个数据集当中出现这种情况，可能就出现了过拟合现象。 一、什么是过拟合与欠拟合 欠拟合一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。(模型过于简单) 过拟合一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂) 二、原因以及解决办法 原因 欠拟合原因： 学习到数据的特征过少 模型复杂度较低 正则化系数过大 过拟合原因： 训练数据过少 原始特征过多 模型过于复杂 正则化系数过小 解决办法几种降低过拟合和欠拟合风险的方法 三、正则化 什么是正则化在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化 注：调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果 正则化类别 L2 正则化 作用：可以使得其中一些W的都很小，都接近于 0，削弱某个特征的影响 优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象 Ridge回归(岭回归) L1 正则化 作用：可以使得其中一些W的值直接为 0，删除这个特征的影响 LASSO回归(稀疏) 原理 线性回归的损失函数用最小二乘法，等价于当预测值与真实值的误差满足正态分布时的极大似然估计； 岭回归的损失函数，是最小二乘法+L2范数，等价于当预测值与真实值的误差满足正态分布，且权重值也满足正态分布（先验分布）时的最大后验估计； LASSO的损失函数，是最小二乘法+L1范数，等价于当预测值与真实值的误差满足正态分布，且权重值满足拉普拉斯分布（先验分布）时的最大后验估计]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（十二）—sklearn之线性回归]]></title>
    <url>%2Fposts%2F16419%2F</url>
    <content type="text"><![CDATA[一、线性回归应用场景 房价预测 销售额度预测 金融：贷款额度预测、利用线性回归以及系数分析因子 二、线性回归的原理 什么是回归在机器学习中，回归就是拟合的意思，我们需要找出一个模型来拟合(回归)数据。 什么是线性回归 线性回归是：利用回归方程(函数)，对特征值和目标值之间关系进行建模的一种分析方式。 特征值和目标值可以是一个或多个，特征值和目标值可以看作函数意义上的自变量和因变量。 特点 只有一个自变量的情况称为单变量回归。 多于一个自变量的情况称为多元回归。 通用公式$$h(\theta) = \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + … + b = \theta^Tx + b$$ 其中： $\theta = (\theta_1,\theta_2,…,\theta_n,b)^T$ $x = (x_1,x_2,…,x_n,1)^T$ 线性回归的特征与目标的关系线性回归当中线性模型有两种，一种是线性关系，另一种是非线性关系。 三、损失函数假设真实值为 $y$ ，我们的预测值为 $h(\theta)$ ，真实结果与我们预测的结果之间可能存在一定的误差。 既然存在这个误差，那我们就可以将这个误差给衡量出来。 损失函数$$J(\theta) = (h_\theta(x_1)-y_1)^2 +(h_\theta(x_2)-y_2)^2 + … +(h_\theta(x_3)-y_3)^2 = \sum_{i=1}^{m}(h_\theta(x_i)-y_i)^2$$ $y_i$ 为第 i 条训练数据的真实值。 $h(x_i)$ 为第 i 条训练数据的预测函数产生的预测值，且 $h(x_i)$ 是关于参数 $\theta$ 的函数。 又称最小二乘法 四、优化算法如何去减少损失函数的损失使我们预测的更加准确呢？ 我们一直说机器学习有自动学习的能力，在线性回归这里更是能够体现。 这里可以通过一些优化方法去优化（本质是求导）回归的总损失！ 正规方程$$\theta = (X^TX)^{-1}X^Ty$$ 理解：X 为特征值矩阵，y 为目标值矩阵。直接求到最好的结果。 缺点：当特征过多过复杂时，求解速度太慢并且得不到结果。 梯度下降(Gradient Descent)$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$$ 理解：α 为学习速率，需要手动指定（超参数），$\frac{\partial}{\partial\theta_j} J(\theta)$ 整体表示下降方向 沿着这个函数下降的方向，最后就能找到山谷的最低点，然后更新 $\theta$ 值 使用：面对训练数据规模十分庞大的任务 ，能够找到较好的结果 梯度下降步骤 随机初始化一个点 自主学习 达到最小，终止学习 有了梯度下降这样一个优化算法，回归就有了”自动学习”的能力 五、sklearn - 线性回归 - APIsklearn提供给我们两种实现的API， 可以根据选择使用。 正规方程 APIsklearn.linear_model.LinearRegression(fit_intercept=True) 通过正规方程优化 fit_intercept 是否计算偏置 LinearRegression.coef_ 回归系数 LinearRegression.intercept_ 偏置 梯度下降 APIsklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=True, learning_rate =’invscaling’, eta0=0.01) SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。 loss 损失类型 loss=”squared_loss”: 普通最小二乘法 fit_intercept 是否计算偏置 learning_rate 学习率填充 ‘constant’ $\eta = \eta_0$ ‘optimal’ $\eta = \frac{1.0}{\alpha * (t + t_0)} $ [default] ‘invscaling’ $\eta = \frac{\eta_0}{pow(t, power_t)}$ power_t=0.25 存在父类当中 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用 $\eta_0$来指定学习率。 SGDRegressor.coef_ 回归系数 SGDRegressor.intercept_ 偏置 回归性能评估 API 均方误差(Mean Squared Error)评价机制： $$MSE = \frac{1}{m}\sum_{i=1}^{m}(y^i-y^{real})^2$$ 注：$y^i$ 为预测值，$y^{real}$ 为真实值 sklearn.metrics.mean_squared_error(y_true, y_pred) 均方误差回归损失 y_true 真实值 y_pred 预测值 return 浮点数结果 六、案例 - 线性回归 - 波士顿房价预测 数据介绍 给定的这些特征，是专家们得出的影响房价的结果属性。我们此阶段不需要自己去探究特征是否有用，只需要使用这些特征。 步骤分析回归当中的数据大小不一致，是否会导致结果影响较大。所以需要做标准化处理。 数据分割与标准化处理 回归预测 线性回归的算法效果评估 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegression,SGDRegressorfrom sklearn.datasets import load_bostonfrom sklearn.metrics import mean_squared_error# 获取数据boston = load_boston()# 划分数据集x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=8)# 特征工程，标准化# 1&gt; 创建一个转换器transfer = StandardScaler()# 2&gt; 数据标准化x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 方法一：正规方程求解# 模型训练# 1&gt; 创建一个估计器estimator_1 = LinearRegression()# 2&gt; 传入训练数据，进行机器学习estimator_1.fit(x_train,y_train)# 3&gt; 打印梯度下降优化后的模型结果系数print(estimator_1.coef_)# 4&gt; 打印梯度下降优化后的模型结果偏置print(estimator_1.intercept_)# 方法二：梯度下降求解# 模型训练# 1&gt; 创建一个估计器，可以通过调参数，找到学习率效果更好的值estimator_2 = SGDRegressor(learning_rate='constant', eta0=0.001)# 2&gt; 传入训练数据，进行机器学习estimator_2.fit(x_train,y_train)# 3&gt; 打印梯度下降优化后的模型结果系数print(estimator_2.coef_)# 4&gt; 打印梯度下降优化后的模型结果偏置print(estimator_2.intercept_)# 模型评估# 使用均方误差对正规方程模型评估y_predict = estimator_1.predict(x_test)error = mean_squared_error(y_test,y_predict)print('正规方程优化的均方误差为:\n',error)# 使用均方误差对梯度下降模型评估y_predict = estimator_2.predict(x_test)error = mean_squared_error(y_test,y_predict)print('梯度下降优化的均方误差为:\n',error) 七、正规方程与梯度下降比较 梯度下降 正规方程 需要选择学习率 不需要 需要迭代求解 一次运算得出 特征数量较大可以使用 需要计算方程，时间复杂度高O(n3) 适用大数据集 适用小数据集]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（十一）—sklearn之随机森林]]></title>
    <url>%2Fposts%2F39976%2F</url>
    <content type="text"><![CDATA[一、什么是集成学习方法集成学习通过建立几个模型组合的来解决单一预测问题。 它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。 这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。 二、什么是随机森林在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。 例如, 如果你训练了5个树, 其中有4个树的结果是True, 1个数的结果是False, 那么最终投票结果就是True 三、随机森林原理过程学习算法根据下列算法而建造每棵树： 用N来表示训练用例（样本）的个数，M表示特征数目。 1 一次随机选出一个样本，重复N次， （有可能出现重复的样本） 2 随机去选出m个特征, m &lt;&lt;M，建立决策树 采取bootstrap抽样 四、为什么采用BootStrap抽样 为什么要随机抽样训练集？ 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的 为什么要有放回地抽样？ 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。 五、sklearn - 随机森林 - API class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2) 随机森林分类器 n_estimators： integer，optional（default = 10） 森林里的树木数量 120,200,300,500,800,1200 criteria： string，可选（default =“gini”） 分割特征的测量方法 max_depth： integer或None，可选（默认=无） 树的最大深度 5,8,15,25,30 max_features=”auto”,每个决策树的最大特征数量 If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features. bootstrap： boolean，optional（default = True） 是否在构建树时使用放回抽样 min_samples_split： 节点划分最少样本数 min_samples_leaf： 叶子节点的最小样本数 超参数： n_estimator max_depth min_samples_split min_samples_leaf 六、案例 - 随机森林 - 随机森林预测tanic生存状况¶ 完整代码1234567891011121314151617181920212223from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCV# 1&gt; 实例化一个估计器estimator=RandomForestClassifier()# 2&gt; 网格搜索优化随机森林模型param_dict=&#123;"n_estimators":[120,200,300,500,800,1200],"max_depth":[5,8,15,25,30]&#125;estimator=GridSearchCV(estimator,param_grid=param_dict,cv=5)# 3&gt; 传入训练集，进行模型训练estimator.fit(x_train,y_train)# 4&gt; 模型评估# 方法1，比较真实值与预测值y_predict=estimator.predict(x_test)print("预测值为:\n",y_predict)print("比较真实值与预测值结果为:\n",y_predict==y_test)# 方法2,计算模型准确率print("模型准确率为:\n",estimator.score(x_test,y_test))print("在交叉验证中最的结果:\n",estimator.best_score_)print("最好的参数模型:\n",estimator.best_estimator_)print("每次交叉验证后的结果准确率为/n",estimator.cv_results_)]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（十）—sklearn之决策树]]></title>
    <url>%2Fposts%2F51802%2F</url>
    <content type="text"><![CDATA[一、决策树分类概述 介绍决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。 原理 信息熵 信息增益 二、信息熵 定义 $H$ 的专业术语称之为信息熵，单位为比特。 公式$$H(X) = \sum_{i=1}^{n}p(x_i)I(x_i) = - \sum_{i=1}^{n}P(x_i)log_2P(x_i)$$ $I(x)$ 用来表示随机变量的信息，$p(x_i)$ 指是当 $x_i$ 发生时的概率。 log可以以别的数为底，只不过值会不同罢了 熵只依赖X的分布，和X的取值没有关系，熵是用来度量不确定性，当熵越大，概率说X=xi的不确定性越大，反之越小，在机器学期中分类中说，熵越大即这个类别的不确定性更大，反之越小 来自百度百科 信息是个很抽象的概念。人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。比如一本五十万字的中文书到底有多少信息量。 直到1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题。信息熵这个词是C．E．香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。 信息论之父克劳德·艾尔伍德·香农第一次用数学语言阐明了概率与信息冗余度的关系。 来自&lt;数学之美&gt; 系统中各种随机性的概率越均等，信息熵越大，反之越小。 从香农给出的数学公式上可以看出，信息熵其实是一个随机变量信息量的数学期望。 查看更多：https://blog.csdn.net/saltriver/article/details/53056816 示例助解举一个的例子：对游戏活跃用户进行分层，分为高活跃、中活跃、低活跃，游戏A按照这个方式划分，用户比例分别为20%，30%，50%。游戏B按照这种方式划分，用户比例分别为5%，5%，90%。那么游戏A对于这种划分方式的熵为：$$H(A) = -(0.2log_{2}0.2+0.3log_{2}0.3+0.5log_{2}0.5) = 1.485$$同理游戏B对于这种划分方式的熵为：$$H(B) = -(0.05log_{2}0.05+0.05log_{2}0.05+0.9log_{2}0.9) = 0.569$$游戏A的熵比游戏B的熵大，所以游戏A的不确定性比游戏B高。用简单通俗的话来讲，游戏B要不就在上升期，要不就在衰退期，它的未来已经很确定了，所以熵低。而游戏A的未来有更多的不确定性，它的熵更高。 三、 信息增益 - 决策树的划分依据之一 定义特征A 对 训练数据集D 的 信息增益 g(D,A) ,定义为 集合D 的 信息熵H(D) 与 特征A条件下 D 的 信息条件熵H(D|A) 的差。 公式$$g(D,A) = H(D) - H(D|A)$$ 公式的详细解释： 信息熵的计算：$$H(D) = - \sum^{K}_{k=1}\frac{|C_k|}{|D|}log_{2}\frac{|C_k|}{|D|}$$ 条件熵的计算： $$H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=- \sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum^{K}{k=1}\frac{|D{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}$$ 注： $C_k$ 表示属于某个类别的样本数 信息增益表示得知特征X的信息而息的不确定性减少的程度使得类Y的信息熵减少的程度 这句话应该有点毛病，太绕了，没有理解透，马上补，然后再回来修改 示例助解假设有下表样本： 第一列为QQ，第二列为性别，第三列为活跃度，最后一列用户是否流失。 我们要解决一个问题：性别和活跃度两个特征，哪个对用户流失影响更大？我们通过计算信息熵可以解决这个问题。 | QQ | gender | active_info | is_lost || :–: | :—-: | :———: | :—–: || 1 | 男 | 高 | 0 || 2 | 女 | 中 | 0 || 3 | 男 | 低 | 1 || 4 | 女 | 高 | 0 || 5 | 男 | 高 | 0 || 6 | 男 | 中 | 0 || 7 | 男 | 中 | 1 || 8 | 女 | 中 | 0 || 9 | 女 | 低 | 1 || 10 | 女 | 中 | 0 || 11 | 女 | 高 | 0 || 12 | 男 | 低 | 1 || 13 | 女 | 低 | 1 || 14 | 男 | 高 | 0 || 15 | 男 | 高 | 0 | 按照分组统计，我们可以得到如下信息： | | 已流失(人) | 未流失(人) | 汇总(人) || :–: | :——–: | :——–: | :——: || 整体 | 5 | 10 | 15 || 男 | 3 | 5 | 8 || 女 | 2 | 5 | 7 || 高 | 0 | 6 | 6 || 中 | 1 | 4 | 5 || 低 | 4 | 0 | 4 | 那么可得到三个熵： 整体熵：$$H(S) = -\frac{5}{15}log_{2}(\frac{5}{15})-\frac{10}{15}log_{2}(\frac{10}{15}) = 0.9182$$性别熵：$$H(g_1) = -\frac{3}{8}log_{2}(\frac{3}{8}) - \frac{5}{8}log_{2}(\frac{5}{8}) = 0.9543$$ $$H(g_1) = -\frac{2}{7}log_{2}(\frac{2}{7}) - \frac{5}{7}log_{2}(\frac{5}{7}) = 0.8631$$ 性别信息增益：$$g(S,g) = H(S)-\frac{8}{15}H(g_1)-\frac{7}{15}H(g_2)=0.0064$$同理计算活跃度熵：$$H(a_1)=0$$ $$H(a_2)=0.7219$$ $$H(a_3)=0.0$$ 活跃度信息增益：$$g(S,a) = H(S)-\frac{6}{15}H(a_1)-\frac{5}{15}H(a_2)-\frac{4}{15}H(a_3)=0.6776活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。$$活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。 在做特征选择或者数据分析的时候，我们应该重点考察活跃度这个指标。 参考：https://blog.csdn.net/guomutian911/article/details/78599450 三、决策树的其它划分依据决策树的原理不止信息增益这一种，还有其他方法。 ID3 信息增益 最大的准则 C4.5 信息增益比 最大的准则 CART 分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的默认原则 优势：划分更加细致（从后面例子的树显示来理解） 四、决策树 APIclass sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None) 决策树分类器 criterion 默认是’gini’系数 也可以选择信息增益的熵’entropy’ max_depth 树的深度大小 不指定树的深度很容易出现过拟合 random_state 随机数种子 gini - 基尼系数 五、决策树 - 案例：鸢尾花分类案例 完整代码 1234567891011121314151617181920212223242526272829from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.tree import DecisionTreeClassifier# 获取数据集iris=load_iris()# 分割数据集x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 特征工程：标准化transfer=StandardScaler()x_train=transfer.fit_transform(x_train)x_test=transfer.transform(x_test)# 模型训练# 1&gt; 实例化一个估计器estimator=DecisionTreeClassifier(criterion='entropy',max_depth=3)# 2&gt; 传入训练数据集，进行机器学习estimator.fit(x_train,y_train)# 模型评估# 方法1，比较真实值与预测值y_predict=estimator.predict(x_test)print("预测值为:\n",y_predict)print("比较真实值与预测值结果为:\n",y_predict==y_test)# 方法2, 计算模型准确率print("模型准确率为:\n",estimator.score(x_test,y_test)) 六、决策树 - 案例：泰坦尼克号乘客生存预测 泰坦尼克号数据 在泰坦尼克号和titanic2数据帧描述泰坦尼克号上的个别乘客的生存状态。这里使用的数据集是由各种研究人员开始的。其中包括许多研究人员创建的旅客名单，由Michael A. Findlay编辑。我们提取的数据集中的特征是票的类别，存活，乘坐班，年龄，登陆，home.dest，房间，票，船和性别。 乘坐班是指乘客班（1，2，3），是社会经济阶层的代表。 其中age数据存在缺失。 数据：http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt 步骤分析 数据预处理，填充缺失值 提取特征值，目标值 特征工程，字典特征提取 数据集划分 模型训练 模型评估 完整代码123456789101112131415161718192021222324252627282930313233343536import pandas as pdfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier# 获取数据tanic=pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")tanic.head()# 数据预处理，填充缺失值tanic['age'].fillna(tanic['age'].mean(),inplace=True)# 提取特征值，目标值x=tanic[['pclass','age','sex']]y=tanic['survived']# 特征工程，字典特征提取transfer=DictVectorizer(sparse=False)x=transfer.fit_transform(x.to_dict(orient="records"))# 数据集划分x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)# 模型训练# 1&gt; 实例化一个转换器estimator=DecisionTreeClassifier()# 2&gt; 进行机器学习estimator.fit(x_train,y_train)# 模型评估# 方法1，比较真实值与预测值y_predict=estimator.predict(x_test)print("预测值为:\n",y_predict)print("比较真实值与预测值结果为:\n",y_predict==y_test)# 方法2,计算模型准确率print("模型准确率为:\n",estimator.score(x_test,y_test))]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（九）—sklearn之朴素贝叶斯算法]]></title>
    <url>%2Fposts%2F37565%2F</url>
    <content type="text"><![CDATA[一、朴素贝叶斯算法 什么是朴素贝叶斯分类方法属于哪个类别概率大，就判断属于哪个类别 概率基础 概率定义为一件事情发生的可能性 P(X) : 取值在[0, 1] 联合概率、条件概率与相互独立 联合概率：包含多个条件，且所有条件同时成立的概率 记作：P(A,B) 条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率 记作：P(A|B) 相互独立：如果P(A, B) = P(A)P(B)，则称事件A与事件B相互独立。 二、朴素贝叶斯公式$$P(C|W) = \frac{P(W|C)P(C)}{P(W)}$$ 朴素贝叶斯：特征之间是相互独立的。 因此当 W 不存在时，可使用相互独立将 W 进行拆分 三、简单案例： 计算下面概率 P(喜欢|产品, 超重) = ？ 套用公式可得$$P(喜欢|产品, 超重) = \frac{P(产品, 超重|喜欢)P(喜欢)}{P(产品, 超重)}$$ 分析 上式中，P(产品, 超重|喜欢) 和 P(产品, 超重) 的结果均为0，导致无法计算结果。 这是因为我们的样本量太少了，不具有代表性，本来现实生活中，肯定是存在职业是产品经理并且体重超重的人的，P(产品, 超重)不可能为0；而且事件“职业是产品经理”和事件“体重超重”通常被认为是相互独立的事件， 但是，根据我们有限的7个样本计算“P(产品, 超重) = P(产品)P(超重)”不成立。 而朴素贝叶斯可以帮助我们解决这个问题。 朴素贝叶斯，简单理解，就是假定了特征与特征之间相互独立的贝叶斯公式。 也就是说，朴素贝叶斯，之所以朴素，就在于假定了特征与特征相互独立。 所以，如果按照朴素贝叶斯的思路来解决，就可以是： $$P(产品, 超重) = P(产品) P(超重) = \frac{2}{7} \frac{3}{7} = \frac{6}{49}$$ $$P(产品, 超重|喜欢) = P(产品|喜欢) P(超重|喜欢) = \frac{1}{2} \frac{1}{4} = \frac{1}{8}$$ $$P(喜欢|产品, 超重) = \frac{P(产品, 超重|喜欢)P(喜欢)}{P(产品, 超重)} = \frac{\frac{1}{8} * \frac{4}{7}}{\frac{6}{49}} = \frac{7}{12}$$ 四、多个特征的朴素贝叶斯1. 公式$$P(C|F_1,F_2,…) = \frac{P(F_1,F_2,…|C)P(C)}{P(F_1,F_2,…)}$$ C 可以是不同类别 2. 公式分为三个部分： P(C)：每个文档类别的概率(某文档类别数／总文档数量) P(W│C)：给定类别下特征（被预测文档中出现的词）的概率 计算方法：P(F1│C)=Ni/N （训练文档中去计算） Ni​ 为该F1词在C类别所有文档中出现的次数 N为所属类别C下的文档所有词出现的次数和 P(F1,F2,…) 预测文档中每个词的概率 如果计算两个类别概率比较： 所以我们只要比较前面的大小就可以，得出谁的概率大 四、案例-朴素贝叶斯-文章分类 数据 分别计算属于两个类的概率 $P(C|Chinese, Chinese, Chinese, Tokyo, Japan)$ $P(not C|Chinese, Chinese, Chinese, Tokyo, Japan)$ 计算过程12345678P(C|Chinese, Chinese, Chinese, Tokyo, Japan)= P(Chinese, Chinese, Chinese, Tokyo, Japan|C) * P(C)= P(Chinese|C)^3 * P(Tokyo|C) * P(Japan|C) * P(C)= 5/8 * 0 * 0P(notC|Chinese, Chinese, Chinese, Tokyo, Japan)= P(Chinese, Chinese, Chinese, Tokyo, Japan|notC) * P(notC)= P(Chinese|notC)^3 * P(Tokyo|notC) * P(Japan|notC) * P(notC)= 1/9 * 1/3 * 1/3 但是我们发现 P(Tokyo|C) 和 P(Japan|C) 都为 0，这是不合理的，如果词频列表里面有很多出现次数都为 0，很可能计算结果都为 0。 我们能够使用拉普拉斯平滑系数解决此问题。 拉普拉斯平滑系数$$P(F1|C) = \frac{Ni+\alpha}{N+\alpha m}$$ Ni​ 为该F1词在C类别所有文档中出现的次数。 N为所属类别C下的文档所有词出现的次数和。 $\alpha$ 为指定的系数，一般为1。 m 为训练集中有多少个特征词种类，如在此案例中 m = 6 。 五、sklearn 朴素贝叶斯 APIsklearn.naive_bayes.MultinomialNB(alpha = 1.0) 朴素贝叶斯分类 alpha 拉普拉斯平滑系数 六、案例-朴素贝叶斯-20类新闻分类 数据集介绍 步骤分析 进行数据集的分割 TFIDF进行的特征抽取 将文章字符串进行单词抽取 朴素贝叶斯预测 完整代码 这里的转换器不能对测试集进行 fit 操作，因为 tfidf 转换器对于不同的文章来说提取的特征值不同，提取的特征不同，训练出的模型就不同 1234567891011121314151617181920212223242526272829303132from sklearn.datasets import fetch_20newsgroupsfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNB# 获取数据news = fetch_20newsgroups()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.3)# 特征抽取 Tfidf# 实例化一个转换器transfer = TfidfVectorizer()x_train = transfer.fit_transform(x_train)# 必须使用transfrom因为要让测试数据和训练数据的特征值是一样的。x_test = transfer.transform(x_test)# 模型训练# 实例化一个估计器estimator = MultinomialNB()estimator.fit(x_train, y_train)# 模型评估# 方法一：比较真实值与预测值y_predict = estimator.predict(x_test)print('预测值为:\n', y_predict)print('比较真实值与预测值结果为:\n', y_predict==y_test)# 方法二：计算模型准确率print('模型准确率为:\n', estimator.score(x_test, y_test)) 七、朴素贝叶斯优缺点 优点： 思想简单、直观 有稳定的分类效率，准确度高，速度快 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 缺点： 由于假设了特征之间相互独立性，但是往往特征之间会有所关联，所以如果特征之间有相关性，效果不会太好。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（八）—sklearn之交叉验证与参数调优]]></title>
    <url>%2Fposts%2F6398%2F</url>
    <content type="text"><![CDATA[一、交叉验证与参数调优 交叉验证(cross validation) 交叉验证：将拿到的训练数据，分为训练集、验证集和测试集。 训练集：训练集+验证集 测试集：测试集 为什么需要交叉验证 为了让被评估的模型更加稳健 参数调优 超参数搜索-网格搜索(Grid Search) 通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。 区分交叉验证和参数调优 交叉验证 使模型更稳健 参数调优 使模型准确性更高 二、模型选择、参数调优和交叉验证集成 APIsklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None) 介绍 对估计器的指定参数值进行详尽搜索 参数介绍 estimator 估计器对象 param_grid 估计器参数(dict){“n_neighbors”:[1,3,5]} cv 指定几折交叉验证 return estimator 新的估计器对象 使用新的估计器对象方法不变 fit：输入训练数据 score：准确率 新估计器对象的属性 bestscore:在交叉验证中验证的最好结果_ bestestimator：最好的参数模型 cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果 三、交叉验证与参数调优-案例：鸢尾花案例增加K值调优 完整代码1234567891011121314151617181920212223242526272829303132333435from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_split,GridSearchCVfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier# 加载数据iris = load_iris()# 划分数据集x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 指定算法及模型选择与调优——网格搜索和交叉验证estimator = KNeighborsClassifier()param_dict = &#123;"n_neighbors": [1, 3, 5]&#125;estimator = GridSearchCV(estimator, param_grid=param_dict, cv=3)# 训练模型estimator.fit(x_train,y_train)# 模型评估# 方法一 比对真实值与预测值y_predict = estimator.predict(x_test)y_test == y_predict# 方法二 计算准确率estimator.score(x_test,y_test)# 然后进行评估查看最终选择的结果和交叉验证的结果print("在交叉验证中验证的最好结果：\n", estimator.best_score_)print("最好的参数模型：\n", estimator.best_estimator_)print("每次交叉验证后的准确率结果：\n", estimator.cv_results_) 四、交叉验证与参数调优-案例：预测facebook签到位置 目标 将根据用户的位置，准确性和时间戳预测用户正在查看的业务。 数据集介绍 两个文件 train.csv test.csv 文件字段 row_id：登记事件的ID xy：坐标 accuracy：定位准确性 time：时间戳 place_id：业务的ID，这是您预测的目标 官网：https://www.kaggle.com/navoshta/grid-knn/data 步骤分析 数据预处理 缩小数据集范围 时间特征提取 将签到数少于n的位置删除 数据集划分 特征工程 标准化 KNN算法 GSCV优化 模型评估 完整代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import pandas as pdfrom sklearn.model_selection import GridSearchCV,train_test_splitfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.preprocessing import StandardScaler#读取数据facebook=pd.read_csv("./data/FBlocation/train.csv")facebook.head()# 数据预处理# 1&gt; 缩小数据集范围facebook = facebook.query("x&lt;1.5&amp;x&gt;1.25&amp;y&gt;2.25&amp;y&lt;2.5")# 2&gt; 时间特征提取time_value = pd.to_datetime(facebook['time'],unit='s')time_value = pd.DatetimeIndex(time_value)facebook['day'] = time_value.dayfacebook['hour'] = time_value.hourfacebook['weekday'] = time_value.weekday# 3&gt; 删除签到数少于n的位置place_count = facebook.groupby(['place_id']).count()place_count = place_count.query('row_id&gt;3')facebook = facebook[facebook['place_id'].isin(place_count.index)]# 数据集划分# 1&gt; 拿取有用的特征数据x=facebook[['x','y','accuracy','day','hour','weekday']]# 2&gt; 拿取目标值数据y=facebook['place_id']# 3&gt; 数据集划分x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=8)# 特征工程：标准化# 1&gt; 创建转换器transfer = StandardScaler()# 2&gt; 计算并标准化训练集数据x_train = transfer.fit_transform(x_train)# 3&gt; 计算并标准化测试集数据x_test = transfer.transform(x_test)# 模型训练及参数优化# 1&gt; 实例化一个K-近邻估计器estimator = KNeighborsClassifier()# 2&gt; 运用网络搜索参数优化KNN算法param_dict = &#123;"n_neighbors":[3,5,7,9]&#125; # K-近邻中分别选取这几个 K 值，最终经过交叉验证会返回各个取值的结果和最好的结果estimator = GridSearchCV(estimator,param_grid=param_dict,cv=5) # 返回优化后的估计器# 3&gt; 传入训练集，进行机器学习estimator.fit(x_train,y_train)# 模型评估# 方法一：比较真实值与预测值y_predict=estimator.predict(x_test)print("预测值为:\n",y_predict)print("比较真实值与预测值结果为:\n",y_predict==y_test)# 方法二：计算模型准确率print("模型准确率为:\n",estimator.score(x_test,y_test))print("在交叉验证中最的结果:\n",estimator.best_score_)print("最好的参数模型:\n",estimator.best_estimator_)print("每次交叉验证后的结果准确率为/n",estimator.cv_results_)]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（七）—sklearn之K-近邻算法]]></title>
    <url>%2Fposts%2F3290%2F</url>
    <content type="text"><![CDATA[一、K-近邻算法(KNN)原理K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法， 总体来说KNN算法是相对比较容易理解的算法 定义如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 来源：KNN算法最早是由Cover和Hart提出的一种分类算法 距离公式两个样本的距离可以通过如下公式计算，又叫欧式距离$$distance = \sqrt{\sum_i^n (a_i-b_i)^2}$$ $A = (a_1,a_2,a_3,…..,a_n)$ $B = (b_1,b_2,b_3,…..,b_n)$ 二、简单实例-电影类型分析假设我们现在有几部电影 其中 $ ？$表示的电影不知道类别，如何去预测？我们可以利用K近邻算法的思想 问题 如果取的最近的电影数量不一样？会是什么结果？ k = 1 ，[爱情片] k = 2 ，[爱情片，爱情片] k = 3 ，[爱情片，爱情片，爱情片] k = 4 ，[爱情片，爱情片，爱情片，动作片] k = 6 ，[爱情片，爱情片，爱情片，动作片，动作片，动作片] 分析K-近邻算法需要做什么样的处理 k 是一个超参数，需要人为指定，好的 k 值更需要人的丰富经验 当 k 取很大值时，受样本均衡影响较大 当 k 取很小值时，受异常点影响较大 三、sklearn - KNN - APIsklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’) n_neighbors 查询默认使用的邻居数 int ,可选（默认= 5） algorithm： 可选用于计算最近邻居的算法：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’} ‘ball_tree’将会使用 BallTree， ‘kd_tree’将使用 KDTree。 ‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率) 四、KNN - 案例：鸢尾花种类预测 数据集介绍Iris数据集是常用的分类实验数据集，由Fisher, 1936收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。关于数据集的具体介绍： 步骤分析 获取数据集与分割数据集 特征工程：标准化 模型训练评估 完整代码12345678910111213141516171819202122232425262728from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier# 加载数据iris = load_iris()# 划分数据集x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.3,random_state=8)# 标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 训练模型estimator = KNeighborsClassifier()estimator.fit(x_train,y_train)# 模型评估# 方法一 比对真实值与预测值y_predict = estimator.predict(x_test)y_test == y_predict# 模型评估# 方法二 计算准确率estimator.score(x_test,y_test) 五、KNN优缺点 优点： 简单，易于理解，易于实现，无需训练 缺点： 懒惰算法，对测试样本分类时的计算量大，内存开销大 必须指定K值，K值选择不当则分类精度不能保证 使用场景：小数据场景，几千～几万样本，具体场景具体业务具体分析]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（六）—sklearn之转换器和估计器]]></title>
    <url>%2Fposts%2F31237%2F</url>
    <content type="text"><![CDATA[一、sklearn转换器 想一下之前做的特征工程的步骤？ 1 实例化 (实例化的是一个转换器类(Transformer)) 2 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用) 我们把特征工程的接口称之为转换器，其中转换器调用有这么几种形式 fit_transform fit transform 这几个方法之间的区别是什么呢？我们看以下代码就清楚了 示例代码1234567891011121314from sklearn.preprocessing import StandardScaler# 创建一个标准差转换器transfer = StandardScaler()a = [[1,2,3],[4,5,6]]# 进行计算均值和标准差，并进行转换，计算均值和标准差的结果会保存在transfer对象中，之后用到均值或标准差都会从对象中直接提取，如果重新计算会重新保存。transfer.fit_transform(a)# 进行均值和标准差的计算，保存在transfer对象中，transfer.fit(a)# 进行转换transfer.transform(a) 二、sklearn估计器在sklearn中，估计器(estimator)是机器学习算法的API，是进行机器学习的面向对象，它的内部能够像转换器那样自动地保存一些运算结果。 列举一些估计器 1 用于分类的估计器： sklearn.neighbors k-近邻算法 sklearn.naive_bayes 贝叶斯 sklearn.linear_model.LogisticRegression 逻辑回归 sklearn.tree 决策树与随机森林 2 用于回归的估计器： sklearn.linear_model.LinearRegression 线性回归 sklearn.linear_model.Ridge 岭回归 3 用于无监督学习的估计器 sklearn.cluster.KMeans 聚类 估计器工作流程 实例化一个估计器 1estimator = LNeighborsClassifier() 传入训练数据集，进行机器训练 1estimator.fit(x_train,y_train) 模型评估 方法1. 比较真实值与预测值 12y_predict = estimator.predict(x_test)y_predict == y_test 方法2. 计算模型准确率 1estimator.score(x_test,y_test)]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（五）—sklearn特征降维]]></title>
    <url>%2Fposts%2F11337%2F</url>
    <content type="text"><![CDATA[一、特征降维概述 为什么要对特征进行降维处理 如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大 什么是降维 降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程 降维的作用 减少特征数量 减少特征相关性，去除相关性强的特征，比如 相对湿度与降雨量 降维的两种方式 特征选择 主成分分析（PCA） 二、什么是特征选择 定义旨在从原有特征中找出主要特征，去除冗余或无关特征。 方法 Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联。 方差选择法：低方差特征过滤 相关系数 Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联） 决策树:信息熵、信息增益 正则化：L1、L2 深度学习：卷积 Embedded方式，在讲解算法时再进行介绍 模块1sklearn.feature_selection 三、降维 - 特征选择 - 过滤式 - 方差选择法 低方差特征过滤，删除低方差的一些特征， 特征方差小：在多个样本中某个特征的值会比较相近 特征方差大：在多个样本中某个特征的值是有些许差别的 APIsklearn.feature_selection.VarianceThreshold(threshold = 0.0) 删除所有低方差特征 Variance.fit_transform(X) X:numpy array格式的数据 返回值：训练集方差 低于 threshold 的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。 示例代码123456789101112131415import pandas as pdfrom sklearn.feature_selection import VarianceThresholddata = pd.read_csv('factor_returns.csv')print(data[data.columns[1:-2]].shape)# 1、实例化一个转换器类transfer = VarianceThreshold(threshold=1)# 2、调用fit_transformnew_data = transfer.fit_transform(data[data.columns[1:-2]])# 3、删除低方差特征的结果print(new_data.shape) 四、降维 - 特征选择 - 过滤式 - 相关系数 皮尔逊相关系数(Pearson Correlation Coefficient) 反映特征之间相关关系密切程度的统计指标 公式(了解)$$r = \frac{n\sum{xy} - \sum{x}\sum{y}}{\sqrt{n\sum{x^2}-(\sum{x})^2}\sqrt{n\sum{y^2}-(\sum{y})^2}}$$ 上面是协方差，下面是各自的标准差 特点 相关系数的值介于 –1 与 +1 之间，即 $–1≤ r ≤+1$ 。 当 $r&gt;0$ 时，表示两变量正相关，$r&lt;0$ 时，两变量为负相关 当 0&lt;|r|&lt;1 时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近于0，表示两变量的线性相关越弱 当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系。 一般可按三级划分：|r|&lt;0.4为低度相关；0.4≤|r|&lt;0.7为显著性相关；0.7≤|r|&lt;1为高度线性相关。 APIfrom scipy.stats import pearsonr 示例代码123456789101112import pandas as pdfrom scipy.stats import pearsonrdata = pd.read_csv('./data/factor_returns.csv')factor = ['pe_ratio', 'pb_ratio', 'market_cap', 'return_on_asset_net_profit', 'du_return_on_equity', 'ev', 'earnings_per_share', 'revenue', 'total_expense']datas = [(factor[i], factor[j + 1], pearsonr(data[factor[i]], data[factor[j + 1]])[0]) for i in range(len(factor)) for j in range(i, len(factor) - 1)]for data in datas: print("指标 &#123;&#125; 与指标 &#123;&#125; 之间的相关性大小为 &#123;&#125; ".format(*data)) 五、降维 - 主成分分析（PCA） 什么是主成分分析(PCA) 定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量 作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。 应用：回归分析或者聚类分析当中 在决策树中’信息’一词会有清晰理解 APIsklearn.decomposition.PCA(n_components=None) 将数据分解为较低维数空间 n_components: 小数：保留百分之多少的信息 整数：减少到具体的多少个特征 PCA.fit_transform(X) X：numpy array 格式的数据 return：转换为指定维度后的 array 示例代码123456789101112131415from sklearn.decomposition import PCAdata = [[2,8,4,5], [6,3,0,8], [5,4,9,1]]# 1、实例化PCA, 小数—保留百分之多少信息transfer = PCA(n_components=0.9)# 2、调用fit_transformdata1 = transfer.fit_transform(data)print("保留90%的信息，降维结果为：\n", data1)# 1、实例化PCA, 整数——指定降维到的维数transfer2 = PCA(n_components=3)# 2、调用fit_transformdata2 = transfer2.fit_transform(data)print("降维到3维的结果：\n", data2) 六、降维 - 案例 目的探究用户对物品类别的喜好细分降维 现有数据 order_products__prior.csv：订单与商品信息 字段：order_id, product_id, add_to_cart_order, reordered products.csv：商品信息 字段：product_id, product_name, aisle_id, department_id orders.csv：用户的订单信息 字段：order_id,user_id,eval_set,order_number,…. aisles.csv：商品所属具体物品类别 字段： aisle_id, aisle 分析 合并表，使得user_id与aisle在一张表当中 进行交叉表变换 进行降维 完整代码12345678910111213141516171819202122232425262728import pandas as pdfrom sklearn.decomposition import PCA# 1、获取数据集 products = pd.read_csv("./data/instacart/products.csv") # 商品信息order_products = pd.read_csv("./data/instacart/order_products__prior.csv") # 订单与商品信息orders = pd.read_csv("./data/instacart/orders.csv") # 用户的订单信息aisles = pd.read_csv("./data/instacart/aisles.csv") # 商品所属具体物品类别# 2、合并表，将user_id和aisle放在一张表上# 1）合并 orders 和 order_products tab1 = pd.merge(aisles, products, on="aisle_id")# 2）合并 tab1 和 productstab2 = pd.merge(tab1, order_products, on="product_id")# 3）合并 tab2 和 aisles tab3 = pd.merge(tab2, orders, on="order_id")# 3、交叉表处理，把 user_id 和 aisle 进行分组 table = pd.crosstab(tab3["user_id"], tab3["aisle"])# 4、主成分分析的方法进行降维# 1）实例化一个转换器类PCAtransfer = PCA(n_components=0.95)# 2）fit_transformdata = transfer.fit_transform(table)# 查看降维结果data.shape]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（四）—sklearn特征预处理]]></title>
    <url>%2Fposts%2F40760%2F</url>
    <content type="text"><![CDATA[一、特征预处理概述 什么是特征预处理12# scikit-learn的解释provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. 翻译过来：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程 数值型数据的无量纲化： 归一化 标准化 为什么我们要进行归一化/标准化？百面机器学习-为什么需要对数值类型的特征做归一化 如果存在特征的数值差别比较大的特征，那么分析出来的结果显然就会倾向于数值差别比较大的特征。 如果存在特征的方差比较大的特征，那么分析出来的结果显然就会倾向于方差比较大的特征。 我们需要用到一些方法进行无量纲化，使不同规格的数据转换到同一规格。 二、归一化 定义通过对原始数据进行变换把数据映射到 [0,1] 之间 公式$$x^{‘} = \frac{x^{old}-min}{max-min}$$ $$x^{new} = x^{‘} * (mx - mi) + mi$$ max、min 分别为该特征数据中的最大值、最小值 mx、mi 分别为设置的归一化区间的最大值、最小值 sklearn API：sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)) MinMaxScalar.fit_transform(X) X：numpy array 格式的数据 return：转换后的形状相同的array 示例代码： 12345678910import pandas as pdfrom sklearn.preprocessing import MinMaxScalerdata = pd.read_csv('dating.txt')# 1、创建传唤器，默认 feature_range=(0,1)transfer = MinMaxScaler(feature_range=(2,3))# 2、调用fit_transformtransfer.fit_transform(data[['milage','Liters','Consumtime','target']]) 归一化缺点： 最大值最小值是变化的，归一化容易受极值影响，稳健性较差。 最大值与最小值容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。 三、标准化 定义通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内 公式$$x^{new} = \frac{x - mean}{σ}$$ 作用于每一列，mean为平均值，σ为标准差 sklearn API：sklearn.preprocessing.StandardScaler( ) 处理之后每列来说所有数据都聚集在均值0附近标准差差为1 StandardScaler.fit_transform(X) X:numpy array格式的数据 return：转换后的形状相同的array 示例代码： 12345678910111213import pandas as pdfrom sklearn.preprocessing import StandardScalerdata = pd.read_csv('dating.txt')# 1、创建转换器transfer = StandardScaler()# 2、调用fit_transformnew_data = transfer.fit_transform(data[data.columns[:3]])# 3、打印标准化后的结果print(new_data) 归一化的缺点在标准化下不存在 对于标准化来说，如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。 在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（三）—sklearn特征工程]]></title>
    <url>%2Fposts%2F25433%2F</url>
    <content type="text"><![CDATA[一、特征工程介绍1. 为什么需要特征工程Andrew Ng ： “Coming up with features is difficult, time-consuming, requires expert knowledge. “Applied machine learning” is basically feature engineering. ” 注：业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 2. 什么是特征工程特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。 意义：会直接影响机器学习的效果 3. 特征工程包含的内容 特征抽取 特征预处理 特征降维 4. 特征工程常用的模块 pandas：数据预处理，一个数据读取非常方便以及基本的处理格式的工具 缺失值处理 数据类型转换 数据清洗 sklearn：特征工程，对于特征的处理提供了强大的接口 特征提取 特征预处理 特征降维 二、特征提取1. 什么是特征提取 将文本/字典/图像转换为数值 设置哑变量，将类别型特征转换为 $0/1$ 格式 2. 特征提取 API sklearn.feature_extraction 三、字典特征提取 作用： 对字典数据进行特征值化 API：sklearn.feature_extraction.DictVectorizer(sparse=True) DictVectorizer.fit_transform(X) X：字典或者包含字典的迭代器 return：返回sparse矩阵 DictVectorizer.inverse_transform(X) X：array数组或者sparse矩阵 return：转换之前数据格式 DictVectorizer.get_feature_names() 返回类别名称 示例代码： 12345678910from sklearn.feature_extraction import DictVectorizerdata = [&#123;'city': '北京','temperature':100&#125;, &#123;'city': '上海','temperature':60&#125;, &#123;'city': '深圳','temperature':30&#125;]# 实例化一个转换器类,sparse：True返回sparse矩阵，False 返回真实矩阵transfer = DictVectorizer(sparse=True)# 调用fit_transformprint(transfer.fit_transform(data))# 打印特征名字print(transfer.get_feature_names()) 四、文本特征提取 作用： 对文本数据进行特征值化 sklearn.feature_extraction.text.CountVectorizer(stop_words=[]) 返回词频矩阵 CountVectorizer.fit_transform(X) X：文本或者包含文本字符串的可迭代对象 return：返回sparse矩阵 CountVectorizer.inverse_transform(X) X：array数组或者sparse矩阵 return：转换之前数据格 CountVectorizer.get_feature_names() return：单词列表 sklearn.feature_extraction.text.TfidfVectorizer tfidf 文本特征提取 示例代码： 123456789101112from sklearn.feature_extraction.text import CountVectorizerdata = ["life is short,i like like python", "life is too long,i dislike python"]# 实例化一个转换器类,stop_words-&gt;过滤单词 如：'is'¶transfer = CountVectorizer(stop_words=['is'])# 返回 sparse 矩阵，节省内存print(transfer.fit_transform(data))# 返回 真实矩阵，利用toarray()进行sparse矩阵转换array数组print(transfer.fit_transform(data).toarray())# 返回特征名字print("返回特征名字：\n", transfer.get_feature_names()) 五、中文文本特征提取1. 需要安装下jieba库1pip3 install jieba 2. jieba.cut() 返回词语组成的生成器 示例代码： 12text = '我爱北京天安门'print(' '.join(list(jieba.cut(text)))) 3. 实例：中文文本特征提取 示例代码： 123456789101112131415161718texts = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。", "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。", "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]# 创建分词匿名函数cut_word = lambda text:' '.join(list(jieba.cut(text)))# 对原始数据进行jieba分词new_texts = [cut_word(text) for text in texts]# 创建分类器transfer = CountVectorizer()# 返回频次数组print(transfer.fit_transform(new_texts).toarray())# 返回特征词print(transfer.get_feature_names()) 六、Tf-idf文本特征提取 TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 公式： $$ tfidf_{ij} = tf_{ij} × idf_{ij}$$ 词频（term frequency，tf） 指的是某一个给定的词语在该文件中出现的频率 逆向文档频率（inverse document frequency，idf） 一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到 词频（term frequency，tf） 指的是某一个给定的词语在该文件中出现的频率 逆向文档频率（inverse document frequency，idf） 一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到 示例： 假如一篇文件的总词语数是100个，而词语”非常”出现了5次。 那么”非常”一词在该文件中的词频就是5/100=0.05。 如果”非常”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,0000）=3。 最后”非常”对于这篇文档的tf-idf的分数为0.05 * 3=0.15 示例代码：123456789101112131415161718texts = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。", "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。", "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]# 创建分词匿名函数cut_word = lambda text:' '.join(list(jieba.cut(text)))# 对原始数据进行jieba分词new_texts = [cut_word(text) for text in texts]# 创建分类器transfer = TfidfVectorizer(stop_words=['一种', '不会', '不要'])# 返回频次数组print(transfer.fit_transform(new_texts).toarray())# 返回特征词print(transfer.get_feature_names())]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（二）—sklearn数据集]]></title>
    <url>%2Fposts%2F33200%2F</url>
    <content type="text"><![CDATA[一、可用数据集 Kaggle网址：https://www.kaggle.com/datasets UCI数据集网址： http://archive.ics.uci.edu/ml/ scikit-learn网址：http://scikit-learn.org/stable/datasets/index.html 各数据集优点 sk 数据量小，方便学习 uci 数据真实，全面 ka 竞赛平台，数据集真实 二、Scikit-learn1. 介绍 Python语言的机器学习工具 Scikit-learn包括许多知名的机器学习算法的实现 Scikit-learn文档完善，容易上手，丰富的API 目前稳定版本0.19.1 2. 安装通过 pip 安装 1pip3 install Scikit-learn==0.19.1 安装好之后可以通过以下命令查看是否安装成功 1import sklearn 注：安装scikit-learn需要Numpy, Scipy等库 3. Scikit-learn 主要的API 分类、聚类、回归 特征工程 模型选择、调优 三、SKlearn 数据集1. 数据集介绍 sklearn.datasets load_*() 获取小规模数据集，数据包含在datasets里 fetch_*(data_home=None) 获取大规模数据集，需要从网络上下载。 函数的第一个参数是data_home，表示数据集下载的目录,默认目录是根目录下的 scikit_learn_data文件夹： ~/scikit_learn_data/ 2. sklearn小数据集 示例： sklearn.datasets.load_iris() 加载并返回鸢尾花数据集 sklearn.datasets.load_boston() 加载并返回波士顿房价数据集 3. sklearn大数据集 示例： sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’) subset：’train’或者’test’，’all’，可选，选择要加载的数据集。 训练集的“训练”，测试集的“测试”，两者的“全部” 4. sklearn数据集返回值介绍 load 和 fetch 返回的数据类型 datasets.base.Bunch (字典格式) data：特征数据数组（特征值输入） target：标签数组（目标输出） feature_names：特征名称 target_names：标签名称 DESCR：数据描述 Bunch 虽然是字典格式，但可以通过 ‘点’ 的形式把属性点出来 示例代码： 12345678910111213141516from sklearn.datasets import load_iris# 获取鸢尾花数据集iris = load_iris()print("鸢尾花数据集的返回值：\n", iris)print("鸢尾花的特征值:\n", iris["data"])print("鸢尾花的目标值：\n", iris.target)print("鸢尾花特征的名字：\n", iris.feature_names)print("鸢尾花目标值的名字：\n", iris.target_names)print("鸢尾花的描述：\n", iris.DESCR) 四、数据集划分 机器学习一般的数据集会划分为两个部分： 训练数据：用于训练、构建模型 测试数据：在模型检验时使用，用于 评估模型是否有效 划分比例： 训练集：70~80% 测试集：20~30% 数据集划分 api sklearn.model_selection.train_test_split( x, y, test_size, random_state ) x 数据集的特征值 y 数据集的标签值 test_size 测试集的大小，一般为float，默认 25% random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。 return 测试集特征 训练集特征 训练标签 测试标签 (默认随机取) 示例代码： 123456789101112131415161718192021from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_split# 获取鸢尾花数据集iris = load_iris()# 默认测试集占比 25%# 第一次划分，随机种子 22x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)# 第二次划分，随机种子 6x_train1, x_test1, y_train1, y_test1 = train_test_split(iris.data, iris.target, random_state=6)# 第三次划分，随机种子 6x_train2, x_test2, y_train2, y_test2 = train_test_split(iris.data, iris.target, random_state=6)# 比较第一次和第二次划分，当随机种子设置不同时，划分结果不同print(x_train == x_train1)# 比较第二次和第三次划分，当随机种子设置相时，划分结果相同print(x_train1 == x_train2)]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实践（一）—概述]]></title>
    <url>%2Fposts%2F55376%2F</url>
    <content type="text"><![CDATA[1956年，人工智能元年。 人类能够创造出人类还未知的东西。 这未知的东西人类能够保证它不误入歧途吗。 一、机器学习和人工智能，深度学习的关系 机器学习是人工智能的一个实现途径 深度学习是机器学习的一个方法发展而来 二、机器学习，深度学习的应用场景 挖掘、预测领域： 应用场景：店铺销量预测、量化投资、广告推荐、企业客户分类、SQL语句安全检测分类… 图像领域： 应用场景：街道交通标志检测、人脸识别等等 自然语言处理领域： 应用场景：语音识别，文本分类、情感分析、自动聊天、文本检测等等 三、什么是机器学习定义 机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测。 解释 我们可以从大量的日常经验中归纳规律，当面临新的问题的时候，就可以利用以往总结的规律去分析现实状况，采取最佳策略。 例子 从数据（大量的猫和狗的图片）中自动分析获得模型（辨别猫和狗的规律），从而使机器拥有识别猫和狗的能力。 从数据（房屋的各种信息）中自动分析获得模型（判断房屋价格的规律），从而使机器拥有预测房屋价格的能力。 四、数据集构成 结构：特征值+目标值 注： 对于每一行数据我们可以称之为样本。 有些数据集可以没有目标值： 五、机器学习算法分类算法分类 监督学习(supervised learning) 定义：输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值(回归），或是输出是有限个离散值（分类）。 回归：线性回归、岭回归 分类：k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归、神经网络 无监督学习(unsupervised learning) 定义：输入数据是由输入特征值所组成。 聚类：k-means 示例： 特征值：猫/狗的图片；目标值：猫/狗-类别 分类问题 特征值：房屋的各个属性信息；目标值：房屋价格-连续型数据 回归问题 特征值：人物的各个属性信息；目标值：无 无监督学习 六、机器学习开发流程 获取数据 sql、mysql 数据预处理 缺失值处理，数据类型转换，数据清洗 特征工程 特征提取，特征预处理，特征降维 机器学习 训练模型 模型评估 准确率，召回率，auc，ks，业务指标 如不合格，返回（4 实施落地 开发产品，api 七、学习框架和资料介绍 算法是核心，数据与计算是基础 算法工程师 线代、高数、概率统计 李航&lt;统计学习方法&gt; 周志华&lt;机器学习&gt; PRML 算法落地工程师 大部分复杂模型的算法设计都是算法工程师在做，而应用者 分析很多的数据 分析具体的业务 应用常见的算法 特征工程、调参数、优化 学会分析问题，使用机器学习算法的目的，想要算法完成何种任务 掌握算法基本思想，学会对问题用相应的算法解决 学会利用库或者框架解决问题 框架 SKlearn tensorflow pytorch]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vscode-插件]]></title>
    <url>%2Fposts%2F57651%2F</url>
    <content type="text"><![CDATA[常用插件 Auto Close TagAuto Rename TagBeautifyBracket Pair ColorizerColor HighlightColor PickerHtml Css SupportHtml SnippetsJavaScript(ES6)code snippetsjQuery Code SnippetsMaterial Icon Themenpmopen in browserOutput ColorizerPreview on Web ServerVeturView In Browser]]></content>
      <categories>
        <category>Vscode</category>
      </categories>
      <tags>
        <tag>Vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[装机必备（官网链接）]]></title>
    <url>%2Fposts%2F48776%2F</url>
    <content type="text"><![CDATA[装机必备 编程 Anaconda https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ Git https://git-scm.com/downloads node https://nodejs.org/zh-cn/ Postman https://www.getpostman.com/apps pycharm http://www.jetbrains.com/pycharm/download/ python-2 and python-3 https://www.python.org/ VSCode https://code.visualstudio.com/Download VSCode常用插件：http://xingtu.info/posts/57651/ 解压 BANDIZIP http://www.bandisoft.com/bandizip/ 浏览器 Chrome https://www.google.cn/chrome/ Twinkstar https://www.twinkstar.com/ 本地文件搜索 Everything http://www.voidtools.com/ 播放器 PotPlayer64 http://potplayer.daum.net/ 即时通讯 tim http://office.qq.com/download.html WeChat https://weixin.qq.com/cgi-bin/readtemplate?t=win_weixin 文本编辑器 typora https://www.typora.io/ WPS http://www.wps.cn/ 词典 YoudaoDict http://cidian.youdao.com/ 必应 https://cn.bing.com/dict/ 防火墙/杀毒/告别360/告别腾讯管家/简洁/强大 火绒 https://www.huorong.cn/ 工具截图 Snipaste https://zh.snipaste.com/download.html 录屏 Faststone Capture (腾讯软件中心下载链接) https://pc.qq.com/detail/0/detail_720.html 格式转换 FormatFactory（格式工厂） http://www.pcfreetime.com/formatfactory/CN/index.html 文件存储 BaiduNetdisk （百度云） https://pan.baidu.com/download 翻墙 Chromium （自带vpn的Chrome） https://github.com/chromium/chromium lantern https://github.com/getlantern/download 驱动 驱动精灵 http://www.drivergenius.com/ 鲁大师 http://www.ludashi.com/ 下载 fdm （类似迅雷/不限速） https://www.freedownloadmanager.org/download.htm JiJiDown （哔哩哔哩下载器） http://client.jijidown.com/ 软件整理 小Q书桌 (腾讯软件中心下载链接) https://pc.qq.com/detail/5/detail_6105.html 编程工具 sublime http://www.sublimetext.com/3 mongodb https://www.mongodb.com/download-center/community mysql https://dev.mysql.com/downloads/installer/ 思维导图 XMind https://www.xmind.net/download/ 幕布 https://mubu.com/apps 输入法 搜狗输入法 https://pinyin.sogou.com/ 系统包 ALI213-Microsoft.Visual.C++.2015.Redistributable.Package.x86.x64 （第三方链接不保证无毒） http://patch.ali213.net/showpatch/52575.html flashplayer http://soft.jxsgdsmb.cn/1/3889.html?tab=42844 Microsoft Visual C++ Build Tools （全文搜索：Visual C++ Build Tools 2015） https://blogs.msdn.microsoft.com/pythonengineering/2016/04/11/unable-to-find-vcvarsall-bat/ vcredist_x64 （CSDN下载） https://download.csdn.net/download/similing/9873980 游戏 Steam https://store.steampowered.com/about/ wegame https://www.wegame.com/]]></content>
      <categories>
        <category>经验</category>
      </categories>
      <tags>
        <tag>经验</tag>
        <tag>装机必备</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[准备考研了]]></title>
    <url>%2Fposts%2F40550%2F</url>
    <content type="text"><![CDATA[从新疆归来，也是大三的末期，没回来前就已经想好要考一考，不是为考上，而是为了学习，学习那些我从未接触过的知识，是的，我决定系统的学习那些科班才会学的知识，我怕自己没有根基，我怕再晚的话高楼崩塌。 数据结构、操作系统，计算机网络，计算机组成原理。 并没有畏惧，只有亢奋。 从前很羡慕计算机系的朋友，而现在我不再渴望。 我还在路上，还只是开始。 加油！]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[醒途-觉醒之路]]></title>
    <url>%2Fposts%2F44353%2F</url>
    <content type="text"><![CDATA[碎碎念 1.耳濡目染关注科班圈大牛圈,一是接触到了,就更能明白自己和大牛之间的差距,认清自己哪些地方欠缺,哪些地方不足,就更能够补上短板.该学什么,怎么学,都有一个清晰的认识.再就是看得多了,更能补全三观,近朱者赤近墨者黑,耳濡目染的多了,思想与认识自然就更开阔了.眼界与认知自然就更深了. 2.忌固化宜变通即使是固化的事情,也要主动思考是否可变通.不要把思维定住,不要认为教的都是不可以变的,不要既定思维,要活泛. 3.优秀与否不重要,学会自信优秀与不优秀,不要妄自菲薄,不要自我否定,人外有人,天外有天,你的人外是有别人,但可能你也属于别人的人外人,如果你觉得自己现在很差,那么意思就是你这么久的努力就这样轻而易举的被自己否定了.当你觉得自己不优秀时,你还怎么让别人觉得你优秀呢?从一知半解开始到现在,你做的已经很好了,相比于从前.所以说,请照一照镜子,看一看自己,你,很优秀,并且会愈来愈优秀.加油. 4.费曼技巧费曼技巧有四个简单的步骤： 选择一个概念 把它教给完全不懂的另外一个人 如果卡壳，回到原始材料 回顾后将语言条理化，简化。 检测知识最终的途径是你能有能力把它传播给另一个人。 那些声称清楚自己所想，但却不能清晰表达的人，其实通常不知道自己所想。— Mortimer Adler python奇淫技巧1.字典中value最大值对应的键123print(max(word_num, key=word_num.get))print(max(zip(word_num.values(),word_num.keys()))) 2.map1234567891011map()函数接收两个参数，一个是函数，一个是序列，map将传入的函数依次作用到序列的每个元素，并把结果作为新的list返回。 def add_one(x): return x+1for i in map(add_one,[1,2,3,4,5]): print(i) 输出:2 3 4 5 6由于list包含的元素可以是任何类型，因此，map() 不仅仅可以处理只包含数值的 list，事实上它可以处理包含任意类型的 list，只要传入的函数f可以处理这种数据类型。 3.列表去重,顺序不变123l = list(set(list))l.sort(key=list.index) 4.sorted()123456789101112131415通常我们得到一个列表排序后产生的新列表很容易这么写: new_ls = ls.sort()但是sort方法并没有返回值,因此new_ls的值为NONE.虽然ls.sort()之后ls已经排序成功,但我们经常并不希望原列表发生改变,这时我们就用到了sorted. 输入:ls=[3,1,2,4] ls.sort() 输出:ls:[1,2,3,4]Python的一个内置函数，使用方法与list.sort()大体一致，不同在于两个地方: 1.sorted(list)返回一个排序后的list，不改变原始的list. sort()是对原始的L进行操作，调用后原始的L会改变，没有返回值。 2.sorted()适用于任何可迭代容器. list.sort()仅支持list（sort()本身就是list的一个方法）基于以上两点，sorted使用频率比list.sort()更高些，所以Python中更高级的排序技巧便是 sorted().PS:如果想按照倒序排序的话，则只要将reverse置为true即可。 5.split默认切割123字符串的切割split()方法中如果没有参数,默认以空格分隔字符串。输入: '2018 08 04'.split()输出: ['2018','08','04'] 6.num进制的数字字符串 –&gt; 整型1int(str,num) # num表示该字符串内的数字为num进制 7.format 格式化123456789101112131415161718192021222324252627282930313233type 【可选】格式化类型 •传入” 字符串类型 “的参数 •s，格式化字符串类型数据•空白，未指定类型，则默认是None，同s •传入“ 整数类型 ”的参数 •b，将10进制整数自动转换成2进制表示然后格式化 •c，将10进制整数自动转换为其对应的unicode字符 •d，十进制整数 •o，将10进制整数自动转换成8进制表示然后格式化； •x，将10进制整数自动转换成16进制表示然后格式化（小写x） •X，将10进制整数自动转换成16进制表示然后格式化（大写X） •传入“ 浮点型或小数类型 ”的参数 •e， 转换为科学计数法（小写e）表示，然后格式化； •E， 转换为科学计数法（大写E）表示，然后格式化; •f ， 转换为浮点型（默认小数点后保留6位）表示，然后格式化； •F， 转换为浮点型（默认小数点后保留6位）表示，然后格式化； •g， 自动在e和f中切换 •G， 自动在E和F中切换 •%，显示百分比（默认显示小数点后6位） a = &quot;i am &#123;&#125;,age &#123;&#125;&quot;.format(&quot;seven&quot;,18,&quot;alex&quot;)b = &quot;i am &#123;&#125;,age &#123;&#125;, &#123;&#125;&quot;.format(*[&quot;seven&quot;, 18 ,&quot;alex&quot;])c = &quot;i am &#123;0&#125;, age &#123;1&#125;, really &#123;0&#125;&quot;.format(&quot;seven&quot;, 18)d = &quot;i am &#123;0&#125;, age&#123;1&#125;, really &#123;0&#125;&quot;.format(*[&quot;seven&quot;, 18])e = &quot;i am &#123;name&#125;, age &#123;age&#125;, really &#123;name&#125;&quot;.format(name=&quot;seven&quot;, age = 18)f = &quot;i am &#123;name&#125;, age &#123;age&#125;, rally &#123;name&#125;&quot;.format(**&#123;&quot;name&quot;:&quot;seven&quot;, &quot;age&quot;:18&#125;)g = &quot;i am &#123;0[0]&#125;,age&#123;0[1]&#125;, really&#123;0[2]&#125;&quot;.format([1,2,3],[11,22,33])h = &quot;i am &#123;:s&#125;, age &#123;:d&#125;, money &#123;:f&#125;&quot;.format(&quot;seven&quot;, 18, 888.1)i = &quot;i am &#123;:s&#125;, age &#123;:d&#125;&quot;.format(*[&quot;seven&quot;, 18])j = &quot;i am &#123;name:s&#125;, age &#123;age:d&#125;&quot;.format(name=&quot;seven&quot;,age=18)k = &quot;i am &#123;name:s&#125;, age &#123;age:d&#125;&quot;.format(**&#123;&quot;name&quot;:&quot;seven&quot;,&quot;age&quot;:18&#125;)l = &quot;numers:&#123;:b&#125;,&#123;:o&#125;,&#123;:d&#125;,&#123;:x&#125;,&#123;:X&#125;,&#123;:%&#125;&quot;.format(15,15,15,15,15,15.32445,2)m = &quot;numbers:&#123;0:b&#125;,&#123;0:o&#125;,&#123;0:d&#125;,&#123;0:x&#125;,&#123;0:%&#125;&quot;.format(15)&quot;numbers: &#123;num:b&#125;,&#123;num:o&#125;,&#123;num:d&#125;,&#123;num:x&#125;,&#123;num:X&#125;, &#123;num:%&#125;&quot;.format(num=15) 8.httplib123httplib模块改名为http.client import http.clienthttplib是一个底层的 HTTP 协议客户端，被更高层的 urllib.request 模块所使用。 9.获取屏幕分辨率/鼠标位置1234567891011from win32api import GetSystemMetricsfrom win32api import GetCursorPos# 分辨率x = GetSystemMetrics(0)y = GetSystemMetrics(1)# 鼠标位置GetCursorPos()python3 安装 pypiwin32后使用 10.pyinstaller123456789101112131415-c 参数 使用控制台，无界面(默认)-w 参数 使用窗口，无控制台.如果程序里有使用到控制台(如print)的就不可以使用-w, 否则会报错 '''failed to excute script xxx''' 如果想要捕捉错误信息可以先用控制台捕捉,没有报错后再使用无控制台. -D 参数 创建一个目录，包含exe文件，但会依赖很多文件（默认选项）。-F 参数 打包成一个exe文件-p 多文件打包时,以-p [其他.py] 的形式跟在主文件后 '''如:pyinstaller -w -F main.py -p view.py -p other.py'''-i 参数 修改打包后的exe图标,图标应放在py同级目录下,需要是ico格式,只改后缀不可用. '''如:pyinstaller -w -F -i zzz.ico main.py -p view.py -p other.py''' 11.程序退出的两种方式123451.import os 'os退出会直接退出,不抛出异常' os._exit() 2.import sys 'sys退出会抛出异常,如果捕捉异常代码依旧执行' sys.exit() 12.PYQT12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class GUI(QMainWindow):def __init__(self): super().__init__() self.iniUI()def iniUI(self): self.move(510,347) self.setWindowTitle('天命') self.resize(362,250) # 设置状态消息栏文本 self.statusBar().showMessage("Mai") # 标签 self.label1 = QLabel('天命', ) # 给标签1设字体 self.label1.setFont(QFont("cour", 25, QFont.Bold)) # 按钮 self.pushButton1 = QPushButton('天灵灵地灵灵') self.pushButton1.setFont(QFont("cour", 18)) self.pushButton1.clicked.connect(self.God_will) # 创建一个网格布局对象 grid_layout = QGridLayout() # 在网格中添加窗口部件 grid_layout.addWidget(self.label1, 0, 0) # 放置在0行0列 grid_layout.addWidget(self.pushButton1, 2, 0) # 2行0列 # 对齐方式 grid_layout.setAlignment(Qt.AlignTop) grid_layout.setAlignment(self.label1, Qt.AlignCenter) grid_layout.setAlignment(self.pushButton1, Qt.AlignCenter) # 创建一个窗口对象 layout_widget = QWidget() # 设置窗口的布局层 layout_widget.setLayout(grid_layout) self.setCentralWidget(layout_widget) # 无边框 self.setWindowFlags(Qt.FramelessWindowHint) # esc退出def keyPressEvent(self, e): if e.key() == Qt.Key_Escape: self.close() # 无边框时候拖动窗口def mousePressEvent(self, event): if event.button() == Qt.LeftButton: self.m_flag = True self.m_Position = event.globalPos() - self.pos() # 获取鼠标相对窗口的位置 event.accept() self.setCursor(QCursor(Qt.OpenHandCursor)) # 更改鼠标图标def mouseMoveEvent(self, QMouseEvent): if Qt.LeftButton and self.m_flag: self.move(QMouseEvent.globalPos() - self.m_Position) # 更改窗口位置 QMouseEvent.accept()def mouseReleaseEvent(self, QMouseEvent): self.m_flag = False self.setCursor(QCursor(Qt.ArrowCursor)) 123456if __name__ == '__main__': app = QApplication(sys.argv) gui = GUI() gui.setWindowOpacity(0.8) gui.show() sys.exit(app.exec_()) 13.python requests接收chunked编码问题 (https://blog.csdn.net/wangzuxi/article/details/40377467)123456789101112131415161718192021IncompleteRead(4360 bytes read)', IncompleteRead(4360 bytes read))原因:接收第一个chunked正常，第二次时line为空，导致int转换时出异常解决方案：方案1、修改httplib.py第581行为： chunk_left = int(line, 16) if line else 0 方案2、自己的程序忽略这个异常；方案3、import http.client try: response = request.urlopen(req) except http.client.IncompleteRead as e: response = e.partial partial 会将error接收到的原文返回 方案4、用pycurl来代替requests，但必须将HTTP协议版本设置为1.0，否则与方案2无差别，因为 Transfer-Encoding:chunked ， Connection:keep-alive 都是HTTP1.1的新特性，如果将 自己的HTTP协议版本设置为1.0，那么服务端将不会再返回chunked，而是以TCP分段的方式 直接返回整个文件内容，最后重组成一个完整的HTTP包。 14.mysql插入数据时,id字段不连续,数据也不连续12原因: id是唯一索引，冲突后id自增1,所以自增字段id不连续是正常的,中间的 空号很多且数据不连续,说明包含该插入语句的事务不成功的比例很多。 15.anaconda使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950查看所有的 packages：conda list如果你记不清 package 的具体名称，也可以进行模糊查询：conda search search_term如何管理Python环境？默认的环境是 root，你也可以创建一个新环境：conda create -n env_name list of packages其中 -n 代表 name，env_name 是需要创建的环境名称，list of packages 则是列出在新环境中需要安装的工具包。例如，当我安装了 Python3 版本的 Anaconda 后，默认的 root 环境自然是 Python3，但是我还需要创建一个 Python 2 的环境来运行旧版本的 Python 代码，最好还安装了 pandas 包，于是我们运行以下命令来创建：conda create -n py2 python=2.7 pandas 细心的你一定会发现，py2 环境中不仅安装了 pandas，还安装了 numpy 等一系列 packages，这就是使用 conda 的方便之处，它会自动为你安装相应的依赖包，而不需要你一个个手动安装。进入名为 env_name 的环境：source activate env_name退出当前环境：source deactivate另外注意，在 Windows 系统中，使用 activate env_name 和 deactivate 来进入和退出某个环境。删除名为 env_name 的环境：conda env remove -n env_name显示所有的环境：conda env list当分享代码的时候，同时也需要将运行环境分享给大家，执行如下命令可以将当前环境下的 package 信息存入名为 environment 的 YAML 文件中。conda env export &gt; environment.yaml同样，当执行他人的代码时，也需要配置相应的环境。这时你可以用对方分享的 YAML 文件来创建一摸一样的运行环境。conda env create -f environment.yaml至此，你已跨入 Anaconda 的大门，后续就可以徜徉在 Python 的海洋中了。 16.git添加公钥后报错sign_and_send_pubkey: signing failed: agent refused operation的解决办法1234在服务器添加完公钥后报错执行下面两句eval "$(ssh-agent -s)"ssh-add]]></content>
      <categories>
        <category>经验</category>
      </categories>
      <tags>
        <tag>经验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdawn使用笔记]]></title>
    <url>%2Fposts%2F13502%2F</url>
    <content type="text"><![CDATA[标题123456# 一阶标题 或者快捷键Ctrl+1## 二阶标题 或者快捷键Ctrl+2### 三阶标题 或者快捷键Ctrl+3#### 四阶标题 或者快捷键Ctrl+4##### 五阶标题 或者快捷键Ctrl+5###### 六阶标题 或者快捷键Ctrl+6 下划线1&lt;u&gt;下划线的内容&lt;/u&gt; 或按快捷键Ctrl+U 下划线的内容 字体加粗1**加粗内容** 或按快捷键Ctrl+B 加粗内容 斜体1*倾斜内容* 或按快捷键Ctrl+I 倾斜内容 删除线1~~删除线的内容~~ 或按快捷键Alt+Shift+5 删除线的内容 文字高亮1==我是最重要的== ==我是最重要的== 角标1x^2^ H~2~O 文本居中1&lt;center&gt;这是要居中的文本内容&lt;/center&gt; 这是要居中的文本内容 PS：Typora目前并不会直接预览居中效果——相应的效果只有输出文本的时候才会显现。 list有序1数字+英文小数点(.)+空格 策划目标 战前准备 开始行动 无序1+ 、- 、* 创建无序列，任意数字开始+空格创建有序列表 兔 马 Table1快捷键Ctrl+T弹出对话框 国籍 省份 市区 中国 山东 聊城市 分割线12***+回车 ---+回车 插入图片 将图片直接拖拽进来，自动生成链接 链接 内行式 百度一下，你就知道 1[百度一下，你就知道](https://www.baidu.com/) 参考式 [百度一下，你就知道][]https://www.baidu.com/ 1[百度一下，你就知道][]https://www.baidu.com/ # 第二个括号内可任意填写(不显) 快速链接 http://blog.csdn.net/wwwfrank2 1&lt;http://blog.csdn.net/wwwfrank2&gt; PS：按住Ctrl点击链接可直接打开。 数学公式(简) Typora支持加入用LaTeX写成的数学公式，并且在软件界面下用MathJax直接渲染。 行内公式(inline math) 可以在偏好设置中单独打开，由一个美元符号将公式围起来；name=将公式围起来；name=\prod \frac{1}{i^2}$ 行外公式，双$+回车 $$∏1i2∏1i2$$ 注：上标和下标可以使用数学表达式来获取 代码块 单行代码 PHP是世界上最好的语言 多行代码： ~~~markdown 1printf(&quot;Hello World!&quot;); 其余引用 引用 1&gt;空格 或按快捷键Ctrl+Shift+Q 表情🥇 1:1 目录1[TOC]]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Markdawn</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端笔记]]></title>
    <url>%2Fposts%2F37938%2F</url>
    <content type="text"><![CDATA[自己写一个博客难免用到前端东西，配合着某宝几块钱的视频也算是系统的过了一遍吧.. 把笔记发上来供以后查阅。 有点后悔写博客没有写笔记…虽然代码特别烂… 对整篇笔记的标记解释 标记 解释 🍟 了解 🌙 熟悉 ⭐ 重点 👑 重中之重 🐚 案例 🍓 甜点,解释,额外理解 🍦 作用,方法,属性 🔔 注意 🚀 💎 种类 ⚡ 🏆 总标题 笔记123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767视口: 在移动端观看不缩放&lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; 块元素-&gt;一行只能有一个 可以更改宽高值内联元素-&gt; 一行可以有多个 不能更改宽高值☢☢☢☢☢☢&lt;标签&gt;☢☢☢☢☢☢HTML 注释 &lt;!--This is a comment --&gt;&lt;html&gt; 描述网页&lt;body&gt; 是可见的页面内容&lt;h1&gt; 被显示为标题 &lt;h1&gt; - &lt;h6&gt; 大-&gt;小 &lt;p&gt; 段落 &lt;hr /&gt; 创建水平线&lt;br /换行&gt; &amp;nbsp;空格, &amp;lt;&lt;, &amp;gt;&gt;&lt;pre&gt; 预格式文本它保留了空格和换行,很适合显示计算机代码&lt;div&gt; 块级使用容器 (不能被P嵌套)&lt;span&gt; 内联使用 容器&lt;a href="http://www.w3school.com.cn" target="_blank(新页面)"&gt;This is a link&lt;/a&gt;&lt;img src="w3school.jpg" alt="图片无法显示时提示" width="104" height="142" /&gt;文本格式化标签 -&gt;直接用以下标签包裹文本或者格式化内容&lt;b&gt; 定义粗体文本。&lt;big&gt; 定义大号字。&lt;em&gt; 定义着重文字。&lt;i&gt; 定义斜体字。&lt;small&gt; 定义小号字。&lt;strong&gt; 定义加重语气。&lt;sub&gt; 定义下标字。&lt;sup&gt; 定义上标字。&lt;ins&gt; 定义插入字。&lt;del&gt; 定义删除字。★ CSS 样式㊙ 外联式&lt;link rel="stylesheet" type="text/css" href="mystyle.css"&gt;㊙ 嵌入式㊙ 内联式★ CSS选择器 &lt;style&gt;&lt;/style&gt; 通用选择器 * ▲ 标签选择器 div ▲ 类选择器 class ▲ id选择器 id★ 组合选择器 &lt;style&gt;&lt;/style&gt; 多元素选择器: 逗号 后代选择器: 空格 子代选择器: 大于 毗邻选择器: 加号 ⭐ 属性选择器 &lt;style&gt;&lt;/style&gt; a 标签的另类选择 [href="www"]&#123;&#125;⭐ 伪类选择器&lt;style&gt; input:focus(点下) &#123;border: 1px solid/dashed red;outline: none;&#125; &lt;/style&gt;☢☢☢☢☢☢☢☢☢☢&lt;属性&gt;☢☢☢☢☢☢☢☢☢☢❤属性 ❤作用 ❤举例width 设置元素(标签)的宽度 width: 200px;height 设置元素(标签)的高度 height: 200px;background-color 设置元素背景色 background-color: pink; rgba(r,g,b,alph)background-image 设置元素背景图片 background-image: url('./images/bg01.jpg')background-repeat 设置背景图片是否重复 background-repeat: not repeat;background 设置元素背景色或者背景图片 background: url('./images/bg01.jpg') pink; rgba(r,g,b,alph) not repeat;border 设置元素四周的边框 border: 1px solid pink;会影响宽高color 设置文字的颜色 color: red;font-size 设置文字的大小 font-size: 12px;font-family 设置文字的字体 font-family: 'Microsoft Yahei'; font-weight 设置文字是否加粗 font-weight: bold; line-height 设置文字的行高 line-height: 24px;text-decoration 设置文字的下划线 text-decoration:none; (取消下划线)margin 设置外边距 上右下左/没有对面拿 设置水平居中 0px autopadding 设置内边距 会影响宽高text-align 设置文字水平对齐方式 text-align:center text-indent 设置文字首行缩进 text-indent:32px;(默认字-16px)display - none 元素隐藏且不占位置 - block 此元素会被显示为块元素 - inline 此元素会被显示为内联元素 - inline-block 行内块元素 ( 了解 )overflow 管理元素异常溢出 hidden隐藏/scroll滑动/auto首选border-radius 边框圆角 border-radius:30px; cursor 鼠标图标变化 cursor:pointer;list-style:none; ⭐ 浮动: ( float )■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■margin父子塌陷: - 如果父级 div 中没有 border, padding, inline content, 子级 div 的 margin 会一直往上找,直到找到某个标签包括 border, padding, inline content中的其中一个, 然后按此 div 进行 margin.margin兄弟塌陷:- 如果上下两个元素都有设置相对的margin值, 则选择大的.盒子的真实尺寸 盒子的width和height值固定时，如果盒子增加border和padding，盒子整体的尺寸会变大，所以盒子的真实尺寸为：- 盒子宽度 = width + padding左右 + border左右- 盒子高度 = height + padding上下 + border上下⭐ 列表 有序列表 -&gt;有序号 &lt;ol&gt; 无序列表 &lt;ul&gt;⭐ emmet 语法 div.box&gt;.box1+.box2 #page&gt;div.logo+ul#navigation&gt;li*5&gt;a&#123;Item $&#125; / 大于号&#123;值&#125;/ $序号 div#header+div.page+"div#footer.class1.class2.class3" /"平级多个id和class"⭐ HTML表单 表单内使用p会有默认宽高 用于搜集不同类型的用户输入🐚 &lt;form&gt;标签 定义整体的表单区域 &lt;form action="http://www..." method="post"&gt;&lt;!-- action-&gt;发送目标地址 --&gt; &lt;p&gt; &lt;label for="username"&gt;姓名：&lt;/label&gt; &lt;!--for 与 input 关联,点击label,焦点也会出现--&gt; &lt;input type="text" name="username" id="username" placeholder="请输入姓名"/&gt; &lt;!-- placeholder 默认底值 --&gt; &lt;/p&gt; &lt;p&gt; &lt;label&gt;密码：&lt;/label&gt; &lt;!-- password-&gt;***** --&gt; &lt;input type="password" name="password" /&gt; &lt;/p&gt; &lt;p&gt; &lt;label&gt;性别：&lt;/label&gt; &lt;!-- radio-&gt;单选 --&gt;&lt;!-- name-&gt;关联 --&gt; &lt;input type="radio" name="gender" value="0" /&gt; 男 &lt;input type="radio" name="gender" value="1" /&gt; 女 &lt;/p&gt; &lt;p&gt; &lt;label&gt;爱好：&lt;/label&gt;&lt;!-- checkbox-&gt;多选 --&gt;&lt;!-- value-&gt;传值 --&gt; &lt;input type="checkbox" name="like" value="sing" /&gt; 唱歌 &lt;input type="checkbox" name="like" value="run" /&gt; 跑步 &lt;/p&gt; &lt;p&gt; &lt;label&gt;照片：&lt;/label&gt;&lt;!-- file-&gt;选择文件 --&gt; &lt;input type="file" name="person_pic"&gt; &lt;/p&gt; &lt;p&gt; &lt;label&gt;个人描述：&lt;/label&gt;&lt;!-- textarea-&gt;区域输入 --&gt; &lt;textarea name="about"&gt;&lt;/textarea&gt; &lt;/p&gt; &lt;p&gt; &lt;label&gt;籍贯：&lt;/label&gt;&lt;!-- select-&gt;下拉 --&gt;&lt;!-- option-&gt;选项 --&gt; &lt;select name="site"&gt; &lt;option value="0"&gt;北京&lt;/option&gt; &lt;option value="1"&gt;上海&lt;/option&gt; &lt;/select&gt; &lt;/p&gt; &lt;p&gt; &lt;input type="submit" name="" value="提交"&gt;&lt;!-- submit-&gt;提交 --&gt; &lt;input type="reset" name="" value="重置"&gt;&lt;!-- reset-&gt;重置 --&gt; &lt;/p&gt;&lt;/form&gt;⭐ 表格元素及相关样式 table&#123; border-collapse:collapse;&lt;!-- 边框重合 --&gt; text-align:center;&lt;!-- 居中 --&gt; &#125; &lt;table&gt;表格 &lt;thead&gt;表头 &lt;tbody&gt;数据体 &lt;tfoot&gt;脚注 &lt;th&gt;标题 &lt;tr&gt;行 &lt;td&gt;列 &lt;td colspan='2'&gt;1&lt;/td&gt;列合并 &lt;td rowspan='2'&gt;2&lt;/td&gt;行合并 ⭐ 定位 文档流(了解) position &gt;&gt; left,top,right,bottom - static 不如不写 - relative 相对定位,占位,相对原来的位置定位 - absolution 绝对定位,不占位,脱离文档流,相对于上一个设置了定位的父级元素定位,如果找不到,脱离文档流,相对于body定位. - fixed 固定定位,不占位,相对于浏览器窗口定位 元素层级 &gt;&gt; 脱离文档流后的层叠顺序 - z-index🐚 弹窗 &gt;&gt; 叠层 - opacity 透明度 position - margin&gt;auto 犯冲🌙 嵌套网页 iframe - src 嵌套网页地址 - frameborder 边框 - name - a 标签搭配 &lt;!-- a的target关联iframe的name --&gt;⭐ CSS权重值计算 - 内联样式 1000 - 标签选择器 1 - 类选择器 10 - 伪类选择器 10 - id选择器 100🌙 打包前端 hbuilder -------------------- ⭐ 打印名片敲三遍 🌙 js引入方式 行间事件 onclick 任何标签都可以添加的属性 嵌入 &lt;head&gt;&lt;script&gt;&lt;/script&gt;&lt;/head&gt; 外部引入 &lt;script src="js/xxx.js"&gt;&lt;/script&gt;🌙 变量 var a = 5⭐ 数据类型 number 数字 string 字符串 boolean 布尔 -&gt; 小写 undefined 未定义类型 -&gt; 未定义类型不是未定义错误,它本身有一个undefined值 null null类型 var age = null , 它的值是null,它的上面有一个object类型 object 复合类型 -- typeof -- 查看类型 🌙 注释与分号 css &lt;!-- 这是注释 --&gt; js // 这是单行注释 通用多行注释 /* 这是注释 */ 分号 根据规定可加可不加🌙 标识符 区分大小写 第一个字符必须是字母/下划线/$ 其他字符可以是字母/下划线/$/数字🌙 命名风格 对象 o 数组 a 字符串 s 整数 i 布尔值 b 浮点数 f 函数 fn 正则 re jQuery对象 $⭐ 函数 定义 function 函数名( 参数 )&#123; &#125; 调用 函数名( 参数 ) js 中有预解析功能,函数调用可放在定义前不出错 变量定义也有预解析,但变量赋值没有预解析 返回值 return 返回值或对象/结束函数运行⭐ 全局变量与局部变量 定义在函数体外的变量都是全局变量,在函数体内不需要global声明可以直接调用. 定义在函数体内的变量如果没有var声明,也是一个全局变量. 定义在函数体内的变量如果使用var声明,则是函数体内的局部变量,在函数体外不可用.⭐ 匿名函数与封闭函数 - 没有名字的函数就是匿名函数. function () &#123;&#125; - 封闭函数 - 匿名函数的调用. (function () &#123;&#125;)() !function () &#123;&#125;() ~function () &#123;&#125;()🔔 条件运算符 == 只要求值相当(比较时会转换数据类型) === 要求值和类型都相等(等同Python'==') &gt; 大于 &lt; 小于 &gt;= 大于等于 &lt;= 小于等于 != 不等于🔔 逻辑运算符 &amp;&amp; and || or ! not👑 异或运算 相同 1 不同 0🏆 if if (判定条件)&#123; 执行语句 &#125;else if&#123; &#125; else &#123; &#125;🐚 事件与取值 onclick 单击后触发 ondbclick 双击后触发 alert(str) -&gt; 弹窗,可用来当作print使用,阻断程序运行,碰到alert,alert下面的代码不加载不显示,直到被点击 console.log() -&gt; 相当于print onmouseover 鼠标进入(进入子元素也触发) onmouseenter 鼠标进入(进入子元素不触发) onmouseout 鼠标离开(离开子元素也触发) onmouseleave 鼠标离开(离开子元素不触发) getElementById 获取到该id的标签 getElementByClassName 获取到该class的集合 window.innerHeight 获取浏览器高度 innerHTML 获取标签内的内容 opacity 透明度 document.body.scrollTop || document.documentElement.scrollTop 获取当前页面滚动条纵坐标的位置 两个都是获取滚动条滚动的高的方法,但因为浏览器的兼容性原因,总会只有一个生效,所以使用或连接 window.onload = function ()&#123; -&gt; 使onload后的函数 在body 加载后执行 var oDiv= document.getElementById(标签id) -&gt; 获取到此id的标签 var oDiv= document.getElementByClassName(class)[0] -&gt; 获取到此class的集合中第0号标签 oDiv.onclick = function ()&#123; -&gt; 匿名函数绑定事件 alter('hello world') &#125; oDiv.href -&gt; 获取元素属性 oDiv.href = '123' -&gt; 更改元素属性 oDiv.style.color = 'red' oDiv.innerHTML -&gt; 获取此标签内的文本(标签/文字) oDiv.innerHTML= '&lt;p&gt;哈哈&lt;p&gt;' -&gt; 改变标签内的文本 oDiv.className = 'username' -&gt; class必须要className才可以获取到,只此一个特殊 可更改类名以更换样式或更改引用的样式文件路径以更换整体样式 &#125;👑 getElementById 写在元素上面,会获取不到 第一种方法: 将js放在页面最下面 第二种方法: 将js语句放在window.onload 触发的函数里面🌙 匿名函数 没有名字的函数前 跟上事件--------------------🌙 数组 var 数组名 = new Array(元素一,元素二,...) var 数组名 = [元素一,元素二,.....] 🍦 数组的属性: length 🍦 数组的方法: join() 将列表所有元素以指定字符串为分隔组成字符串 push() pop() 从数组最后增加或删除元素 reverse() 将数组反转 indexOf() 返回数组中元素第一次出现的下标索引值,元素不存在返回-1不报错 splice() 在数组中增加或删除成员 -&gt; 数组名.splice(操作元素下标, 操作几位[该参数为0默认增加元素不删除,否则默认进行删除操作], 添加的新元素[可不写]) -&gt; 返回被操作对象 🐚 案例 var aList = [1,2,3] console.log(aList.length) alert(aList[1]) -&gt; 只有从左向右取,不可以倒取 console.log(aList.join('-')) aList.push('哈哈') aList.pop() aList.reverse() aList.indexOf('2') alert(aList.splice(1,2)) alert(aList.splice(1,2,'哈哈','呵呵'))⭐ for循环 for (条件) &#123; 执行代码 &#125; 🐚 案例 for (var i = 10;i&gt;0;i++)&#123; 执行代码 &#125;🌙 字符串 字符串无论和什么相加,结果都是字符串 🍦 字符串的方法 parseInt() 将数字字符串转化为整数 parseFloat() 将数字字符串转化为小数 parsedoubleFloat() 将数字字符串转化为小数(长) split() 把一个字符串以字符组成的数组 substring() 切片,无步长 🐚 案例 var a = 123 var str = '123' console.log(parseInt(str)) str.substring(0,2) str.split('').reverse().join('') 字符串反转🍓 调试 alert() 弹窗 console.log() 浏览器控制台 document.title() 网页标题 document.write() 写到body里👑 定时器 🍦 作用: 定时调用函数 制作动画 模拟网络延时的情况 💎 种类: setTimeout() 执行一次的定时器 setTimeout(func,3000) -&gt;函数名,毫秒 setInterval() 执行多次的定时器 clearTimeout() 关闭只执行一次的定时器 clearInterval(哪个定时器) clearInterval() 关闭反复执行的定时器 🐚 案例: setTimeout(func,3000) var timer = setInterval(func,3000) function func () &#123; alert('调用了func') clearInterval(timer) &#125;--------------------一个框架里,API始终在更新,是记不完的.--------------------🍓 库和框架,没有具体的界限,一般的来讲: 库 小而精 框架 大而全&lt;!-- 淘宝模版市场 --&gt;🏆 jQuery 🏆🍓 加载顺序 body-&gt;标签-&gt;jQuery.ready-&gt;资源-&gt;window.onload🍦 方法 $(document).ready(function()&#123;&#125;) 作用与window.onload相同,完全版 $(function()&#123;&#125;) 作用与window.onload相同,精简版 index() 获取元素相对于同级元素的下标位置 animate - 翻译为'动画' 动画效果 参数一:要改变的样式属性值,写成字典 参数二:动画持续的事件,单位为毫秒 参数三:动画曲线,默认为'swing':缓冲运动(加速-匀速-减速),'linear':匀速运动 参数四:动画回调函数,动画完成后执行的匿名函数 html() 作用与innerHTML相同,里面写内容为替换,不写为取值 val() 不传值获取input的内容,传值默认写入 prop() 获取标签自带的属性值 attr() 获取标签内的所有属性值 each() 标签对象循环 click() 作用与onclick相同 dblclick() 作用与ondbclick相同 focus() 元素获得焦点,可添加回调函数 blur() 元素失去焦点,可添加回调函数 hover() 同时为mouseenter和mouseleave事件指定处理函数,可以写两个调用函数 submit() 用户递交表单,自己手写就使用自己的,不写就自动发给form的action后台 如果内部返回一个false,表单不会被提交 keyup keydown mousemove🍦 jQuery选择器 - 选择规则和css样式相同 🍓 - 通过选择器拿到的是jQuery对象,通俗的讲究是一个容器,能够使用下标取值 $('#box') $('div') $('.pbox') $('a[href="#"]') $(this) 代表调用事件的标签对象 this-&gt;标签 $(this)-&gt;对象⭐ 选择集过滤 - 得到的都是对象 $().has() 包含 $('div').has('p') 选择包含p元素的div元素 $().not() 不包含 $('div').not('.myclass') 选择clss不等于myClass的div元素 $().eq() 下标等于 $('div').eq(5) 选择下标为6的div元素 - equal:相等 使用[index]获取的不是对象,不方便使用⭐ 选择集转移 $('').prev() 被选择的元素前面紧挨的同辈元素 $('').prevAll() 被选择的元素前面所有的同辈元素 $('').next() 被选择的元素后面紧挨的同辈元素 $('').nextAll() 被选择的元素前面所有的同辈元素 $('').parent() 被选择的元素的父元素 $('').children() 被选择的元素的所有子元素 $('').siblings() 被选择的元素的所有的同级元素(不包括自己)) $('').find('') 被选择的元素'内'的满足条件的标签⭐ 操作样式 $('').css('width') 获取样式 $('').css('width','100px') 单个样式的修改/设置 $('').css(&#123;'width':'100px','color':'pink'&#125;) 多个样式的修改/设置 $('').addClass('divbox') 追加类名 $('').removeClass('divbox') 移除类名 $('').toggleClass('divbox') 追加/移除,自动检测元素是否有此类名,若有则移除,若没有则追加🐚 案例&lt;script src="./jquery-1.12.4.min.js"&gt;&lt;/script&gt;&lt;style&gt; .box &#123; width: 100px; height: 100px; background-color: green; &#125; .divbox &#123; width: 300px; height: 300px; background-color: pink; &#125;&lt;/style&gt;&lt;script&gt; $(function()&#123; // 选取元素 $('div') $('#box') $('.pbox') $('a[href="#"]') // 选择集过滤 $('div').has('p') $('div').not('.box') $('ul li').eq(2) // 选择集转移 $('.box').prev().html('哈哈') $('.box').prevAll().html('hehe') $('.box').next().html('hehe') $('.box').nextAll().html('hehe') $('.box').parent().prev().html('嘿嘿') $('.box').parent().next().children().html('哈哈') $('.box').parent().siblings().html('haha') $('div').find('.secondbox').html('haha') // 操作样式 $('#box').css(&#123;'width':'100px','height':'100px','background-color':'yellow','color':'pink'&#125;) //修改样式 $('.box').addClass('divbox') //追加类名 $('.box').removeClass('box') //移除类名 $('button').click(function () &#123; $('.divbox').toggleClass('box') //新增/移除类名 $('.box').toggleClass('divbox') //新增/移除类名 &#125;) &#125;) &lt;/script&gt;&lt;body&gt; &lt;!-- 选取元素 --&gt; &lt;div &gt;&lt;/div&gt; &lt;div id="box"&gt;box&lt;/div&gt; &lt;div class="pbox"&gt;pbox&lt;/div&gt; &lt;a href="#"&gt;a&lt;/a&gt; &lt;!-- 选择集过滤 --&gt; &lt;div id="box"&gt;box&lt;/div&gt; &lt;div&gt;item $$$ &lt;p&gt;&lt;/p&gt;&lt;/div&gt; &lt;div&gt;item $$$ &lt;p&gt;&lt;/p&gt;&lt;/div&gt; &lt;div&gt;item $$$&lt;/div&gt; &lt;ul&gt; &lt;li&gt;item 1&lt;/li&gt; &lt;li&gt;item 2&lt;/li&gt; &lt;li&gt;item 3&lt;/li&gt; &lt;li&gt;item 4&lt;/li&gt; &lt;li&gt;item 5&lt;/li&gt; &lt;/ul&gt; &lt;!-- 选择集转移 --&gt; &lt;div id="box"&gt;1&lt;/div&gt; &lt;div&gt;2&lt;/div&gt; &lt;div&gt;&lt;div class="box"&gt;3&lt;/div&gt;&lt;/div&gt; &lt;div&gt;&lt;a href="#"&gt;四号的a标签&lt;/a&gt;&lt;/div&gt; &lt;div&gt;&lt;p class="secondbox"&gt;&lt;/p&gt;&lt;/div&gt; &lt;div class="last"&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=""&gt;微微&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;wewe&lt;/p&gt; &lt;/div&gt; &lt;!-- 操作样式 --&gt; &lt;div &gt;&lt;/div&gt; &lt;div id="box"&gt;box&lt;/div&gt; &lt;div class="pbox"&gt;pbox&lt;/div&gt; &lt;a href="#"&gt;a&lt;/a&gt; &lt;div class="box"&gt;哈哈&lt;/div&gt; &lt;button&gt;按钮&lt;/button&gt;&lt;/body&gt;--------------------下载node,自带npm,npm install animate--------------------⭐ jQuery特殊效果 - 与display:none;搭配使用 fadeIn() 淡入 fadeOut() 淡出 fadeToggle() 切换淡入/淡出 hide() 隐藏 show() 显示 toggle() 切换隐藏/显示 slideDown() 收起 slideUp() 展开 slideToggle() 切换收起/展开 🐚 案例 &lt;script&gt; $(function()&#123; $('.box').fadeIn() $('.box').fadeOut() $('.box').fadeToggle() $('.box').hide() $('.box').show() $('.box').toggle() $('.box').slideDown() $('.box').slideUp() $('.box').slideToggle() &#125;) &lt;/script&gt; &lt;style&gt; .box &#123; width: 100px; height: 100px; background: blue; display: none; &#125; &lt;/style&gt; &lt;body&gt; &lt;button class="button"&gt;&lt;/button&gt; &lt;div class="box"&gt; &lt;/div&gt; &lt;/body&gt;🍟 正则表达式 🔔 定义 var re = /规则/参数 参数[全文检索:g,忽略大小写:i],可以不写 🔔 规则 规则和Python一样 🔔 测试 var result = re.test(str) 返回值要么是true要么是false 🍓 为jQuery添加函数 🍟 jQuery.fn.slideLeftHide = function slideLeftHide(speed) &#123;&#125;--------------------🌙 事件冒泡 小明被打一下,无论他反击不反击, 都会告诉他爸,看他爸咋干, 他爸不论反击不反击,也都会告诉小明他爷 以此类推,click和mouseover都属于冒泡事件 在一个对象上触发某类时间,如果此对象定义了此事件的处理程序, 那么此事件就会调用这个处理程序,如果没有定义此事件处理程序或者事件返回true, 那么这个事件就会向这个对象的父级对象传播,从里到外,直至它被处理(父级对象所有 同类事件都将被激活),或者它到达了对象层次的最顶层,即document对象(有些浏览器是window) 🍦 事件冒泡的作用 事件冒泡允许多个操作被集中处理(把时间处理器添加到一个父级元素上,避免把 事件处理器添加到多个子级元素上),它还可以让你在对象层的不同级别捕获事件. ⭐ 阻止事件冒泡和默认行为 return false⭐ 设计模式 解耦 - 解除耦合度 - 把复杂的事情简单化 耦合度越高对代码后期越不利 ⭐ 事件委托 $('父级').delegate(参数一,参数二,参数三) 参数一 代理对象 参数二 代理对象委托的事件 参数三 代理事件的触发函数🌙 Dom操作 1.移动现有标签的位置 2.将新创建的标签插入到现有的标签中 🍦 方法 $('selector').append($(content)) 在元素内部添加,从后面放入 $(content).appendTo($('selector')) $('selector').prepend($(content)) 在元素内部添加,从前面放入 $(content).prependTo($('selector')) $('selector').after($(content)) 在元素外部添加,从后面放入 $(content).insertAfter($('selector')) $('selector').before($(content)) 在元素外部添加,从前面放入 $(content).insertBefore($('selector')) $('selector').remove($(content)) 删除标签🌙 js对象 🐚 案例 var person = &#123; name:'maimai', age:18, sayName:function()&#123; alert('My name is' + this.name); &#125; &#125; alert(person.name) person.name = 'shuaibi' person.sayName()⭐ Json - 普遍使用的数据传输形式 - 除了数字,都要加双引号 🐚 案例 var json = &#123; "name":"tom", "age":18 "school":&#123; "name":"luoyangnormaluniversity" "location":"luoyang" &#125; &#125; json.school.name⭐ ajax - 封装在jQuery 🍓 同步 - 做完一件事后再做另一件事 异步 - 同时做几件事 ajax实现了局部刷新(无刷新) - ajax可以自己发送http请求不用通过浏览器的地址栏, 所以页面整体不会刷新,ajax获取到后台数据,更新页面显示数据的部分,就做到了页面局部刷新. 🍦 ajax使用方法(常用参数) $.ajax(&#123; url:'请求地址', type:'请求方式:默认是'GET',常用的还有'POST'', dataType:'设置返回的数据格式,常用的是'json'格式,也可以设置为'html'', data:'设置发送给服务器的数据', success:'设置请求成功后的回调函数', error:'设置请求失败后的回调函数', async:'设置是否异步,默认值为'true',表示异步' &#125;) 🐚 案例 $.ajax(&#123; url:'/change_data', type:'GET', dataType:'json', data:&#123;'code':30000&#125;, &#125;).done(function(data)&#123; console.log(data) &#125;).fail(function(error)&#123; console.log('error') &#125;)--------------------nodemon 运行js文件--------------------👑Vue👑🔔 方便理解 Vue的使用目的就是让js少写点,使固定的逻辑尽量写在标签中,这样就把jquery获取标签的步骤给省掉了 ⭐ vue使用 第一步引入 第二步div给坑 第三步vue对象实例化 var vm = new Vue({ // 变量赋值可写可不写 }) 🔔 注意 - new Vue() 实例化对象 - 参数 el:关联HTML部分标签,使vue中的内容能够加载到HTML里面 data:页面中需要的数据,可以通过这个属性进行初始化,进而赋值到HTML页面去 methods:可以给当前vue对象添加方法,一般我们都会把方法放在这个对象里面 computed:计算属性,可以给data里面的值添加一些管理 watch:如果需要监控data中的某些属性值,可以在watch中添加监听方法 - 属于设计模式(观察者模式)用的很少 - 小胡子语法:{{}}(插值表达式) - 不可以在属性中使用 - 点击事件:v-on ⭐ 指令 &apos;v-&apos; 开头的都是指令 - 只要是指令,data中的值可以直接使用 v-if 接受一个布尔值,True显示,false隐藏 - 隐藏的实质是标签被销毁 v-else 和v-if挨着 -共享if布尔值 v-else-if 接受另一个布尔值 - 可以无限使用和v-if挨着 v-on 触发事件 - 缩写 &gt;&gt;&gt; : v-bind 属性绑定data - 缩写 &gt;&gt;&gt; @ v-show 隐藏或展示 - 隐藏的实质是标签display:none v-hide 隐藏或展示 - 隐藏的实质是标签display:none ⭐ 三元表达式 - 三元表达式在属性里必须写在数组中 {{isTrue ? '123' : '345'}} {{ok ? 'YES':'NO'}} ⭐ Class绑定(class和v-bind配合使用) - 受data控制 &lt;div :class=&quot;{}&quot;&gt;&lt;/div&gt; &lt;div :class=&quot;[]&quot;&gt;&lt;/div&gt; 🐚 案例 &lt;script src=&quot;./vue.min.js&quot;&gt;&lt;/script&gt; &lt;body&gt; &lt;div id=&quot;app&quot;&gt; {{ msg }} // 小胡子语法 {{isTrue ? '123' : '345'}} // 如果True,输出123,否则输出345 {{ok ? 'YES':'NO'}} // 如果True.输出YES,否则输出NO &lt;div v-if=&apos;isOK&apos;&gt;if&lt;/div&gt; &lt;div v-else-if=&apos;isOK&apos;&gt;elif&lt;/div&gt; &lt;div v-else&gt;else&lt;/div&gt; &lt;button v-on:click=&quot;func&quot;&gt;&lt;/button&gt; // 函数使用 &lt;button @click=&quot;func&quot;&gt;&lt;/button&gt; // 函数使用 - 缩写 &lt;img src=&quot;&quot; v-bind:alt=&apos;msg&apos;&gt; // 属性展示data数据 &lt;img src=&quot;&quot; :alt=&apos;msg&apos;&gt; // 属性展示data数据 - 缩写 {{ msg+1 }} // 可写逻辑不推荐,推荐写入computed {{ newMsg }} // computed中的逻辑运算 &lt;button @click=&quot;msg += 1&quot;&gt;按钮&lt;/button&gt; &lt;button @click=&quot;func&quot;&gt;按钮&lt;/button&gt; &lt;div :class=&quot;{box:isTrue,divbox:isHave}&quot;&gt;div&lt;/div&gt; // class 绑定 &lt;div :class=&quot;[firstStyle, secondStyle]&quot;&gt;div&lt;/div&gt; &lt;div :class=&quot;[isTrue ? &apos;box&apos; : &apos;divbox&apos;]&quot;&gt;div&lt;/div&gt; &lt;div :style=&quot;backgroundColor:colorname,color:color1&quot;&gt;div&lt;/div&gt; &lt;div :style=&quot;[firstName,secondName]&quot;&gt;div&lt;/div&gt; &lt;/div&gt; &lt;script&gt; new Vue({ el:&apos;#app&apos;, data:{ msg:123, // 变量:&quot;值&quot; isOK:true, isTrue:true, isHave:true, firstStyle:{ box:true, divbox:true, }, secondStyle:{ box:false, divbox:false, }, colorname:pink, color1:red, firstName:{ colorname:pink, }, secondName:{ color1:red, } }, methods:{ func:function(){ // 函数名:函数体 this.msg += 1 } }, computed:{ newMsg:function(){ return this.msg+123 // this指代vue对象,不需要写data } }, watch:{ //watch中的function不是函数 msg:function(newValue,oldValue){ console.log(newValue) console.log(oldValue) 观察msg,如果msg发生变化,执行本方法 } } }) &lt;/script&gt; &lt;/body&gt; ⭐ 列表渲染 - v-for 🍦 遍历数组 v-for=”item in items” 🍦 遍历对象 v-for=&quot;(value,key,index) in obj&quot; 🐚 案例 &lt;script&gt; new Vue({ el:&quot;#ul&quot;, data:{ items:[&apos;foo&apos;,&apos;bar&apos;], object:{ firstName:&apos;John&apos;, lastName:&apos;Doe&apos;, age:30 } } }) &lt;/script&gt; &lt;body&gt; &lt;div id=&quot;ul&quot;&gt; &lt;ul&gt; &lt;li v-for=&quot;(item,index) in items&quot;&gt; {{index}} - {{item.message}} &lt;/li&gt; &lt;/ul&gt; &lt;ul&gt; &lt;li v-for=&quot;(key,value,index) in object&quot;&gt; {{index}} - {{key}}: {{index}} &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/body&gt; ⭐ 事件修饰符 🍦 作用 stop 阻止事件冒泡 prevent 阻止默认行为 🍦 使用方法 //点击事件后触发doThat方法,然后阻止事件冒泡和默认行为 //点击提交后什么都不做 ⭐ v-model - 表单元素,使用data并更新data 🐚 案例 new Vue({ data:{ msg:’123’, check:’’, checks:[], radio:’’, select:’0’} }) &lt;/script&gt; &lt;body&gt; {{msg}} // input改变msg的值后,msg进行更新 &lt;input type=&quot;text&quot; v-model=&quot;msg&quot;&gt; &lt;textarea v-model=&quot;msg&quot; cols=&quot;30&quot; rows=&quot;10&quot;&gt;&lt;/textarea&gt; {{check}} // 单个复选框 &lt;input type=&quot;checkbox&quot; v-model=&quot;check&quot;&gt; {{checks}} //多个复选框 -加value &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;0&quot; v-model=&quot;checks&quot;&gt; &lt;input type=&quot;checkbox&quot; name=&quot;hobby&quot; value=&quot;1&quot; v-model=&quot;checks&quot;&gt; {{radio}} //单选 -加value &lt;input type=&quot;radio&quot; v-model=&quot;radio&quot; value=&quot;0&quot; name=&quot;gender&quot;&gt; &lt;input type=&quot;radio&quot; v-model=&quot;radio&quot; value=&quot;1&quot; name=&quot;gender&quot;&gt; {{select}} //下拉框 -加value &lt;select v-model=&quot;select&quot; name=&quot;&quot; id=&quot;&quot;&gt; &lt;option value=&quot;0&quot;&gt;0&lt;/option&gt; &lt;option value=&quot;1&quot;&gt;1&lt;/option&gt; &lt;option value=&quot;2&quot;&gt;2&lt;/option&gt; &lt;option value=&quot;3&quot;&gt;3&lt;/option&gt; &lt;option value=&quot;4&quot;&gt;4&lt;/option&gt; &lt;/select&gt; &lt;/body&gt; ⭐ 过滤器 - 函数工厂 🍦 定义 Vue.filter(‘工厂函数名’,function(参数){ }) 🍦 使用 | - 管道 - 可在小胡子里或者v-bind中使用 🔔 注意 过滤器可以写在vue外面也可以写在里面 🐚 案例 &lt;script src=&quot;./日常琢磨与练习/2. 前端/js/vue.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; window.onload = function (){ // 过滤器写在Vue外面 Vue.filter(&apos;Upper&apos;,function(value){ return value.toUpperCase() }) new Vue({ el:&apos;#app&apos;, data:{ msg:&apos;abcdefg&apos;, msg2:&apos;ABCDEFG&apos; }, // 过滤器写在Vue里面 filters:{ lower:function(value){ return value.toLowerCase() } } }) } &lt;/script&gt; &lt;body&gt; &lt;div id=&quot;app&quot;&gt; {msg} {msg | Upper} {msg2 | lower} &lt;/div&gt; &lt;/body&gt; ⭐ 自定义指令 🍦 作用 对普通DOM元素进行底层操作 🍦 使用 v-自定义指令名 🔔 注意 directive可以写在vue外面也可以写在里面 🐚 案例 &lt;script src=&quot;./日常琢磨与练习/2. 前端/js/vue.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; window.onload = function(){ Vue.directive(&apos;haha&apos;,{ inserted:function(el){ el.style.width = &apos;300px&apos;, el.style.height = &apos;300px&apos;, el.style.backgroundColor = &apos;pink&apos; } }) new Vue({ el:&apos;#app&apos;, data:{ } }) } &lt;/script&gt; &lt;body&gt; &lt;div id=&quot;app&quot;&gt; &lt;div v-hah&gt;&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; ⭐ 生命周期 (beforeCreate) - (created) - el - template - (beforMount)(挂载) - (mounted) -(是否交互)- (是)(beforeUpdate) - (updated) - (是否交互循环) - (是否销毁) - (是)(beforeDestroy) - (destroyed) 🍓 钩子函数(中间件) 程序从始至终一条线走到头,但我们可以在程序走的路上埋上一些函数,当程序走到这个地方时候必定会触发该函数 🍓 挂载 渲染前做的事情 🍓 vue对象的销毁方法 vm.$destroy() ⭐ 数据交互 - Vue里的ajax 🔔 注意 ajax是在jQuery里,但axios不在vue里 🐚 示例 window.onload = function(){ var vm = new Vue({ el:’#app’, methods:{ func:function(){ axios.get({ url:’’ }) .then(function(){ }) .catch(function(){ }) } } }) } &lt;/script&gt; `]]></content>
      <categories>
        <category>前端</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>前端</tag>
        <tag>HTML</tag>
        <tag>JS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录搭博客]]></title>
    <url>%2Fposts%2F65123%2F</url>
    <content type="text"><![CDATA[记：结合视频和flask狗书，讲真的关于Python的许多东西还都没有搞清楚，什么类初始化什么self都是什么东西… 但是总要上手项目能快速进步些，即使确实写的特别烂… 最终界面： 算是极简风格吧…还是蛮开心的-.- 2017-2-1所有页面的排版重新调整 14:22 关于我页面的设计排版 15:53 2017-1-31文章还需要实现编辑功能和主页分页功能、评论功能 所有的功能简单实现都很简单，但做到完善很难，混乱的逻辑整理清晰并不简单，加油。 今天先简单实现删除功能 \简单的实现出来并没有难度添加几行代码的事，但是牵扯到角色用户权限的逻辑功能 2017-1-30实现主页个人资料排版和主页最后整版 17:37 实现jinjia2模板整理整洁干净有逻辑 14:29 2017-1-29实现MVC模式 什么是MVC 什么是耦合性 16:22 实现主页显示概要 18:19 2017-1-26心里骂娘啊，换行是因为数据原因，然后最后居然是在jinjia模板里数据后面加一个|safe ，！！ 当我看到正确显示了所有html标签后我的心里是崩溃的。。 实现了文章的正确显示 15：03 接下来几天要实现的是: 主页应该只显示一个概要，现在却是把所有文字显示了出来，应该是数据长短的问题，改一下就好了。 然后是文章的编辑和删除功能，应该是判断目前登录的人是不是文章的作者，如果是的话，出现编辑和删除功能， 最后就是评论功能了 最后的最后，域名服务器阿喂我刚开始看的时候完全懵逼啊都是些什么跟什么-。- 不管了，井然有序的执行吧loser~ 2017-1-25实现了登录页面重新排版 16:05 实现了发表文章页面的重新排版和表单实现 17:47 本来想要把登录之后输入登录页面自动跳转到主页，在模板里写出来了登陆判断，就是没法写怎么在模板里实现页面跳转，只好显示点击返回主页好了，blog[“byuser”] = user.find_one({‘_id’:ObjectId(current_user.get_id())})[‘username’]，current_user函数可以实现python里判断登录人，之后可以把跳转页面和这个函数坐下关联实现后端判断登录并页面跳转。 16:38 换行依旧没有解决，数据类型不知道哪里爆炸了转换不过来，待解决 这着实大坑啊占了三天了阿喂！！！ 18:55 2017-1-24富文本编辑器出现个问题，在本页面下拉滚动条时，富文本编辑器的功能栏会始终贴着浏览器上方滚动 当浏览器滚动条下拉到看不见编辑器时功能栏才会消失，看了下源代码应该是编辑器属性css的问题，但是找不到这个编辑器css在哪，只好把浏览器和编辑器调整好尺寸不让浏览器的滚动条出现。 1.实现换行功能 未完成 2.实现登录回话保持 – 22:05 3.实现注销 –22:10 4.实现登录和注销在模板中的条件判断显现 – 22:57 \5. 实现了登录表单，之前的登录都是伪登录，并且不能保持登录状态，这次不仅可以验证登录，也可以保持登录状态并实现注销和 其他页面的登录验证条件判断 换行并没有解决，明天实现发表文章的正确表单入库试试，应该可以解决，登录界面今天因为加了表单算是毁了需要重新排版，由此也知道了程序一开始的基础就要打牢，不然坑真的好多。 –17：07 2017-1-23文章:文章的功能实现不好，需要根本上的改动，还有就是，发表的文章不能实现换行功能。 所以，今天的目标是 第一: 实现 点击阅读全文进入文章界面。 20:05 完成 第二：实现文章的换行功能 22:36 了解富文本编辑器功能，准备实现 第三:实现文章按时间先后显现 23:13 完成 富文本编辑器功能已实现，换行功能出bug啊妥妥的！！不知道什么原因明天再战！ 1:01 2017-1-22主页测试blog排版、显现 2:47 完成 （显现排版时间顺序不对，需改进） 发表文章页面排版、功能跳转的思路捋清 23:56完成 （超额完成，实现了发表文章功能，并成功显现在主页 ） 数据库blog、user、comment等设计 1:22 完成 2017-1-21实现数据库连接 01：23 完成 实现post跳转 23：34 完成 从数据库中读出测试博客并显现在主页 3:11 完成]]></content>
  </entry>
  <entry>
    <title><![CDATA[编程之路（五）——和时间赛跑]]></title>
    <url>%2Fposts%2F3689%2F</url>
    <content type="text"><![CDATA[选择一条学习的路很重要，选择岔了偏了，那必定是浪费时间导致延长学习线。 ​ 大二下学期期末考结束，一年一度的假期狂欢来临，而我愈发紧张愈发不安。 ​ 我知道在这门行业上和别人已经出现了数年的差距，当他人遇到挫折颓唐遇见难题停滞不前时候，正是我趁机赶超时候。 ​ 而这个假期的学习计划由于考驾照很不愉快的半夭折了，当然还有一部分原因在于那时刚起步基础知识搞不懂很枯燥导致学习兴趣并没有那么高涨。而这个阶段还是马马虎虎把假期之前学习的教程又过了一遍，然后又机缘巧合得到一套很不错的爬虫教程，这套教程对当时的我来说除了知道用了什么语法外一丁点都搞不懂，但它让我知道了基础知识里哪些东西很重要是必须要去理解通透的。比如模块、包、方法、面向对象等等，这些如果理解不了，根本就寸步难行。这让我在浑浑噩噩的假期结束后立马开始了第三遍基础知识学习，之前的两次学习根本上是让我捋清楚我究竟在学习什么，他们的作用是什么，而这次的目的就是理解，理解它怎么去运用怎么去使用。 ​ 在假期结束后的9月到12月之间，从基础知识到猜数字到简单爬虫，可以说这段日子成长确实很快，但其中却还是充满了曲折坎坷。除去感情生活不谈，单点知识学习选择上，我不清楚要学习哪个方向的书籍，不清楚打开哪个学习网站，不清楚点开哪个视频教程，我对学习路线一无所知，只能无论深浅迈出一步，不过还好，踏出的每一步都挺结实。 经历的学习线如下：书籍《笨方法学Python》 《计算机科学导论》 《Python基础教程》 《大话数据结构》 《Python核心编程》 《Flask Web开发：基于Python的Web应用开发实战》 《图解HTTP》 《Python Web开发实战》 《浪潮之巅》 视频麦子学院 《Python语言编程基础》 《Python面向对象变成》 《简易爬虫编程实战》 《Python课程初探》 网易云课堂 《Python快速入门》 《零基础入门学习Python》 《0基础Python实战：爬虫课程免费试听》 《Python实战计划：7天上线苹果官网》（准备开始） 中国大学慕课 《C语言程序设计》 翁恺 《C语言程序设计进阶》 翁恺 文档 Flask中文文档 Bootstrap教程|菜鸟教程 廖雪峰Python教程 廖雪峰Git教程 ​ 此上的学习选择是尝试了各式各样后适合本人的，也可以在各学习平台搜索自行查找适合自己口味的教程，每个人的口味不同，适合自己最重要。 ​ 此外要针对web框架说一说，在Pyhton3里我是在Django和Flask里选择了后者，原因是它属于轻量级，对新手友好，你们也看得出我选择基本上都是那些对新手友好的，毕竟有难度的我怕给自己兴趣玩坏了。Python2里会多一个选择web.py，我没有接触过就不说它了。 ​ 在数据库选择里，我同样选择了对新手友好的MongoDB，配合pymongo增删查改无往不利。 ​ IDE选择里，我是用的Pycharm，这个还是看个人口味了，别的vim、notepad++、eclipse、sublime text等等等等看感觉自由选择。 ​ 还有没有谈到的没有想到就稍后再写。最后还有很多书籍都很好但是我还没有买来看的，像《编程珠玑》、《代码大全》、《算法导论》等等等等，等到一定火候向上再进一阶时再买来研究吧。 ​ 时间不多，今年的计划本是找个实习做一做，毕竟在公司里工作成长会更快，但现在的demo只有一个还拿不出手的，如果暑假前可以有两三个不错的demo就去找实习，如果没有就做demo，然后找实习。总而言之就是和时间赛跑了，多看多学多做多练习，编程之路现在也不过是刚刚开始。接下来就把现在手上唯一的以Flask做的本博客的构造过程写一写。 ​ 世界在变，我在改变。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程之路（四）——HelloWorld]]></title>
    <url>%2Fposts%2F53470%2F</url>
    <content type="text"><![CDATA[​ 世界上有90％的人学习阶段是荒废的，而如果我们在这个阶段学习一分钟，我们就已经从90％里脱颖而出了，而是选择90%还是10%，全得靠自己。 ​ 2016年6月，我开始学习Python。当时收集了许多IT网络学习平台，后来选择了麦子学院的一门Python基础教程，选择它是因为我不知道IDE是什么，不知道编程怎么去安装又怎么去使用，更不知道计算机的前世今生，而这门课程从安装到“Hello World”都很详细，最主要的是我看得懂听得懂，这已经很不容易了。当时已经临近期末，只想着要死不死的先把基础看一遍，对于当时Print都拼得错的我来讲，课程大纲里什么逻辑结构字典元组完全都没有概念，而第一次我完全冲着过一遍知识点去的，能理解就是我的目标。而现实往往是最残酷的，在看完一遍教程后自己完全是懵逼状态，总觉得自己是不是智商不够了欠费了没缴费？什么if、while什么else、when，完全照抄也能跑错好吧，各种不能理解不可描述阿喂！冒号引号少了也无所谓了，中英文符号我也分不清了，可能我学的是假的教程吧！总之首战惨败，但一场战争无论胜败总会有收获的，最起码我可是向另一个世界打了招呼呢： ​ Print(“Hello World !”) ​ 让你久等了，虽然晚了，但，我来了。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程之路（三）——语言的抉择]]></title>
    <url>%2Fposts%2F12531%2F</url>
    <content type="text"><![CDATA[​ 对于专业性较强的知识领域，我们一定要找到它自身的突破口，以点破面，而不是冒然前行，否则当施展浑身解数而不得一知时，往往只能放弃。 ​ 对于门外汉的我来讲，C和JAVA意识里我都将它们归属于科班人员专业性领域，是属于我触之即死的禁区。而在经过基础简单的HTML和CSS练习后，脱离后端的静态网页被我玩坏了，而且啪啪啪敲出来的静态网址我并不会部署啊（天真的以为学习一门语言就可以部署），所以我一定要开始学习一门语言了，那么我要选择哪一门语言呢？ ​ 当时每天在知乎编程类似话题下游荡，而就在游荡的过程中我突然发现有一门叫做Python的语音好评巨多，虽然说PHP是世界上最好的语言，但是我查了一下当时的编程语言排行榜，Python赫然仅次于C、JAVA和C++，随即心想：C和JAVA我不敢碰，你Python排名这么靠前还听说对新手友好，我还犹豫什么。 ​ 直到现在我还心有余悸，若我当时选择了C或者JAVA，恐怕自己就要成为编程门槛前的一抔黄土了，那我的编程路也可能随风飒飒飒没了…… ​ 上手的语言选好了，接下来就是我踏上醒途的时候了。这一年，历经坎坷与磨难，甚至颓唐到要放弃，但终究是挺过来了。而现在，奔腾的心，愈发坚定。 ​ 世界不灭，代码不止。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程之路（二）编程前奏——HTML]]></title>
    <url>%2Fposts%2F26151%2F</url>
    <content type="text"><![CDATA[​ 无论是知识的学习、 能力的培养还是品德的养成, 都应该由简入繁、由浅至深。 ​ 在接触一门行业之前，首先要了解这门行业，我也属于走一步看两步的人，而在我选择编程的时候，我已经通过网上丰富的交流平台对编程有个初步的了解。从前端到后端，从码农到产品经理，从PHP到PHP……从入门到放弃……好吧是从编程语言的选择到将来的发展方向，我心里都有了一个大概的了解和倾向，而当一切准备就绪，开始学习是自然而然的事情，而且心理压力也不会那么大。 ​ 当时的我正处于迷茫期，临近升入大三，对未知的未来充满了疑惑和惆怅，专业是师范类专业，赶上了末班车毕业管发教师资格证，完全可以安安稳稳考个招教或者考个公务员，但是这并不是我想要的，当一切只能循规蹈矩，不说碌碌无为倒一定是不温不火，而我也始终记得一句话“一个家族的衰败可能是时运不济，但一个家族的兴起必定是几代人的积累”。出生到现在，家里从贫穷到温饱自如，家境经过父辈们的努力逐渐转好，而我如果连闯出去的勇气都没有，不做些对自己对亲人有益的事情，那我当真自行惭秽了。 ​ 而就在这个时候，我选择了从入门友好简单易懂的前端方向，开始第一次尝试迈出编程之路的第一步。第一步选择前端是有备而去的，当时首先是知道自己几分几两，而学习又自当是由简入繁，其次也是听说学好了HTML5和CSS3，就足以找到一份前端工作了，而在所有前端入门的书籍中我又买了针对新手最友好的《Head First HTML和CSS》。 ​ 当初学习这本书的时候，每天就照着书上的标签敲啊敲，照着书上的例子敲啊敲，针对着一些细枝末叶每次都要敲上半天HTML，再跟着书上CSS样式一点点的调试，最后倒是真的敲出来一个像（low）模（dao）像（bao）样（zha）的一个静态网页。我当时真的是高兴坏了，现在想想嗯……情绪倒还是挺正常……不愧还是那个智障的我啊……（捂眼哭） ​ 可然后呢……把自己愁坏了……]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程之路（一）——前言]]></title>
    <url>%2Fposts%2F11225%2F</url>
    <content type="text"><![CDATA[前言​ 2016年注定不凡，这年，我的人生轨迹第二次出现转折——我开始了我的编程修道之路，而这条路，也必定应了：“路漫漫其修远兮，吾将上下而求索。”。 ​ 在这条修行路上人才济济，初始我感觉是孤独的，并不是形神上的孤独，而是内心无助的惶然。我是2014级汉语言文学专业学生，从文到理跨专业，无人带领，这一路，必定坎坷。我很庆幸在黑暗里一深一浅的摸索了过来，直到现在看得见眼前一丝曙光。 ​ 从2016年到现在，我学习到的并不多，从始至终我以一种愉快的心境在学习，我并不是不能不可以学的快一些，而是我知道我还没有踏上这条路的资格，我必须要保持对编程的兴趣，毕竟在我的处境中，四周皆为暗，欲速则不达。当时大二的我距离毕业还有两年，我就想，哪怕我学的再慢，就不信我花一年时间达不到入门级水平，而等我拥有了上路的资格， 我还有一年时间不管不顾的去学习。而事实证明我确实应该如此，事实也证明我思维确实挺难转个弯从文到理，一年时间到现在也不过是刚刚入门。 ​ 入门到现在我也想把我的过程，我的经历写下来，给还未入门在入门阶段徘徊的同学们一个参考，这可能是最差劲的参考了……下面我就先写写在编程路上我学到的一些值得感悟的东西，这些东西可以说是我们程序人生中推动自己快速成长的催化剂。 专一​ 当你选定一条路,另一条路的风景便与你无关。 ​ 这句话首先是建立在你选择了最适合自己的编程语言后，编程语言有那么多：C、JAVA、C#、C++、PHP、Python等等等等。首先我的第一门语言是Python，而我现在要做的，正是在Python的学习道路上坚持下去，但这并不是只学习一门语言，而是在入门阶段并不适宜多种语言同时学习，虽然他们可以做一样的事,但每种语言的语法各不相同。语言的专一也并不能太过极端……比如”PHP是世界上最好的语言”……我们一定要抱着包容开放的学习心态去学习，但这仅限于我们已经掌握了一门语言后。 ​ 其次专一的体现应该在你所选择的发展方向上，比如：Web、爬虫、软件编程、游戏等等等等。当你选择方向前你可以一一浅斟，都稍微接触一些，找到你所感兴趣的，而当你选择好你要发展的方向后，就应该专注于此，三心二意不可为大家都懂得，有很多同学这边学一些那边搞一搞，到头来什么都沾过，却什么都没有学会学精，这是最可惜的。 多写多实操​ 在接触小项目前，入门级的语法练习是必须要有的，有许多语法逻辑只看是不中用的，这一点想必大家都知道，但真正能后做到多写多练的人们并不多，我还是想强调一遍（认真脸），哪怕是简单的if、while、for，哪怕是print、echo，你写出来是没有坏处的。 ​ 而在接触了项目之后你还是只看教程视频上面人家的操作，你看一遍然后噢我懂了，然而你并没有实际操作一遍你还是压根不懂，一切妄想不实操看视频学精的人都是不切实际的，因为我就是这样啊！血淋淋的例子放在这……当我真的操作起来时候才追悔莫及——看了辣么多视频教程都白瞎了。所以一定要实操，从入门开始，养成良好习惯。 开源精神​ 编程之路重塑了我自身本人。可能有人会觉得我说的过了点，但一点都不过，我感谢编程的最重要一点正是如此，编程让我融入到了互联网世界的深处，它给予我的不仅仅是知识，而是三观的重塑，而在重塑过程中首当其冲的，正是开源精神。 ​ 开源即是自由的化身。它讲述了一种公开的、自由的精神。在计算机发展的早期阶段，软件几乎都是开放的，任何人使用软件的同时都可以查看软件的源代码，或者根据自己的需要去修改它。在程序员的社团中大家互相分享软件，共同提高知识水平。这种自由的风气给大家带来了欢乐，也带来了进步。 ​ 在编程的坎坷中，我真真切切感受到了开源的真谛所在，我不再感受孤独，因为在网上我能够搜索到许多与我同行的人，虽然处在千里之外，但他们贡献出的力量，是我确确实实感受的到的，而参与开源则是最漫长的一步。当我们真正参与到开源过程中时，我们会发现开源真正给予的，除了拿到的知识资源外，还给予了我们认识美发现美的独特眼光和懂得感恩懂得回馈的心。我们收获的不仅仅是技术上的进步，在思维和思想上，也会收获颇多，这种收获是会在本质上慢慢积累的，当积累达到足以发生质变的程度后，你终会发现：你的自信，油然而生。 ​ 开源是个大概念，我做不到太多，但总会真正参与进去贡献我绵薄之力，正如当前如此。也希望同行的你们，也如此。 总结​ 拿现在的自己做对比，以前的自己宛如一个智障，虽然现在也还是个智障……但总归是比以往好了那么一些…… ​ 在编程生活中，我们需要沉下心、多尝试多实操、学会分析问题、勤于总结，而这些在学习过程中我们都会不自觉的慢慢学会，以至于现在我还很差异我什么时候变得不一样了。 ​ 有许多想和入门阶段的同学们说，但还是先到这里吧。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[i-wanna-say]]></title>
    <url>%2Fposts%2F24302%2F</url>
    <content type="text"><![CDATA[时隔多天，记些琐事 首先要写的是学了什么、学到了什么、正在学什么、将要学些什么。 1.学了什么2016.6 麦子学院 《Python入门》《Python面向对象》 视频教学 慕课 《Python入门》 视频教学 廖雪峰 Python教学模块前 教程理解 ​ 2016年6月，多年的IT欲望突破闸口。由于事先都是以旁观者观察IT界，C和JAVA我都望而生怯，最终在哪里看见Python已经不记得了，但还记得Python的入门课程是在麦子学院里看的视频公开课，通俗易懂，让我有种多年期盼的愿望即将实现的错觉。然而，非科班甚至文跨理的弧度不可忽视，在抽象层的面向对象层次鱼跃失败，龙门终不得过。于是硬啃了一遍麦子学院的面向对象，在慕课上参照着同样的Python入门进行巩固，最后在知乎大家都说廖的教程，于是启程。不得不说这个月的初探已经受挫，计算思维和抽象能力在三年的纯文学环境下丝毫不剩，基本的Python模块、函数、面向对象，都苦苦思虑郁郁不得。 2016.6 - 9 麦子学院 《Python基础》 视频教学 网易云阅读 《与小卡特一起学Python》 书籍通读40％ 《Head First HTML与CSS》第二版 书籍通读 网易云课堂 《Python爬虫》 视频教学 《计算机科学导论》 书籍通读 网易云课堂 《哈佛大学公开课：计算机科学导论》 视频教学前六集 ​ 2016年6月至9月，暑假来临，本打算在家好好的把入门给入门了，结果还是学车···学车···学车···学Python的同学都知道，Python必经的是爬虫，因为不知道究竟怎么样才可以进阶，从入门到基础，知乎看了不知几个昼夜，看着函数还发懵，暑假刚开始还是默默的学习Python基础，同时想着会有书籍看，于是先下载了网易云在上面看了一本知乎推荐的入门书籍，但还是越看越迷糊。 ​ 直到最后在知乎上看到一篇软文推荐爬虫项目直接上手，说上项目理解的透也容易理解，于是在网易云课堂参加了这篇软文推荐的课程，开始了趴虫之旅···好吧在我这里爬虫都趴了···在爬虫阶段接触到了HTML，深思熟虑将自己就那么多的零花钱买了一本《Head First HTML与CSS》第二版。犹豫此书通俗易懂易吸收，我就不说沉迷于此直至暑假终结甚至一度想去学前端拉倒了···哦还有一个不得不说的问题···正则···都是泪。 ​ 暑假就这么结束了，然而根本毫无进展好不！！！！心要碎了！！！！学车也是想哭就不说了··· ​ 开学那么几天，终于开窍了，觉得不着急，我不能急，因为着急也没用啊，学学不会，看看不会，那就学点能学会能看会的吧。我是非科班，既然是非科班就要学一些科班的基础弥补自己，看不懂指不定就差在基础上了，我觉得我做了最聪明的决定。买了一本《计算机科学导论》，一页一页一字一句的看，从计算机构成与图灵机开始看，就这样，开学的一个月啃掉了这本书，我不求可以达到通透，我当时想，只要我可以理解就行，只要我能理解就行，我慢慢来，不着急。于是在看完这本基础导论后又在网上搜了搜导论的著名课程——哈佛公开课CS50，每节课六七十分钟，看了六集后，我觉得懒癌犯了，又或者是因为这节课是C语音的缘故中断了。这个阶段我搜索了一大堆的计算机导论的课，还有计算机组成原理之类的，但都是只看几节课就没看了，我觉得它们讲的都千篇一律，导论书上都有似的。九月就要结束了，时间不等人，我又着急了。 2016.10-至今 《浪潮之巅》上···的前几页... 《Python核心编程》···的前几章... 廖雪峰Git 教程理解 廖雪峰Python 教程理解 《Flsak web开发》 书籍阅读 Flask 文档 文档阅读 中国大学MOOC C程序设计入门——C语言 视频理解 中国大学MOOC 大学计算机基础 视频理解 中国大学MOOC Python语言程序设计 视频理解 中国大学MOOC 程序猿与攻城狮 视频理解 《大话数据结构》 书籍阅读 ​ 2016年10月到现在，一句话来说就是坠入了MOOC天堂，感觉来到了一世桃源，本买了几本书要看，然而看了几章都觉得不得解，没有一种彻悟的感觉。于是兢兢业业，把之前落下的廖的Python给补上了，顺势学了Git，但实际上了解Git已经许久，却和C的感觉一样不敢上阵，但是廖的Git教程着实让我欢喜一番，读了一个晚上到天亮，居然理解的通。​ 忘了说买的那几本书先到的是《大话数据结构》，看它是因为程序=算法+数据结构嘛，本要看算法，却看不懂···买了本入门的数据结构看一看，现在还是不敢看算法，觉得还没有达到该接触的那个地步，大话看了一遍，理解可以，却不能通透，排序、树、队列、栈、线性表、图，都只理解了大概吧···只能说是尽力的去读了两遍，也对时间复杂度、空间复杂度、和算法与数据结构的关系有个初步了解。​ 总的来说阅读还是很有用的，有时候结合视频看有一种水到渠成的感觉，融会贯通，但自拙，不得解的依旧很多。学习Flask是因为想往Web方向学习，能做出blog就足够让现在的我满足了，但期间看了中慕的大学计算机基础课程，像上瘾了般补基础，基础确实影响长远发展，基础决定以后能走多远应该是真理吧，我坚守它。于是在 看了几章Flask之后，将中慕的Python和计算机基础的课程基础又补了好久，后来参加了翁凯老师的C语言基础学习这门课，现在也还在跟学，然后发现自己居然可以很快的C语音入门了···看来基础补的还是有用的，毕竟那句所有的语言同出本源被我印证事实了··· 2.学到了什么​ 学到的很多，但很杂，也都是入门，不多说，因为学到的并不多 3.正在学什么​ 正在学Flask Web开发、计算机组成原理、Mysql数据库、C程序设计入门 4.将要学些什么​ 将要学的也很多，先把上面的学会，寒假结束前搞个blog。]]></content>
  </entry>
</search>
