<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[经验]]></title>
    <url>%2F2018%2F10%2F28%2F%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[讲述人简介 工作半年 学历 - 硕士 做的人工智能的自然语言处理，招聘系统里的算法组， 只面试了一家，当时找的数据挖掘，看重了学习能力留下来进了算法组，当场给的offer 填简历几个维度去看你,让你的简历更容易被别人记下来 薪资填一下无论HR还是自己都对自己有预期，可以填一下 职能与投递的公司契合度在哪里 学历 很看重，四六级，211与否，本科or硕士，占不到优势就只能侧重于技能方向 技能(重点) 无论做什么方向，讲述自己具备技能时需要有自己的特色，或者说相比于别人你需要更好的去表述自己擅长的方面：宏观上你对项目的贡献，微观上贡献的技术点深入(吹)，学会吹 ：），吹牛逼不是撒谎 = =/。注意细节，描述的越细越真，不能含糊 自学能力描述自己的学习过程，遇到的困难坎坷，表现出自己的自学能力很强 热情要嗨起来，展示对技术的追求与对未知的好奇心，正能量 关于不是本专业的(非科班)，怕受到歧视不用担心，大部分科班都是走的校招找工作，而我们去找工作是社招，别人看重的是我们的技能，讲述人所在的算法组中都不是计算机本专业的，在公司，周围的同事大部分也不是本专业的，所以不用太担心，嗨起来 关于爬虫可能你们分布式爬虫都搞不出来，老师教的很有限，目前来讲爬虫方向的门槛很高，讲述人的公司面了很多爬虫最后都没有要 关于就业讲述人建议，当前的问题就在于就业，想走高端方向可以慢慢学，但是目前来讲没有经济基础还是先就业比较好，要现实一点先找到工作，目前的测试需求非常大，他的朋友就是先工作然后转到了算法工程师，python就是一个起点，也就是从零到一的开始，先开个头，再莽。 建议 住的地方离公司近一点，开销会少很多 把目前的公司当跳板，好好利用周末时间进行个人技能的提升 简历有老师辅导很简单了，重点就在面试了，注意细节，要嗨起来 数据结构与算法最好系统的学一学，有手写算法的面试题的都用的上]]></content>
  </entry>
  <entry>
    <title><![CDATA[Music]]></title>
    <url>%2F2018%2F10%2F27%2FMusic%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>音乐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-正则化]]></title>
    <url>%2F2018%2F10%2F27%2FMachineLearning-%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1. 正则化它可以改善或者减少过度拟合问题 2. 欠拟合(模型的高偏差)欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或者模型没有很好地捕捉到数据特征，不能够很好地拟合数据。 3. 过拟合(模型的高方差)为什么出现过拟合 特征过多 训练集数据较少 模型复杂 对过拟合的理解如果我们拟合一个高阶多项式，那么这个函数能很好的拟合训练集能拟合几乎所有的训练数据，这就面临可能函数太过庞大的问题，即变量太多。同时如果我们没有足够的数据去约束这个变量过多的模型，就会出现过度拟合的情况。虽然训练出的模型能够很好的拟合训练集的样本数据，但很有可能无法泛化新样本。 如何解决过拟合 尽量减少选取特征数量。后面会学到模型选择算法 ，它能够自动选择采用哪些特征变量，且自动舍弃不需要的变量 正则化保留所有特征变量，但是减少参数$\theta_j$的大小 4. 怎么应用正则化思想与做法修改代价函数，从而收缩(惩罚)所有的参数值，因为我们并不知道具体的去收缩(惩罚)哪些参数， 修改后的代价函数如下：$$J\left(\theta\right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{({h_\theta}({x}^{(i)})-{y}^{(i)})^{2}+\lambda \sum\limits_{j=1}^{n}\theta_{j}^{2}]}$$$\lambda $又称为正则化参数（Regularization Parameter），它能够平衡代价函数，使$\theta_j$尽可能的小。 注：根据惯例，我们的$j$是从1开始的，也就是我们不对${\theta_{0}}$ 进行惩罚。 举一个例子我们看这个假设函数: $h_\theta\left( x \right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^2+\theta_{3}x_{3}^3+\theta_{4}x_{4}^4$ 。通常地，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。于是我们将修改代价函数，在其中${\theta_{3}}$和${\theta_{4}}$ 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的${\theta_{3}}$和${\theta_{4}}$。修改后如下 :$$\underset{\theta}{\mathop\min }\,\frac{1}{2m}[\sum\limits_{i=1}^{m}{\left({h}_{\theta }\left( {x}^{(i)} \right)-{y}^{(i)} \right)^{2}+1000\theta _{3}^{2}+10000\theta _{4}^{2}]}$$但是正是因为我们并不知道具体的哪一个$\theta$是高次项，因此我们只能去收缩(惩罚)所有参数。 但是如果我们令 $\lambda$ 的值很大的话，那么$\theta $（不包括${\theta_{0}}$）都会趋近于0，这样我们所得到的只能是一条平行于$x$轴的直线。所以对于正则化，我们要取一个合理的 $\lambda$ 的值，这样才能更好的应用正则化。 5. 正则化线性回归正则化代价函数$$J\left(\theta\right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[(({h_\theta}({x}^{(i)})-{y}^{(i)})}^{2}+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2})]}$$ 正则化梯度下降要使梯度下降法令正则化后的线性回归代价函数最小化，因为我们没有对$\theta_0$进行正则化，所以梯度下降算法有两种情形：$${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}(({h_\theta}({x}^{(i)})-{y}^{(i)})x_{0}^{(i)})$$ $${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left(i\right)}+\frac{\lambda }{m}{\theta_j}]$$ 对第二个式子进行变化后，可得:$${\theta_j}:={\theta_j}(1-a\frac{\lambda }{m})-a\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left(i\right)}$$可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta $值减少了一个额外的值。 PS：梯度下降仍然是对$J(\theta)$进行最小化，通过求导，得出梯度下降算法 正则化正规方程 注：图中的矩阵尺寸为 $(n+1)*(n+1)$。 值得一提的是，哪怕此时$X$不可逆，经过$\lambda$相加变化后的矩阵将是可逆的。 6. 正则化逻辑回归正则化代价函数$$J\left(\theta\right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{y}^{(i)}\log \left({h_\theta}\left({x}^{(i)}\right)\right)-\left(1-{y}^{(i)} \right)\log\left(1-{h_\theta}\left({x}^{(i)}\right) \right)]}+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta _{j}^{2}$$ ​ ps：注意这里$\lambda$的仍为$\frac{1}{2m}$ 正则化梯度下降类似地，因为我们没有对$\theta_0$进行正则化，所以梯度下降算法有两种情形：$${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}(({h_\theta}({x}^{(i)})-{y}^{(i)})x_{0}^{(i)})$$ $${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}({h_\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{\left( i \right)}+\frac{\lambda }{m}{\theta_j}]$$ 看起来和线性回归的一模一样，实际上我们知道这里 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$，所以与线性回归不同。 PS：值得注意的是，${\theta_{0}}$仍然不参与其中的任何一个正则化。 注:泛化能力（generalization ability）泛化能力是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的模型也能给出合适的输出，该能力称为泛化能力。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-逻辑回归]]></title>
    <url>%2F2018%2F10%2F25%2FMechineLearning-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[什么是逻辑回归逻辑回归算法是分类算法，可能它的名字里出现了“回归”让我们以为它属于回归问题，但逻辑回归算法实际上是一种分类算法，它主要处理当 $y$ 取值离散的情况，如：1 0 。 为什么不使用线性回归算法处理分类问题假设我们遇到的问题为 二分类问题，那么我们可能将结果分为负向类和正向类，即$y\in0,1$ ，其中 0 表示负向类，1 表示正向类。如果我们使用线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，但是我们需要的假设函数输出值需要在0到 1 之间，因此我们需要用到逻辑回归算法。 逻辑回归的假设函数与理解逻辑回归的假设函数 sigmoid function 表示方法 :$$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$$ 理解记忆:其实里面的$\theta_Tx$就是线性回归时的假设函数 h(x) ，$$h(x) = \theta^Tx = \sum_{j=0}^{n}{\theta_jx_j}$$而逻辑回归的假设函数其实就是将线性回归的表达式 h(x) 以 z 的形式代入到了 S 型函数(sigmoid function) 中 :$$g(h(x)) = g(z) = \frac{1}{1+e^{-z}}$$ps: 这里我们用$h(x)$表示的是线性回归的假设函数，之后的$h$都将表示 sigmoid 函数。 对假设函数的解释 :给定 x ，根据选择的参数计算出y = 1 的概率 ，具体的概率公式如下 :$$h_\theta(x) = P(y=1|x; \theta)$$ Sigmoid - Python:1234import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z)) 判定边界(decision boundary)如何得出判定边界 :在 Sigmoid 中，但凡输出结果 $h_\theta(x)$大于 0.5 的，我们都将预测结果 $y$ 收敛于 1 ；小于 0.5 的，收敛于 0 ；而恰好等于 0.5 的，收敛1 或 0 都可以，我们可以自己设定它如何收敛。由此，我们的输出值就都在 0 到 1 之间了。而当 $h_\theta(x)$ 大于 0.5 时，$\theta^Tx$ 大于 0.5， $h_\theta(x)$ 小于 0.5 时，$\theta^Tx$ 小于 0.5， $h_\theta(x)$ 等于 0.5 时，$\theta^Tx$ 等于 0.5。当然，具体的阈值是可以调整的，比如说你是一个比较保守的人，可能将阈值设为 0.9 ，也就是说有超过 90% 的把握，才相信这个$y$收敛于 1 。 由此，我们能够绘制出判定边界 :$$\theta^Tx = 0$$ 关于判定边界 : 决策边界不是训练集的属性，而是假设本身及其参数的属性 只要给出确定的参数$\theta$，就确定了我们的决策边界 高阶多项式(多个特征变量)能够让我们得到更复杂的决策边界 逻辑回归的代价函数，梯度下降自动拟合$\theta$，以及代价函数的推导过程逻辑回归的代价函数 :$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})$$其中$Cost$ :$$Cost(h_\theta(x),y) = -ylog(h_\theta(x)) - (1-y)log(1-h_\theta(x))$$因此$J(\theta)$ :$$J(\theta) =-\frac{1}{m}[\sum_{i=1}^{m} y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$ 使用对数几率的原因: 代价函数 $J(\theta)$ 会是一个凸函数，并且没有局部最优值。否则我们的代价函数将是一个非凸函数。 逻辑回归的梯度下降算法 :Repeat {$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$$(simultaneously update all )} 求导后得到： Repeat {$$\theta_j := \theta_j - \alpha\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$ (simultaneously update all )} ps : 逻辑回归梯度下降结果与线性回归梯度下降结果一致，但其中的$h_\theta(x)$并不一样，因此本质上是不同的。 关于特征缩放和均值归一化:思想:在有多个特征的情况下，如果你能确保这些不同的特征都处在一个相近的范围，这样梯度下降法就能更快地收敛。使代价函数$J(θ)$的轮廓图的形状就会变得更圆一些。 做法:一般地，我们执行特征缩放时，我们通常将特征的取值约束到接近−1到+1的范围。其中，特征x0总是等于1，因此这已经是在这个范围内了，但对于其他的特征，我们需要通过除以不同的数来让它们处于同一范围内。除了在特征缩放中将特征除以最大值以外，有时候我们也会进行一个称为均值归一化的操作:$$x_n = \frac{x_n-μ_n}{s_n}$$其中，$μ_n$是平均值，$s_n$是标准差 好处: 更好的进行梯度下降，提高代价函数的收敛速度 提高代价函数求解的精度 更适合解决大型机器学习的问题​ 其他相较于梯度下降算法更好的的令代价函数最小的算法(高级优化[超纲])常用算法: 共轭梯度(Conjugate Gradient) 局部优化法(BFGS - Broyden fletcher goldfarb shann) 有限内存局部优化法(LBFGS) 好处: 这些算法内部有一个智能的内部循环(线性搜索算法)，能够尝试不同的 $\alpha​$ 并自动的选择一个好的学习速率 $\alpha​$ ，这样就不需要手动选择 $\alpha​$ 收敛速度通常比梯度下降算法更快速 缺点: 比梯度下降算法更加复杂 使用逻辑回归算法解决多类别问题思想:将多分类问题拆分成多个二分类问题并得出多个模型。最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 做法:我们将多个类中的一个类标记为正向类（$y=1$），然后将其他所有类都标记为负向类，这个模型记作$h_\theta^{\left( 1 \right)}\left( x \right)$。接着，类似地我们选择另一个类标记为正向类（$y=2$），再将其它类都标记为负向类，将这个模型记作 $h_\theta^{\left( 2 \right)}\left( x \right)$,依此类推。最后我们得到一系列的模型简记为： $h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)$其中：$i=\left( 1,2,3….k \right)$ 。然后我们将这多个逻辑回归分类器进行训练并得出最终模型：$h_\theta^{\left( i \right)}\left( x \right)$， 其中 $i$ 对应每一个可能的 $y=i$，最后，当我们需要进行预测时，输入一个新的 $x$ 值，我们要做的就是在这多个分类器里面输入 $x$，然后在多个分类器得出的结果中，选出一个最大的$ i$，即$\mathop{\max}\limits_i\,h_\theta^{\left( i \right)}\left( x \right)$。 逻辑回归梯度下降中代价函数求导过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-向量化]]></title>
    <url>%2F2018%2F10%2F25%2FMechineLearning-%E5%90%91%E9%87%8F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[向量化 - 传统累加运算 - 代码实现:1234567891011121314151617181920import timeimport numpy as np# 定义两组向量vector1 = np.random.rand(100000)vector2 = np.random.rand(100000)# 使用向量化start_time = time.time() # 开始时间res = np.dot(vector1, vector2) # 向量直接相乘得到最终结果end_time = time.time() # 结束时间print("Vectorized: " + str((end_time - start_time)*1000) + "ms" + " res =" + str(res))# 使用for循环res = 0start_time = time.time() # 开始时间for i in range(100000): # 传统的累加运算,需要累加100000次 res += vector1[i] * vector2[i]end_time = time.time() # 结束时间print("For loop: " + str((end_time - start_time)*1000) + "ms" + " res =" + str(res)) 结果对比:12Vectorized :1.0001659393310547ms res =24969.775960643143For loop:79.94818687438965ms res =24969.775960642968 ​ 从执行结果来看向量化的运算速度要比非向量化的运算快了近80倍，而这个对比结果还会随着运算集的数目增加而增加。 为什么:​ CPU 与 GPU 都能够使用 SIMD 指令进行并行化操作，即以同步方式，在同一时间内执行同一条指令。一般来讲可扩展的深度学习都在 GPU 上做，但其实 CPU 也不是太差，只是没有 GPU 擅长。 ​ 而 Python 的 numpy 的一些内置函数能够充分利用并行化来加速运算，比如 np.dot，因此，不到逼不得已，还是不要使用 for 循环吧 注:​ GPU - 图形处理器也，叫做图像处理单元，显卡的处理器。与 CPU 类似，只不过 GPU 是专为执行复杂的数学和几何计算而设计的，这些计算是图形渲染所必需的。​ SIMD - 单指令多数据流，以同步方式，在同一时间内执行同一条指令。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
